{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyObwnHufpthph9lbnZNmMxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrwiren/ilmanlaatu-ennuste-helsinki/blob/main/Uusi_esik%C3%A4sittely_skripti_2025-04-11_1046.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "==========================================================================================\n",
        " TÄYDELLINEN SKRIPTI HELSINGIN SÄÄ- JA ILMANLAATUDATAN YHDISTÄMISEEN (FMI)\n",
        "==========================================================================================\n",
        "\n",
        " Versio: 2.0\n",
        " Päivitetty: 2025-04-11 10:46 (BST)\n",
        "\n",
        " Skriptin tarkoitus:\n",
        " --------------------\n",
        " 1. Ladata kolme erillistä CSV-datatiedostoa (2 x säädata Kaisaniemestä,\n",
        "    1 x ilmanlaatudata Kalliosta) määritellyistä URL-osoitteista GitHubista.\n",
        "    Nämä tiedostot oletetaan alun perin ladatun FMI:n avoimen datan palvelusta.\n",
        " 2. Esikäsitellä jokainen tiedosto huolellisesti:\n",
        "    - Muodostaa yhtenäisen, aikavyöhykkeet huomioimattoman ('naive') datetime-indeksin\n",
        "      ('Timestamp') yhdistämällä Vuosi, Kuukausi, Päivä ja Aika -sarakkeet.\n",
        "    - Poistaa tarpeettomat alkuperäiset aika- ja paikkatiedot.\n",
        "    - Muuntaa mittausdata numeeriseen muotoon, käsitellen mahdolliset virheet.\n",
        "    - Pudottaa sarakkeet, jotka ovat kokonaan tyhjiä (vain NaN).\n",
        "    - Nimetä sarakkeet selkeiksi, englanninkielisiksi nimiksi analyysin helpottamiseksi\n",
        "      ja lisätä päätteitä (_W1, _W2) erottamaan samankaltaiset sarakkeet eri\n",
        "      lähdetiedostoista.\n",
        " 3. Yhdistää esikäsitellyt sää- ja ilmanlaatudatat yhdeksi kattavaksi Pandas\n",
        "    DataFrameksi aikaleiman perusteella.\n",
        " 4. Tunnistaa ja käsitellä mahdolliset duplikaattiaikaleimat, jotka voivat syntyä\n",
        "    esimerkiksi datan keruussa tai yhdistämisessä. Duplikaatit käsitellään\n",
        "    laskemalla päällekkäisten rivien numeeristen arvojen keskiarvo.\n",
        " 5. Varmistaa, että lopullisessa DataFramessa on tasainen tunnin välein etenevä\n",
        "    aikasarjaindeksi (`freq='h'`) koko data-ajan mitalta. Tämä täyttää mahdolliset\n",
        "    puuttuvat tunnit NaN-arvoilla.\n",
        " 6. Täyttää kaikki jäljelle jääneet puuttuvat arvot (NaN) käyttäen ensin\n",
        "    eteenpäin täyttöä (`ffill`) ja sitten taaksepäin täyttöä (`bfill`).\n",
        " 7. (Valinnaisesti) Tallentaa lopullisen, siivotun ja täydennetyn DataFramen\n",
        "    tehokkaaseen Parquet-tiedostomuotoon, joka soveltuu hyvin jatkokäyttöön.\n",
        "\n",
        " Vaaditut kirjastot:\n",
        " -------------------\n",
        " - pandas: Datan käsittely ja analysointi (DataFrame).\n",
        " - numpy: Numeerinen laskenta (käytetään esim. NaN-arvojen esitykseen).\n",
        " - requests: HTTP-pyyntöjen tekeminen (datan lataus URL:sta).\n",
        " - io: Tekstipohjaisen datan käsittely muistissa (CSV-datan luku).\n",
        " - os: Käyttöjärjestelmäriippumattomat polkujen käsittelytoiminnot (tallennus).\n",
        " - pyarrow: Vaaditaan Parquet-tiedostojen kirjoittamiseen (`pip install pyarrow`).\n",
        "\n",
        " Käyttö:\n",
        " -------\n",
        " 1. Varmista, että vaaditut kirjastot on asennettu.\n",
        " 2. Aja skripti Python-ympäristössä.\n",
        " 3. Tarkkaile tulosteita varmistaaksesi, että kaikki vaiheet suoritetaan odotetusti.\n",
        "    Erityisesti sarakkeiden uudelleennimeämiskohdat Vaiheessa 2 saattavat vaatia\n",
        "    säätöä, jos lähde-CSV:n sarakkeiden nimet muuttuvat.\n",
        " 4. Jos haluat tallentaa lopputuloksen, poista kommenttimerkit Vaiheen 5\n",
        "    tallennuskoodista ja varmista, että 'pyarrow' on asennettu.\n",
        "\"\"\"\n",
        "\n",
        "# === Vaihe 1: Alustus ja Kirjastojen Tuonti ===\n",
        "print(\"=\"*60)\n",
        "print(\" Vaihe 1: Alustus ja Kirjastojen Tuonti\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tuodaan kirjastot selittävin kommentein\n",
        "import pandas as pd  # Peruskirjasto datan manipulointiin DataFramien avulla\n",
        "import numpy as np   # Tieteellisen laskennan peruskirjasto, käytetään NaN-arvoihin ym.\n",
        "import requests      # Mahdollistaa HTTP-pyyntöjen tekemisen, esim. datan lataus verkosta\n",
        "import io            # Mahdollistaa merkkijonojen käsittelyn tiedostojen kaltaisina virtauksina\n",
        "import os            # Tarjoaa käyttöjärjestelmäriippumattomia toimintoja, kuten polkujen käsittely\n",
        "\n",
        "# Määritellään datatiedostojen URL-osoitteet GitHubin raakasisältöön.\n",
        "# Nämä ovat suoria linkkejä itse tiedostoon, eivät GitHubin web-sivulle.\n",
        "url_weather_1 = \"https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/data/raw/Helsinki%20Kaisaniemi_%2011.4.2023%20-%2011.4.2025_4c0a0316-74e0-4792-9854-fd6315fbc965.csv\"\n",
        "url_weather_2 = \"https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/data/raw/Helsinki%20Kaisaniemi_%2011.4.2023%20-%2011.4.2025_84370efb-e7a0-48ed-b991-0432c84c1475.csv\"\n",
        "url_aq = \"https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/data/raw/Helsinki%20Kallio%202_%2011.4.2023%20-%2011.4.2025_8bbe3500-d31d-459d-ac37-16b4bd64a2cc.csv\"\n",
        "\n",
        "# === Apufunktio CSV-datan lataamiseen ja alustavaan lukuun ===\n",
        "def load_csv_from_url(url):\n",
        "    \"\"\"\n",
        "    Lataa CSV-tiedoston annetusta URL-osoitteesta ja yrittää lukea sen DataFrameksi.\n",
        "\n",
        "    Args:\n",
        "        url (str): Ladattavan CSV-tiedoston URL-osoite.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame or None: Ladattu data DataFrame-objektina, tai None jos epäonnistui.\n",
        "    \"\"\"\n",
        "    print(f\"\\nYritetään ladata ja lukea: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Tarkistaa HTTP-statuskoodin (esim. 200 OK)\n",
        "        csv_data = io.StringIO(response.text) # Luetaan tekstisisältö muistiin virtana\n",
        "        # Yritetään lukea pilkulla erotettuna (perustuen aiempaan onnistumiseen)\n",
        "        df = pd.read_csv(csv_data, sep=',')\n",
        "        print(f\"  -> Luettu onnistuneesti (oletus sep=',')\")\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  -> VIRHE LADATESSA: {e}\")\n",
        "        return None\n",
        "    except Exception as e: # Muu virhe (esim. CSV-luvussa)\n",
        "        print(f\"  -> VIRHE LUETTAESSA CSV: {e}\")\n",
        "        # Tässä voisi olla lisälogiikkaa kokeilla eri erottimia (';') tai enkoodauksia ('latin1')\n",
        "        return None\n",
        "\n",
        "# === Ladataan kaikki kolme dataa ===\n",
        "print(\"\\nLadataan datatiedostot...\")\n",
        "df_w1_raw = load_csv_from_url(url_weather_1)\n",
        "df_w2_raw = load_csv_from_url(url_weather_2)\n",
        "df_aq_raw = load_csv_from_url(url_aq)\n",
        "\n",
        "# Tarkistetaan latausten onnistuminen\n",
        "if df_w1_raw is None or df_w2_raw is None or df_aq_raw is None:\n",
        "    print(\"\\nKRIITTINEN VIRHE: Kaikkia tiedostoja ei voitu ladata. Tarkista URL-osoitteet ja verkkoyhteys. Keskeytetään.\")\n",
        "    # exit() # Voit aktivoida tämän, jos haluat skriptin pysähtyvän tähän.\n",
        "else:\n",
        "    print(\"\\nKaikki tiedostot ladattu onnistuneesti. Jatketaan esikäsittelyyn.\")\n",
        "    data_loaded_successfully = True # Asetetaan lippu jatkoa varten\n",
        "\n",
        "# Suoritetaan loput vain, jos lataus onnistui\n",
        "if data_loaded_successfully:\n",
        "\n",
        "    #==============================================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Vaihe 2: Datan Esikäsittely (jokainen tiedosto erikseen)\")\n",
        "    print(\"=\"*60)\n",
        "    #==============================================================================\n",
        "\n",
        "    # --- Apufunktio FMI-datan yleiseen esikäsittelyyn ---\n",
        "    def preprocess_fmi_data(df, data_type_name):\n",
        "        \"\"\"\n",
        "        Suorittaa FMI:n CSV-datalle yleiset esikäsittelyvaiheet.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): DataFrame, joka sisältää FMI:n raakadataa.\n",
        "            data_type_name (str): Datan kuvaava nimi tulosteita varten.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame or None: Esikäsitelty DataFrame tai None, jos virhe.\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n--- Esikäsitellään: {data_type_name} ---\")\n",
        "        original_cols = df.columns.tolist()\n",
        "        print(\"Alkuperäiset sarakkeet:\", original_cols)\n",
        "\n",
        "        # --- 1. Aikaleiman muodostus ---\n",
        "        # Oletetaan FMI:n käyttävän näitä sarakkeita päivämäärän ja ajan esittämiseen.\n",
        "        date_cols = ['Vuosi', 'Kuukausi', 'Päivä']\n",
        "        time_col = 'Aika [Paikallinen aika]'\n",
        "        # Varmistetaan, että kaikki tarvittavat sarakkeet ovat olemassa.\n",
        "        if all(col in original_cols for col in date_cols) and time_col in original_cols:\n",
        "            try:\n",
        "                # Yhdistetään sarakkeet muotoon \"YYYY-MM-DD HH:MM:SS\" merkkijonoksi.\n",
        "                # zfill(2) lisää etunollan tarvittaessa (esim. 1 -> 01).\n",
        "                datetime_str_series = df['Vuosi'].astype(str) + '-' + \\\n",
        "                                      df['Kuukausi'].astype(str).str.zfill(2) + '-' + \\\n",
        "                                      df['Päivä'].astype(str).str.zfill(2) + ' ' + \\\n",
        "                                      df[time_col].astype(str)\n",
        "                # Muunnetaan merkkijonot datetime-objekteiksi. Pandas tunnistaa muodon automaattisesti.\n",
        "                df['Timestamp'] = pd.to_datetime(datetime_str_series)\n",
        "                print(\"  -> Aikaleima ('Timestamp') muodostettu onnistuneesti.\")\n",
        "                # Pudotetaan alkuperäiset aika/pvm-sarakkeet ja vakio 'Havaintoasema'-sarake.\n",
        "                cols_to_drop = date_cols + [time_col]\n",
        "                if 'Havaintoasema' in original_cols:\n",
        "                     cols_to_drop.append('Havaintoasema')\n",
        "                df = df.drop(columns=cols_to_drop, errors='ignore')\n",
        "            except Exception as e:\n",
        "                print(f\"  -> VIRHE aikaleiman muodostamisessa: {e}. Tarkista datan muoto.\")\n",
        "                return None # Palautetaan None, koska ilman aikaleimaa ei voida jatkaa.\n",
        "        else:\n",
        "            missing_cols = [col for col in date_cols + [time_col] if col not in original_cols]\n",
        "            print(f\"  -> VIRHE: Tarvittavia aikaleimasarakkeita ei löytynyt. Puuttuvat: {missing_cols}\")\n",
        "            return None\n",
        "\n",
        "        # --- 2. Indeksin asetus ---\n",
        "        # Asetetaan juuri luotu 'Timestamp' DataFramen indeksiksi. Tämä on standardikäytäntö\n",
        "        # aikasarjadatan käsittelyssä Pandasilla, ja se helpottaa yhdistämistä ja aikaperusteisia operaatioita.\n",
        "        try:\n",
        "            df = df.set_index('Timestamp')\n",
        "            print(\"  -> 'Timestamp' asetettu indeksiksi.\")\n",
        "        except KeyError:\n",
        "            print(\"  -> VIRHE: 'Timestamp'-saraketta ei löytynyt indeksiksi asettamista varten.\")\n",
        "            return None\n",
        "\n",
        "        # --- 3. Numeeriseksi muunto ---\n",
        "        # Muunnetaan kaikki jäljellä olevat sarakkeet numeerisiksi (float tai int).\n",
        "        # Tämä on tärkeää laskentaa ja mallinnusta varten.\n",
        "        print(\"  -> Muunnetaan data numeeriseksi (virheet -> NaN)...\")\n",
        "        for col in df.columns:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                # errors='coerce' korvaa arvot, joita ei voi muuntaa, NaN-arvolla.\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        print(\"    -> Muunnos valmis.\")\n",
        "\n",
        "        # --- 4. Täysin tyhjien sarakkeiden poisto ---\n",
        "        # Joskus FMI-data voi sisältää sarakkeita, joissa ei ole lainkaan mittauksia (vain NaN).\n",
        "        # Nämä poistetaan turhina. axis=1 tarkoittaa sarakkeita, how='all' tarkoittaa \"poista vain jos kaikki arvot ovat NaN\".\n",
        "        df = df.dropna(axis=1, how='all')\n",
        "        print(f\"  -> Lopulliset sarakkeet esikäsittelyn jälkeen: {df.columns.tolist()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    # --- Sovelletaan esikäsittelyä ja uudelleennimeämistä ---\n",
        "\n",
        "    # Säädata 1 (Kaisaniemi)\n",
        "    df_w1 = preprocess_fmi_data(df_w1_raw, \"Säädata 1 (Kaisaniemi)\")\n",
        "    if df_w1 is not None:\n",
        "        # Määritellään halutut uudet nimet (englanniksi) alkuperäisille suomenkielisille nimille.\n",
        "        # Käytä _W1-päätettä erottamaan mahdolliset päällekkäiset sarakkeet toisesta säädatatiedostosta.\n",
        "        # TÄRKEÄÄ: Tarkista, että avaimet (vasen puoli) vastaavat TARKALLEEN datassa olevia nimiä!\n",
        "        rename_map_w1 = {\n",
        "            'Ilman lämpötila keskiarvo [°C]': 'Temperature_W1',\n",
        "            'Tuulen suunta keskiarvo [°]': 'WindDirection', # Oletetaan tämä uniikiksi tiedostolle 1\n",
        "            'Keskituulen nopeus keskiarvo [m/s]': 'WindSpeed_W1',\n",
        "            'Näkyvyys keskiarvo [m]': 'Visibility',\n",
        "            'Pilvisyys [1/8]': 'Cloudiness', # Huom: Tämä saattaa puuttua, jos se oli tyhjä\n",
        "            'Ilmanpaine merenpinnan tasolla keskiarvo [hPa]': 'Pressure_SeaLevel'\n",
        "        }\n",
        "        # Suoritetaan uudelleennimeäminen vain niille sarakkeille, jotka löytyvät datasta.\n",
        "        valid_rename_map_w1 = {k: v for k, v in rename_map_w1.items() if k in df_w1.columns}\n",
        "        df_w1 = df_w1.rename(columns=valid_rename_map_w1)\n",
        "        print(\"  -> Säädata 1 uudelleennimetty. Sarakkeet:\", df_w1.columns.tolist())\n",
        "        print(\"     Esimerkkirivi:\\n\", df_w1.head(1).to_string()) # Tulostetaan 1 rivi luettavammin\n",
        "\n",
        "    # Säädata 2 (Kaisaniemi)\n",
        "    df_w2 = preprocess_fmi_data(df_w2_raw, \"Säädata 2 (Kaisaniemi)\")\n",
        "    if df_w2 is not None:\n",
        "        # Vastaava uudelleennimeäminen toiselle säädatatiedostolle (_W2-pääte).\n",
        "        rename_map_w2 = {\n",
        "            'Lämpötilan keskiarvo [°C]': 'Temperature_W2',\n",
        "            'Ylin lämpötila [°C]': 'Temperature_Max',\n",
        "            'Alin lämpötila [°C]': 'Temperature_Min',\n",
        "            'Keskituulen nopeus [m/s]': 'WindSpeed_W2',\n",
        "            'Tuulen suunnan keskiarvo [°]': 'WindDirection_W2',\n",
        "            'Ilmanpaineen keskiarvo [hPa]': 'Pressure_W2'\n",
        "        }\n",
        "        valid_rename_map_w2 = {k: v for k, v in rename_map_w2.items() if k in df_w2.columns}\n",
        "        df_w2 = df_w2.rename(columns=valid_rename_map_w2)\n",
        "        print(\"  -> Säädata 2 uudelleennimetty. Sarakkeet:\", df_w2.columns.tolist())\n",
        "        print(\"     Esimerkkirivi:\\n\", df_w2.head(1).to_string())\n",
        "\n",
        "    # Ilmanlaatu (Kallio 2)\n",
        "    df_aq = preprocess_fmi_data(df_aq_raw, \"Ilmanlaatu (Kallio 2)\")\n",
        "    if df_aq is not None:\n",
        "        # Määritellään, mitkä ilmanlaatusarakkeet halutaan mukaan ja millä nimillä.\n",
        "        aq_cols_to_keep_and_rename = {\n",
        "            'Otsoni [µg/m3]': 'Ozone', # TÄRKEIN: Kohdemuuttuja\n",
        "            'Hengitettävät hiukkaset <10 µm [µg/m3]': 'PM10',\n",
        "            'Pienhiukkaset <2.5 µm [µg/m3]': 'PM25',\n",
        "            'Typpidioksidi [µg/m3]': 'NO2',\n",
        "            'Typpimonoksidi [µg/m3]': 'NO',\n",
        "            'Hiilimonoksidi [µg/m3]': 'CO',\n",
        "            'Rikkidioksidi [µg/m3]': 'SO2',\n",
        "            'Musta hiili [µg/m3]': 'BlackCarbon'\n",
        "        }\n",
        "        # Valitaan vain ne sarakkeet, jotka löytyvät datasta\n",
        "        cols_to_select = [k for k in aq_cols_to_keep_and_rename.keys() if k in df_aq.columns]\n",
        "        # Erityinen tarkistus kohdemuuttujalle (Ozone)\n",
        "        if 'Otsoni [µg/m3]' not in cols_to_select:\n",
        "             print(\"  -> KRIITTINEN VIRHE: Otsonisaraketta 'Otsoni [µg/m3]' ei löytynyt ilmanlaatudatasta!\")\n",
        "             df_aq = None # Estetään jatko\n",
        "        else:\n",
        "            df_aq_filtered = df_aq[cols_to_select] # Valitaan halutut sarakkeet\n",
        "            valid_rename_map_aq = {k: v for k, v in aq_cols_to_keep_and_rename.items() if k in cols_to_select}\n",
        "            df_aq = df_aq_filtered.rename(columns=valid_rename_map_aq) # Nimetään ne uudelleen\n",
        "            print(\"  -> Ilmanlaatudata suodatettu ja uudelleennimetty. Sarakkeet:\", df_aq.columns.tolist())\n",
        "            print(\"     Esimerkkirivi:\\n\", df_aq.head(1).to_string())\n",
        "\n",
        "    #==============================================================================\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Vaihe 3: DataFramesien Yhdistäminen\")\n",
        "    print(\"=\"*60)\n",
        "    #==============================================================================\n",
        "\n",
        "    # Tarkistetaan taas, että kaikki tarvittavat DataFramet ovat valmiina yhdistämistä varten\n",
        "    if df_w1 is not None and df_w2 is not None and df_aq is not None:\n",
        "\n",
        "        # --- Yhdistetään ensin kaksi säädataa ---\n",
        "        print(\"\\nYhdistetään säädatat (df_w1 ja df_w2) indeksin perusteella...\")\n",
        "        # how='outer' säilyttää kaikki rivit molemmista. Jos aikaleima on vain toisessa,\n",
        "        # toisen sarakkeisiin tulee NaN. Jos sama aikaleima on molemmissa, ne yhdistyvät samalle riville.\n",
        "        df_weather_combined = pd.merge(df_w1, df_w2, left_index=True, right_index=True, how='outer', suffixes=('_Check1', '_Check2'))\n",
        "        print(f\"  -> Yhdistetyn säädatan muoto: {df_weather_combined.shape}\")\n",
        "        print(f\"  -> Sarakkeet: {df_weather_combined.columns.tolist()}\")\n",
        "        # Tarkistetaan, syntyikö yhdistämisessä päällekkäisiä sarakkeita (ei pitäisi, koska käytimme päätteitä)\n",
        "        if any('_Check' in col for col in df_weather_combined.columns):\n",
        "            print(\"  -> Varoitus: Yhdistämisessä syntyi päällekkäisiä sarakkeita? Tarkista sarakelistat.\")\n",
        "\n",
        "        # --- Yhdistetään yhdistetty säädata ilmanlaatutataan ---\n",
        "        print(\"\\nYhdistetään sää- ja ilmanlaatutata (df_aq) indeksin perusteella...\")\n",
        "        # Käytetään jälleen 'outer' mergeä, jotta kaikki uniikit ajankohdat säilyvät.\n",
        "        df_final = pd.merge(df_weather_combined, df_aq, left_index=True, right_index=True, how='outer', suffixes=('_Weather', '_AQ'))\n",
        "        print(f\"  -> Lopullisen yhdistetyn datan muoto: {df_final.shape}\")\n",
        "        print(f\"  -> Lopulliset sarakkeet: {df_final.columns.tolist()}\")\n",
        "\n",
        "        #==========================================================================\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\" Vaihe 4: Lopullinen Siivous ja NaN-arvojen Käsittely\")\n",
        "        print(\"=\"*60)\n",
        "        #==========================================================================\n",
        "\n",
        "        # --- 1. Aikajärjestys ---\n",
        "        # Varmistetaan, että data on aikajärjestyksessä, mikä on tärkeää aikasarjaoperaatioille.\n",
        "        df_final = df_final.sort_index()\n",
        "        print(\"\\nDataFrame lajiteltu aikaleiman ('Timestamp' indeksi) mukaan.\")\n",
        "\n",
        "        # --- 2. Duplikaatti-indeksien käsittely ---\n",
        "        # Tarkistetaan, onko sama aikaleima useamman kerran indeksissä.\n",
        "        if not df_final.index.is_unique:\n",
        "            num_duplicates = df_final.index.duplicated().sum()\n",
        "            print(f\"\\nVAROITUS: Löydettiin {num_duplicates} duplikaatti-indeksiä!\")\n",
        "            print(\"  -> Käsitellään duplikaatit laskemalla keskiarvo...\")\n",
        "            # Ryhmitellään indeksin mukaan ja lasketaan keskiarvo kustakin sarakkeesta\n",
        "            # Tämä yhdistää duplikaattirivit yhdeksi riviksi keskiarvoilla.\n",
        "            df_final = df_final.groupby(level=0).mean()\n",
        "            print(f\"  -> Duplikaatit käsitelty. DataFramen uusi muoto: {df_final.shape}\")\n",
        "            if not df_final.index.is_unique:\n",
        "                 print(\"  -> KRIITTINEN VIRHE: Duplikaatteja jäi vielä käsittelyn jälkeen!\")\n",
        "            else:\n",
        "                 print(\"  -> Duplikaatti-indeksit poistettu onnistuneesti.\")\n",
        "        else:\n",
        "            print(\"\\nIndeksi on uniikki, ei duplikaatteja.\")\n",
        "\n",
        "        # --- 3. Tasaisen tuntifrekvenssin varmistaminen ---\n",
        "        print(\"\\nVarmistetaan tasainen tuntifrekvenssi ('freq=h')...\")\n",
        "        if not df_final.empty:\n",
        "          min_ts, max_ts = df_final.index.min(), df_final.index.max()\n",
        "          print(f\"  -> Datan aika-alue: {min_ts} - {max_ts}\")\n",
        "          # Luodaan täydellinen tuntiperusteinen aikaindeksi datan alku- ja loppuhetken välille.\n",
        "          full_time_range = pd.date_range(start=min_ts, end=max_ts, freq='h')\n",
        "          print(f\"  -> Odotettu tuntien määrä tällä aikavälillä: {len(full_time_range)}\")\n",
        "          # Uudelleenindeksoidaan DataFrame tällä täydellisellä indeksillä.\n",
        "          # Mahdolliset puuttuvat tunnit lisätään NaN-arvoilla.\n",
        "          df_final = df_final.reindex(full_time_range)\n",
        "          print(f\"  -> DataFrame uudelleenindeksoitu. Uusi muoto: {df_final.shape}\")\n",
        "        else:\n",
        "          print(\"  -> DataFrame on tyhjä, ei voida uudelleenindeksoida.\")\n",
        "\n",
        "        # --- 4. Puuttuvien arvojen (NaN) käsittely ---\n",
        "        print(\"\\nKäsitellään puuttuvat arvot (NaN)...\")\n",
        "        if not df_final.empty:\n",
        "          nan_counts_before = df_final.isnull().sum()\n",
        "          print(\"  -> NaN-arvojen määrä / sarake ENNEN täyttöä:\")\n",
        "          print(nan_counts_before[nan_counts_before > 0].to_string())\n",
        "\n",
        "          # Käytetään ffill() (täyttö edellisellä arvolla) ja sen jälkeen bfill() (täyttö seuraavalla).\n",
        "          # ffill on yleinen aikasarjoille, koska se ei käytä tulevaisuuden tietoa.\n",
        "          # bfill lisätään täyttämään mahdolliset NaN:t datan ihan alussa.\n",
        "          # Huom: Tämä on yksinkertainen strategia. Monimutkaisempia (esim. interpolointi)\n",
        "          # voitaisiin harkita, mutta tämä on hyvä lähtökohta.\n",
        "          df_final = df_final.ffill().bfill()\n",
        "\n",
        "          nan_counts_after = df_final.isnull().sum().sum()\n",
        "          if nan_counts_after == 0:\n",
        "              print(\"\\n  -> Kaikki NaN-arvot täytetty onnistuneesti (ffill + bfill).\")\n",
        "          else:\n",
        "              print(f\"\\n  -> VAROITUS: {nan_counts_after} NaN-arvoa jäi täytön jälkeen! Tarkista data.\")\n",
        "              print(\"     Sarakkeet joissa NaN:\\n\", df_final.isnull().sum()[df_final.isnull().sum() > 0])\n",
        "        else:\n",
        "          print(\"  -> DataFrame on tyhjä, NaN-arvojen käsittelyä ei suoriteta.\")\n",
        "\n",
        "        # --- 5. Lopullinen tarkistus ja yhteenveto ---\n",
        "        if not df_final.empty:\n",
        "          print(\"\\n\" + \"-\"*40)\n",
        "          print(\" Lopullisen DataFramen Yhteenveto\")\n",
        "          print(\"-\"*40)\n",
        "          print(\"\\nEnsimmäiset 5 riviä:\")\n",
        "          print(df_final.head())\n",
        "          print(\"\\nViimeiset 5 riviä:\")\n",
        "          print(df_final.tail())\n",
        "          print(\"\\nPerustilastot (describe):\")\n",
        "          print(df_final.describe())\n",
        "          print(\"\\nInfo (sarakkeet, datatyypit, muisti):\")\n",
        "          df_final.info()\n",
        "        else:\n",
        "            print(\"\\nLopullinen DataFrame on tyhjä.\")\n",
        "\n",
        "        #==========================================================================\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\" Vaihe 5: Tallenna Käsitelty Data Parquet-muodossa (Valinnainen)\")\n",
        "        print(\"=\"*60)\n",
        "        #==========================================================================\n",
        "        # Tallennetaan lopputulos Parquet-tiedostoon. Parquet on tehokas, sarakepohjainen\n",
        "        # tallennusmuoto, joka säilyttää datatyypit hyvin ja on usein nopeampi lukea\n",
        "        # kuin CSV, erityisesti suurilla data-aineistoilla.\n",
        "        if not df_final.empty:\n",
        "          # Tarkistetaan, onko 'pyarrow'-kirjasto asennettu, sitä tarvitaan Parquet-kirjoitukseen.\n",
        "          try:\n",
        "              import pyarrow\n",
        "              print(\"\\n'pyarrow'-kirjasto löytyy. Parquet-tallennus jatkuu.\")\n",
        "              pyarrow_installed = True\n",
        "          except ImportError:\n",
        "              print(\"\\nVAROITUS: 'pyarrow'-kirjastoa ei löydy. Parquet-tallennus vaatii sen.\")\n",
        "              print(\"          Voit asentaa sen komentoriviltä komennolla: pip install pyarrow\")\n",
        "              print(\"          tai notebookissa: !pip install pyarrow\")\n",
        "              pyarrow_installed = False\n",
        "\n",
        "          if pyarrow_installed:\n",
        "              # Määritellään tallennushakemisto ja tiedostonimi.\n",
        "              output_folder = \"data/processed\"\n",
        "              output_filename = \"Helsinki_Kaisaniemi_Kallio_Combined_Processed_v2.parquet\" # Lisätty versio nimeen\n",
        "              output_path_parquet = os.path.join(output_folder, output_filename)\n",
        "\n",
        "              try:\n",
        "                  # Luodaan 'data/processed' -hakemisto, jos sitä ei ole olemassa.\n",
        "                  # exist_ok=True estää virheen, jos hakemisto on jo olemassa.\n",
        "                  os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "                  # Tallennetaan DataFrame Parquet-tiedostoon.\n",
        "                  # index=True varmistaa, että Timestamp-indeksi tallennetaan mukaan.\n",
        "                  df_final.to_parquet(output_path_parquet, index=True)\n",
        "                  print(f\"\\nKäsitelty data tallennettu Parquet-tiedostoon: {output_path_parquet}\")\n",
        "              except Exception as e:\n",
        "                  # Tulostetaan virhe, jos tallennus epäonnistuu.\n",
        "                  print(f\"\\nVIRHE tallennettaessa käsiteltyä dataa Parquet-muodossa: {e}\")\n",
        "          else:\n",
        "              print(\"\\nParquet-tallennusta ei suoritettu, koska 'pyarrow'-kirjasto puuttuu.\")\n",
        "\n",
        "    else: # Tämä else-haara ajetaan, jos jokin df_w1, df_w2 tai df_aq oli None yhdistämisvaiheessa\n",
        "        print(\"\\nYhdistämistä tai lopullista siivousta ei voitu suorittaa, koska yksi tai useampi DataFrame puuttui.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Skriptin suoritus päättyi\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Tämä else-haara ajetaan, jos alkuperäinen datan lataus epäonnistui\n",
        "elif not data_loaded_successfully:\n",
        "    print(\"\\nSkriptin suoritus keskeytettiin datan latausvirheen vuoksi.\")"
      ],
      "metadata": {
        "id": "aY-ATViSRCHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}