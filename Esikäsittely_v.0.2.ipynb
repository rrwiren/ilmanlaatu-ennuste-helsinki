{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXg6bNIbKO/9UUywAtD1cY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrwiren/ilmanlaatu-ennuste-helsinki/blob/main/Esik%C3%A4sittely_v.0.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "==========================================================================================\n",
        " PIPELINE DATAN ESIKÄSITTELYYN JA TUTKIMISEEN - HELSINGIN SÄÄ- JA ILMANLAATUDATA (FMI)\n",
        "==========================================================================================\n",
        "\n",
        " Versio: 0.2 (Lisätty EDA-funktio)\n",
        " Päivitetty: 2025-04-11 23:36 (EEST)\n",
        " Edellinen versio: 0.1 (Refaktorointi funktioiksi)\n",
        "\n",
        " Skriptin tarkoitus:\n",
        " --------------------\n",
        " Tämä skripti muodostaa data pipelinen, joka:\n",
        " 1. Lataa raakadatatiedostot (FMI:n sää- ja ilmanlaatudata).\n",
        " 2. Esikäsittelee ja yhdistää datan yhdeksi DataFrameksi.\n",
        " 3. Suorittaa eksploratiivista data-analyysiä (EDA) käsitellylle datalle:\n",
        "    - Käsittelee päällekkäiset sarakkeet.\n",
        "    - Korjaa epärealistiset negatiiviset pitoisuudet.\n",
        "    - Visualisoi kohdemuuttujaa ('Ozone') ja sen jakaumaa.\n",
        "    - Analysoi korrelaatioita ja riippuvuuksia säämuuttujien kanssa.\n",
        "    - Tutkii ajallista vaihtelua.\n",
        " 4. (Valinnaisesti) Tallentaa lopullisen, EDA:n jälkeen siivotun DataFramen\n",
        "    Parquet-muotoon.\n",
        "\n",
        " Vaaditut kirjastot:\n",
        " -------------------\n",
        " - pandas, numpy, requests, io, os, matplotlib, seaborn\n",
        " - pyarrow (Parquet-tallennukseen/lukuun)\n",
        "\n",
        " Käyttö:\n",
        " -------\n",
        " 1. Varmista, että vaaditut kirjastot on asennettu.\n",
        " 2. Aja skripti. Se lataa, esikäsittelee ja tutkii datan. Tulosteet ja kuvaajat\n",
        "    näytetään suorituksen aikana.\n",
        " 3. Jos haluat tallentaa lopputuloksen, varmista, että 'pyarrow' on asennettu\n",
        "    ja poista kommenttimerkit Vaiheen 5 tallennuskoodista pääsuorituslohkossa.\n",
        "\"\"\"\n",
        "\n",
        "# === Vaihe 1: Alustus ja Kirjastojen Tuonti ===\n",
        "print(\"=\"*60)\n",
        "print(\" Vaihe 1: Alustus ja Kirjastojen Tuonti\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt # Tuodaan visualisointikirjastot\n",
        "import seaborn as sns\n",
        "\n",
        "# Asetetaan visualisointien tyyli jo tässä vaiheessa\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Määritellään datatiedostojen URL-osoitteet\n",
        "DATA_URLS = {\n",
        "    \"weather1\": \"https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/data/raw/Helsinki%20Kaisaniemi_%2011.4.2023%20-%2011.4.2025_4c0a0316-74e0-4792-9854-fd6315fbc965.csv\",\n",
        "    \"weather2\": \"https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/data/raw/Helsinki%20Kaisaniemi_%2011.4.2023%20-%2011.4.2025_84370efb-e7a0-48ed-b991-0432c84c1475.csv\",\n",
        "    \"aq\": \"https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/data/raw/Helsinki%20Kallio%202_%2011.4.2023%20-%2011.4.2025_8bbe3500-d31d-459d-ac37-16b4bd64a2cc.csv\"\n",
        "}\n",
        "# Määritellään tallennuspolku ja -nimi käsitellylle datalle\n",
        "PROCESSED_DATA_FOLDER = \"data/processed\"\n",
        "PROCESSED_DATA_FILENAME = \"Helsinki_Kaisaniemi_Kallio_Combined_Processed_Pipeline_v0.2.parquet\"\n",
        "PROCESSED_DATA_PATH = os.path.join(PROCESSED_DATA_FOLDER, PROCESSED_DATA_FILENAME)\n",
        "\n",
        "\n",
        "# === Apufunktiot (Lataus & Esikäsittely) ===\n",
        "\n",
        "def _load_single_csv_from_url(url, key_name):\n",
        "    \"\"\"Apufunktio: Lataa YHDEN CSV-tiedoston URL-osoitteesta.\"\"\"\n",
        "    print(f\"  Yritetään ladata {key_name}: {url[:70]}...\")\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        csv_data = io.StringIO(response.text)\n",
        "        df = pd.read_csv(csv_data, sep=',')\n",
        "        print(f\"    -> {key_name} luettu onnistuneesti.\")\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"    -> VIRHE LADATESSA ({key_name}): {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"    -> VIRHE LUETTAESSA CSV ({key_name}): {e}\")\n",
        "        return None\n",
        "\n",
        "def _preprocess_single_fmi_df(df, data_key_name):\n",
        "    \"\"\"Apufunktio: Esikäsittelee YHDEN FMI DataFrame:n (aika, numerot).\"\"\"\n",
        "    if df is None: return None\n",
        "    print(f\"  Esikäsitellään (aika, numerot): {data_key_name}\")\n",
        "    original_cols = df.columns.tolist()\n",
        "    date_cols = ['Vuosi', 'Kuukausi', 'Päivä']\n",
        "    time_col = 'Aika [Paikallinen aika]'\n",
        "    if all(col in original_cols for col in date_cols) and time_col in original_cols:\n",
        "        try:\n",
        "            datetime_str_series = df['Vuosi'].astype(str) + '-' + \\\n",
        "                                  df['Kuukausi'].astype(str).str.zfill(2) + '-' + \\\n",
        "                                  df['Päivä'].astype(str).str.zfill(2) + ' ' + \\\n",
        "                                  df[time_col].astype(str)\n",
        "            df['Timestamp'] = pd.to_datetime(datetime_str_series)\n",
        "            cols_to_drop = date_cols + [time_col]\n",
        "            if 'Havaintoasema' in original_cols: cols_to_drop.append('Havaintoasema')\n",
        "            df = df.drop(columns=cols_to_drop, errors='ignore').set_index('Timestamp')\n",
        "        except Exception as e:\n",
        "            print(f\"    -> VIRHE aikaleiman muodostamisessa ({data_key_name}): {e}.\")\n",
        "            return None\n",
        "    else:\n",
        "        missing_cols = [col for col in date_cols + [time_col] if col not in original_cols]\n",
        "        print(f\"    -> VIRHE: Aikaleimasarakkeita ei löytynyt ({data_key_name}). Puuttuvat: {missing_cols}\")\n",
        "        return None\n",
        "    print(f\"    -> Aikaleima & indeksi OK ({data_key_name}).\")\n",
        "    print(f\"    -> Muunnetaan numeeriseksi ({data_key_name})...\")\n",
        "    for col in df.columns:\n",
        "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "    print(f\"    -> Esikäsittely valmis ({data_key_name}). Jäljellä sarakkeet: {df.columns.tolist()}\")\n",
        "    return df\n",
        "\n",
        "# === Pipeline Vaihe 1: Datan Lataus ===\n",
        "def lataa_raakadata(url_dict):\n",
        "    \"\"\"Lataa raakadatatiedostot määritellyistä URL-osoitteista.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Pipeline Vaihe 1: Datan Lataus\")\n",
        "    print(\"=\"*60)\n",
        "    raw_dataframes = {}\n",
        "    success = True\n",
        "    for key, url in url_dict.items():\n",
        "        df_raw = _load_single_csv_from_url(url, key)\n",
        "        if df_raw is None: success = False\n",
        "        raw_dataframes[key] = df_raw\n",
        "    if not success:\n",
        "        print(\"\\nVAROITUS: Kaikkien datatiedostojen lataus ei onnistunut.\")\n",
        "        return None\n",
        "    print(\"\\nKaikki datatiedostot ladattu onnistuneesti sanakirjaan.\")\n",
        "    return raw_dataframes\n",
        "\n",
        "# === Pipeline Vaihe 2: Datan Esikäsittely ja Yhdistäminen ===\n",
        "def esikasittele_yhdistetty_data(raw_dfs_dict):\n",
        "    \"\"\"Yhdistää ja esikäsittelee ladatut raakadatan DataFramet.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Pipeline Vaihe 2: Datan Esikäsittely ja Yhdistäminen\")\n",
        "    print(\"=\"*60)\n",
        "    required_keys = ['weather1', 'weather2', 'aq']\n",
        "    if not isinstance(raw_dfs_dict, dict) or not all(key in raw_dfs_dict for key in required_keys):\n",
        "        print(\"VIRHE: Syöte 'raw_dfs_dict' ei ole kelvollinen.\")\n",
        "        return None\n",
        "\n",
        "    # --- 1. Esikäsittele ---\n",
        "    df_w1 = _preprocess_single_fmi_df(raw_dfs_dict.get('weather1'), 'weather1')\n",
        "    df_w2 = _preprocess_single_fmi_df(raw_dfs_dict.get('weather2'), 'weather2')\n",
        "    df_aq = _preprocess_single_fmi_df(raw_dfs_dict.get('aq'), 'aq')\n",
        "    if df_w1 is None or df_w2 is None or df_aq is None:\n",
        "        print(\"VIRHE: Kaikkien DataFramejen alustava esikäsittely ei onnistunut.\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. Nimeä uudelleen ---\n",
        "    print(\"\\nUudelleennimetään sarakkeita...\")\n",
        "    rename_map_w1 = {'Ilman lämpötila keskiarvo [°C]': 'Temperature_W1','Tuulen suunta keskiarvo [°]': 'WindDirection_W1', 'Keskituulen nopeus keskiarvo [m/s]': 'WindSpeed_W1','Näkyvyys keskiarvo [m]': 'Visibility','Pilvisyys [1/8]': 'Cloudiness', 'Ilmanpaine merenpinnan tasolla keskiarvo [hPa]': 'Pressure_SeaLevel'}\n",
        "    valid_rename_map_w1 = {k: v for k, v in rename_map_w1.items() if k in df_w1.columns}\n",
        "    df_w1 = df_w1.rename(columns=valid_rename_map_w1)\n",
        "    print(f\"  -> weather1 sarakkeet: {df_w1.columns.tolist()}\")\n",
        "\n",
        "    rename_map_w2 = {'Lämpötilan keskiarvo [°C]': 'Temperature_W2', 'Ylin lämpötila [°C]': 'Temperature_Max','Alin lämpötila [°C]': 'Temperature_Min', 'Keskituulen nopeus [m/s]': 'WindSpeed_W2','Tuulen suunnan keskiarvo [°]': 'WindDirection_W2','Ilmanpaineen keskiarvo [hPa]': 'Pressure_W2'}\n",
        "    valid_rename_map_w2 = {k: v for k, v in rename_map_w2.items() if k in df_w2.columns}\n",
        "    df_w2 = df_w2.rename(columns=valid_rename_map_w2)\n",
        "    print(f\"  -> weather2 sarakkeet: {df_w2.columns.tolist()}\")\n",
        "\n",
        "    aq_cols_to_keep = {'Otsoni [µg/m3]': 'Ozone', 'Hengitettävät hiukkaset <10 µm [µg/m3]': 'PM10','Pienhiukkaset <2.5 µm [µg/m3]': 'PM25','Typpidioksidi [µg/m3]': 'NO2', 'Typpimonoksidi [µg/m3]': 'NO', 'Hiilimonoksidi [µg/m3]': 'CO', 'Rikkidioksidi [µg/m3]': 'SO2', 'Musta hiili [µg/m3]': 'BlackCarbon'}\n",
        "    cols_to_select = [k for k in aq_cols_to_keep.keys() if k in df_aq.columns]\n",
        "    if 'Otsoni [µg/m3]' not in cols_to_select:\n",
        "         print(\"  -> KRIITTINEN VIRHE: Otsonisaraketta ei löytynyt!\")\n",
        "         return None\n",
        "    valid_rename_map_aq = {k: v for k, v in aq_cols_to_keep.items() if k in cols_to_select}\n",
        "    df_aq = df_aq[cols_to_select].rename(columns=valid_rename_map_aq)\n",
        "    print(f\"  -> aq sarakkeet: {df_aq.columns.tolist()}\")\n",
        "\n",
        "    # --- 3. Yhdistä ---\n",
        "    print(\"\\nYhdistetään DataFrameja...\")\n",
        "    df_weather_combined = pd.merge(df_w1, df_w2, left_index=True, right_index=True, how='outer')\n",
        "    df_final = pd.merge(df_weather_combined, df_aq, left_index=True, right_index=True, how='outer')\n",
        "    print(f\"  -> Yhdistetty. Muoto: {df_final.shape}\")\n",
        "\n",
        "    # --- 4. Siivoa ---\n",
        "    print(\"\\nSuoritetaan lopullinen siivous (lajittelu, duplikaatit, reindex, NaN)...\")\n",
        "    df_final = df_final.sort_index()\n",
        "    if not df_final.index.is_unique:\n",
        "        num_duplicates = df_final.index.duplicated().sum()\n",
        "        print(f\"  -> Käsitellään {num_duplicates} duplikaatti-indeksiä keskiarvolla...\")\n",
        "        df_final = df_final.groupby(level=0).mean()\n",
        "        if df_final.index.has_duplicates: print(\"  -> VIRHE: Duplikaatteja jäi!\")\n",
        "        else: print(\"  -> Duplikaatit käsitelty.\")\n",
        "    else: print(\"  -> Ei duplikaatti-indeksejä.\")\n",
        "\n",
        "    if not df_final.empty:\n",
        "        print(\"  -> Varmistetaan tasainen tuntifrekvenssi (reindex)...\")\n",
        "        min_ts, max_ts = df_final.index.min(), df_final.index.max()\n",
        "        full_time_range = pd.date_range(start=min_ts, end=max_ts, freq='h')\n",
        "        df_final = df_final.reindex(full_time_range)\n",
        "        print(f\"     -> Uusi muoto reindexin jälkeen: {df_final.shape}\")\n",
        "    else: return None\n",
        "\n",
        "    print(\"  -> Täytetään NaN-arvot (ffill + bfill)...\")\n",
        "    nan_before = df_final.isnull().sum().sum()\n",
        "    df_final = df_final.ffill().bfill()\n",
        "    nan_after = df_final.isnull().sum().sum()\n",
        "    if nan_after == 0: print(f\"     -> NaN-arvot täytetty (alkuperäinen määrä: {nan_before}).\")\n",
        "    else: print(f\"     -> VAROITUS: {nan_after} NaN jäi täytön jälkeen!\")\n",
        "\n",
        "    print(\"\\nEsikäsittely ja yhdistäminen valmis.\")\n",
        "    return df_final\n",
        "\n",
        "# === Pipeline Vaihe 3: Datan Tutkiminen (EDA) ===\n",
        "def tutki_dataa(df):\n",
        "    \"\"\"\n",
        "    Suorittaa eksploratiivista data-analyysiä (EDA) esikäsitellylle DataFramelle.\n",
        "\n",
        "    Toimenpiteet:\n",
        "    - Päällekkäisten sääsarakkeiden käsittely (keskiarvoistus).\n",
        "    - Negatiivisten pitoisuuksien korjaus nollaksi.\n",
        "    - Kohdemuuttujan ('Ozone') perustilastojen ja visualisointien tulostus.\n",
        "    - Korrelaatioanalyysi ja visualisointi (Otsoni vs. Sää).\n",
        "    - Hajontakaavioiden piirto (Otsoni vs. valitut säämuuttujat).\n",
        "    - Ajallisen vaihtelun analyysi ja visualisointi (kuukausi, tunti).\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): Esikäsitelty DataFrame (esim. esikasittele_yhdistetty_data-funktion palauttama).\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame or None: DataFrame, jossa päällekkäiset sarakkeet ja\n",
        "                                   negatiiviset arvot on käsitelty, tai None jos syöte puuttui.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Pipeline Vaihe 3: Datan Tutkiminen (EDA)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        print(\"VIRHE: Ei dataa tutkittavaksi (syöte-DataFrame on None tai tyhjä).\")\n",
        "        return None\n",
        "\n",
        "    # Tehdään kopio, jotta alkuperäinen DataFrame ei muutu tämän funktion ulkopuolella\n",
        "    df_eda = df.copy()\n",
        "\n",
        "    # --- 3a: Päällekkäisten Sääsarakkeiden Käsittely ---\n",
        "    print(\"\\n--- 3a: Päällekkäisten Sääsarakkeiden Käsittely ---\")\n",
        "    duplicate_pairs = {\n",
        "        'Temperature': ('Temperature_W1', 'Temperature_W2'),\n",
        "        'WindSpeed': ('WindSpeed_W1', 'WindSpeed_W2'),\n",
        "        'WindDirection_Avg': ('WindDirection_W1', 'WindDirection_W2'), # Korjattu nimi W1:lle\n",
        "        'Pressure': ('Pressure_SeaLevel', 'Pressure_W2')\n",
        "    }\n",
        "    columns_to_drop = []\n",
        "    for new_col_name, (col1, col2) in duplicate_pairs.items():\n",
        "        if col1 in df_eda.columns and col2 in df_eda.columns:\n",
        "            print(f\"  Käsitellään pari: {col1} & {col2} -> {new_col_name}\")\n",
        "            if new_col_name == 'WindDirection_Avg':\n",
        "                 rad1 = np.deg2rad(df_eda[col1])\n",
        "                 rad2 = np.deg2rad(df_eda[col2])\n",
        "                 x_avg = (np.cos(rad1) + np.cos(rad2)) / 2.0\n",
        "                 y_avg = (np.sin(rad1) + np.sin(rad2)) / 2.0\n",
        "                 avg_rad = np.arctan2(y_avg, x_avg)\n",
        "                 avg_deg = np.rad2deg(avg_rad)\n",
        "                 df_eda[new_col_name] = (avg_deg + 360) % 360\n",
        "                 print(\"    -> Vektorikeskiarvo laskettu tuulen suunnalle.\")\n",
        "            else:\n",
        "                 df_eda[new_col_name] = df_eda[[col1, col2]].mean(axis=1)\n",
        "            columns_to_drop.extend([col1, col2])\n",
        "        else:\n",
        "            missing = [c for c in [col1, col2] if c not in df_eda.columns]\n",
        "            print(f\"  -> HUOM: Paria {col1}/{col2} ei voitu käsitellä. Puuttuu: {missing}\")\n",
        "\n",
        "    columns_to_drop_unique = list(set(columns_to_drop))\n",
        "    columns_to_drop_final = [col for col in columns_to_drop_unique if col in df_eda.columns]\n",
        "    if columns_to_drop_final:\n",
        "        df_eda = df_eda.drop(columns=columns_to_drop_final)\n",
        "        print(f\"  -> Poistettu alkuperäiset: {columns_to_drop_final}\")\n",
        "    if 'WindDirection_Avg' in df_eda.columns: # Nimetään tuulen suunta lopullisesti\n",
        "        df_eda = df_eda.rename(columns={'WindDirection_Avg': 'WindDirection'})\n",
        "        print(\"  -> Sarake 'WindDirection_Avg' nimetty 'WindDirection'.\")\n",
        "    print(\"  -> Päällekkäisten sarakkeiden käsittely valmis.\")\n",
        "    print(\"     Nykyiset sarakkeet:\", df_eda.columns.tolist())\n",
        "\n",
        "\n",
        "    # --- 3b: Negatiivisten Pitoisuuksien Käsittely ---\n",
        "    print(\"\\n--- 3b: Negatiivisten Pitoisuuksien Tarkistus ja Korjaus ---\")\n",
        "    concentration_cols = ['Ozone', 'PM10', 'PM25', 'NO2', 'NO', 'SO2', 'BlackCarbon', 'CO']\n",
        "    cols_to_check = [col for col in concentration_cols if col in df_eda.columns]\n",
        "    negative_counts = (df_eda[cols_to_check] < 0).sum()\n",
        "    if (negative_counts > 0).any():\n",
        "        print(\"  Korjataan negatiiviset pitoisuudet nollaksi...\")\n",
        "        print(\"  Negatiivisten määrät ennen korjausta:\\n\", negative_counts[negative_counts > 0].to_string())\n",
        "        for col in cols_to_check:\n",
        "            if negative_counts.get(col, 0) > 0: # Käytä get() jos sarake puuttuu\n",
        "                df_eda[col] = df_eda[col].clip(lower=0)\n",
        "        print(\"  -> Negatiiviset pitoisuudet korjattu.\")\n",
        "    else:\n",
        "        print(\"  -> Ei negatiivisia pitoisuuksia havaittu.\")\n",
        "\n",
        "    # --- 3c: Kohdemuuttujan (Ozone) Analyysi ---\n",
        "    print(\"\\n--- 3c: Kohdemuuttujan (Ozone) Alustava Analyysi ---\")\n",
        "    target_col = 'Ozone'\n",
        "    if target_col not in df_eda.columns:\n",
        "        print(f\"VIRHE: Kohdemuuttujaa '{target_col}' ei löydy! Keskeytetään EDA.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"  Perustilastot '{target_col}':\")\n",
        "    print(df_eda[target_col].describe().to_string())\n",
        "\n",
        "    print(f\"  Visualisoidaan '{target_col}' aikasarja...\")\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    df_eda[target_col].plot(alpha=0.8, title='Otsonipitoisuus (Ozone) ajan funktiona')\n",
        "    plt.ylabel('Otsoni (µg/m³)')\n",
        "    plt.xlabel('Aika')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(f\"  Visualisoidaan '{target_col}' jakauma...\")\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(df_eda[target_col], kde=True, bins=50)\n",
        "    plt.title(f'{target_col}-pitoisuuksien jakauma'); plt.xlabel('Otsoni (µg/m³)')\n",
        "    plt.ylabel('Frekvenssi / Tiheys'); plt.grid(True, axis='y')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # --- 3d: Korrelaatioanalyysi (Otsoni vs. Sää) ---\n",
        "    print(\"\\n--- 3d: Korrelaatioanalyysi (Otsoni vs. Sää) ---\")\n",
        "    weather_cols_for_corr = ['Ozone', 'Temperature', 'WindSpeed', 'WindDirection', 'Pressure', 'Visibility', 'Temperature_Max', 'Temperature_Min']\n",
        "    weather_cols_for_corr = [col for col in weather_cols_for_corr if col in df_eda.columns]\n",
        "    print(f\"  Lasketaan korrelaatiot sarakkeille: {weather_cols_for_corr}\")\n",
        "    correlation_matrix = df_eda[weather_cols_for_corr].corr()\n",
        "    plt.figure(figsize=(9, 7))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "    plt.title('Korrelaatiomatriisi: Otsoni vs. Säämuuttujat'); plt.show()\n",
        "    print(\"\\n  Otsonin korrelaatiot muihin (laskevassa järj.):\")\n",
        "    print(correlation_matrix['Ozone'].sort_values(ascending=False).drop('Ozone').to_string())\n",
        "\n",
        "    # --- 3e: Hajontakaaviot (Otsoni vs. Sää) ---\n",
        "    print(\"\\n--- 3e: Hajontakaaviot (Otsoni vs. Valitut Säämuuttujat) ---\")\n",
        "    scatter_features = ['Temperature', 'WindSpeed', 'Visibility', 'Pressure']\n",
        "    scatter_features = [col for col in scatter_features if col in df_eda.columns]\n",
        "    print(f\"  Piirretään hajontakaaviot: Ozone vs {scatter_features}\")\n",
        "    num_features = len(scatter_features)\n",
        "    num_rows = (num_features + 1) // 2\n",
        "    plt.figure(figsize=(14, 5 * num_rows))\n",
        "    for i, feature in enumerate(scatter_features):\n",
        "        plt.subplot(num_rows, 2, i + 1)\n",
        "        sns.scatterplot(data=df_eda, x=feature, y='Ozone', alpha=0.1, s=5)\n",
        "        plt.title(f'Otsoni vs. {feature}'); plt.xlabel(feature); plt.ylabel('Otsoni (µg/m³)')\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # --- 3f: Ajallinen Analyysi ---\n",
        "    print(\"\\n--- 3f: Ajallinen Analyysi (Kausi- ja Vuorokausivaihtelu) ---\")\n",
        "    print(\"  Visualisoidaan keskimääräinen kuukausivaihtelu...\")\n",
        "    monthly_avg = df_eda.groupby(df_eda.index.month)[['Ozone', 'Temperature']].mean()\n",
        "    monthly_avg.index.name = 'Kuukausi'\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "    ax2 = ax1.twinx()\n",
        "    monthly_avg['Ozone'].plot(ax=ax1, color='tab:red', marker='o', label='Otsoni')\n",
        "    monthly_avg['Temperature'].plot(ax=ax2, color='tab:blue', marker='s', linestyle='--', label='Lämpötila')\n",
        "    ax1.set_ylabel('Keskim. Otsoni (µg/m³)', color='tab:red'); ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "    ax2.set_ylabel('Keskim. Lämpötila (°C)', color='tab:blue'); ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "    ax1.set_xlabel('Kuukausi'); ax1.set_title('Keskimääräinen kuukausivaihtelu'); ax1.grid(True)\n",
        "    ax1.set_xticks(monthly_avg.index); fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
        "    fig.tight_layout(); plt.show()\n",
        "\n",
        "    print(\"  Visualisoidaan keskimääräinen tuntivaihtelu...\")\n",
        "    hourly_avg = df_eda.groupby(df_eda.index.hour)[['Ozone', 'Temperature']].mean()\n",
        "    hourly_avg.index.name = 'Tunti'\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "    ax2 = ax1.twinx()\n",
        "    hourly_avg['Ozone'].plot(ax=ax1, color='tab:red', marker='o', label='Otsoni')\n",
        "    hourly_avg['Temperature'].plot(ax=ax2, color='tab:blue', marker='s', linestyle='--', label='Lämpötila')\n",
        "    ax1.set_ylabel('Keskim. Otsoni (µg/m³)', color='tab:red'); ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "    ax2.set_ylabel('Keskim. Lämpötila (°C)', color='tab:blue'); ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "    ax1.set_xlabel('Tunti vuorokaudessa'); ax1.set_title('Keskimääräinen tuntivaihtelu'); ax1.grid(True)\n",
        "    ax1.set_xticks(hourly_avg.index[::2]); fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
        "    fig.tight_layout(); plt.show()\n",
        "\n",
        "    print(\"\\nEDA-vaihe suoritettu.\")\n",
        "    # Palautetaan DataFrame, jossa päällekkäiset sarakkeet ja negatiiviset arvot on käsitelty\n",
        "    return df_eda\n",
        "\n",
        "# === Pipeline Vaihe 4: Tallenna Käsitelty Data (Valinnainen) ===\n",
        "# HUOM: Nimeä tallennusfunktio uudelleen, koska se tallentaa nyt EDA:n jälkeisen datan\n",
        "def tallenna_eda_jalkeen_data(df, polku):\n",
        "    \"\"\"Tallentaa annetun DataFramen Parquet-tiedostoon EDA-vaiheen jälkeen.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" Pipeline Vaihe 4: Datan Tallennus EDA:n Jälkeen (Parquet)\")\n",
        "    print(\"=\"*60)\n",
        "    if df is None or df.empty:\n",
        "        print(\"VIRHE: Ei dataa tallennettavaksi.\")\n",
        "        return False\n",
        "    try:\n",
        "        import pyarrow\n",
        "        print(\"'pyarrow'-kirjasto löytyy.\")\n",
        "    except ImportError:\n",
        "        print(\"VAROITUS: 'pyarrow' puuttuu. Asenna: pip install pyarrow\")\n",
        "        return False\n",
        "    try:\n",
        "        output_folder = os.path.dirname(polku)\n",
        "        if output_folder: os.makedirs(output_folder, exist_ok=True)\n",
        "        df.to_parquet(polku, index=True)\n",
        "        print(f\"\\nEDA:n jälkeinen data tallennettu Parquet-tiedostoon: {polku}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nVIRHE tallennettaessa EDA:n jälkeistä dataa Parquet-muodossa tiedostoon {polku}: {e}\")\n",
        "        return False\n",
        "\n",
        "# === Pääsuorituslohko ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" ALOITETAAN DATAN ESIKÄSITTELY- JA EDA-PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Lataa raakadata\n",
        "    raaka_datat_dict = lataa_raakadata(DATA_URLS)\n",
        "\n",
        "    # 2. Esikäsittele ja yhdistä data\n",
        "    df_kasitelty = None\n",
        "    if raaka_datat_dict:\n",
        "        df_kasitelty = esikasittele_yhdistetty_data(raaka_datat_dict)\n",
        "\n",
        "    # 3. Suorita EDA (jos esikäsittely onnistui)\n",
        "    df_eda_valmis = None\n",
        "    if df_kasitelty is not None:\n",
        "        df_eda_valmis = tutki_dataa(df_kasitelty) # Kutsutaan uutta EDA-funktiota\n",
        "    else:\n",
        "        print(\"\\nPipeline keskeytyi, koska datan esikäsittely epäonnistui.\")\n",
        "\n",
        "    # 4. Tulosta yhteenveto EDA:n jälkeisestä datasta (jos EDA onnistui)\n",
        "    if df_eda_valmis is not None:\n",
        "        print(\"\\n\" + \"-\"*40)\n",
        "        print(\" EDA:N JÄLKEISEN DATAN YHTEENVETO \")\n",
        "        print(\"-\"*40)\n",
        "        print(\"\\nInfo:\")\n",
        "        df_eda_valmis.info()\n",
        "        print(\"\\n5 viimeistä riviä:\") # Vaihdettu head -> tail\n",
        "        print(df_eda_valmis.tail())\n",
        "\n",
        "        # 5. Tallenna EDA:n jälkeinen data (Valinnainen)\n",
        "        # Voit nyt tallentaa tämän df_eda_valmis DataFramen\n",
        "        tallennettu_ok = tallenna_eda_jalkeen_data(df_eda_valmis, PROCESSED_DATA_PATH)\n",
        "        if tallennettu_ok: print(\" -> Tallennus onnistui.\")\n",
        "        else: print(\" -> Tallennus ei onnistunut tai sitä ei suoritettu.\")\n",
        "    else:\n",
        "         print(\"\\nPipeline keskeytyi ennen EDA-vaiheen päättymistä tai se epäonnistui.\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" PIPELINE SUORITETTU LOPPUUN\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "pppKmfF7krXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}