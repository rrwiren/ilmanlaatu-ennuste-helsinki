{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn/TR1PQunqXPYdZgea+gq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrwiren/ilmanlaatu-ennuste-helsinki/blob/main/GRU_pilkottu_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGmD_FU7uxLX"
      },
      "outputs": [],
      "source": [
        "# @title 0. Esitiedot ja Tavoite (GRU v3 - Uusi Data)\n",
        "\"\"\"\n",
        "Otsoniennuste Helsinki - GRU-malli v3 (Uudella Datalla)\n",
        "\n",
        "Tavoite:\n",
        "1. Ladata uusi esikäsitelty data Parquet-tiedostosta, joka sisältää nyt\n",
        "   myös pilvisyyden ('processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet').\n",
        "2. Esikäsitellä data neuroverkkomallille sopivaksi:\n",
        "   - Ominaisuuksien muokkaus (sykliset aika- ja tuulipiirteet).\n",
        "   - Datan jako harjoitus-, validointi- ja testijoukkoihin.\n",
        "   - Datan skaalaus.\n",
        "   - Sekvenssien luonti.\n",
        "3. Määritellä ja kouluttaa GRU (Gated Recurrent Unit) -malli PyTorchilla.\n",
        "4. Tehdä ennusteita testijaksolle (seuraavat 24 tuntia).\n",
        "5. Arvioida mallin suorituskykyä monipuolisesti (RMSE, MAE, baseline-vertailu,\n",
        "   8h varoitusrajan metriikat) ja analysoida tuloksia.\n",
        "6. Visualisoida ennusteita.\n",
        "\"\"\"\n",
        "print(\"Osa 0: Esitiedot (GRU v3) - OK\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Tuonnit ja Asetukset (GRU v3)\n",
        "\n",
        "# Peruskirjastot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import traceback\n",
        "import warnings\n",
        "\n",
        "# Sklearn esikäsittelyyn ja metriikoihin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report\n",
        "\n",
        "# PyTorch kirjastot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Edistymispalkki\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Data-asetukset ---\n",
        "BASE_GITHUB_URL = 'https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/'\n",
        "# *** TÄRKEÄÄ: Käytetään uutta Parquet-tiedostoa ***\n",
        "PARQUET_PATH = 'data/processed/processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet'\n",
        "DATA_URL = BASE_GITHUB_URL + PARQUET_PATH\n",
        "LOCAL_DATA_PATH = 'processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet' # Paikallinen nimi lataukselle\n",
        "\n",
        "# --- Sarakemääritykset ---\n",
        "TARGET_COLUMN = 'Otsoni [µg/m³]'\n",
        "\n",
        "# Sarakkeet, jotka luetaan Parquet-tiedostosta ja käytetään *ennen* feature engineeringiä\n",
        "# Varmista, että nämä vastaavat Parquet-tiedoston sarakkeita\n",
        "ALL_COLUMNS_IN_PARQUET = [\n",
        "    'Otsoni [µg/m³]',\n",
        "    'Lämpötilan keskiarvo [°C]',\n",
        "    'Keskituulen nopeus [m/s]',\n",
        "    'Ilmanpaineen keskiarvo [hPa]',\n",
        "    'Tuulen suunnan keskiarvo [°]',\n",
        "    'Pilvisyys [okta]'\n",
        "]\n",
        "# Lopulliset ominaisuudet mallille määritetään feature engineeringin jälkeen\n",
        "\n",
        "# --- Mallinnusasetukset ---\n",
        "# Aikasarjaparametrit\n",
        "SEQUENCE_LENGTH = 72      # Kuinka monta tuntia historiaa syötteenä (3 vrk)\n",
        "PREDICTION_HORIZON = 24 # Kuinka monta tuntia ennustetaan\n",
        "\n",
        "# GRU-mallin Hyperparametrit\n",
        "# INPUT_SIZE lasketaan myöhemmin feature engineeringin jälkeen\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "OUTPUT_SIZE = PREDICTION_HORIZON # Ennustetaan 24h kerralla\n",
        "DROPOUT_PROB = 0.2\n",
        "\n",
        "# --- Koulutusparametrit ---\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 75  # Kasvatetaan hieman epochien määrää neuroverkolle\n",
        "TEST_SPLIT_RATIO = 0.15\n",
        "VALID_SPLIT_RATIO = 0.15\n",
        "EARLY_STOPPING_PATIENCE = 10 # Kasvatetaan myös kärsivällisyyttä\n",
        "\n",
        "# --- Varoitusraja (8h liukuva keskiarvo) ---\n",
        "# Määritellään tässä, ja voidaan määrittää uudelleen arviointisolussa varmuuden vuoksi\n",
        "O3_THRESHOLD_8H_AVG = 85 # µg/m³ (tai esim. 85 testailuun)\n",
        "\n",
        "# --- Yleisasetukset ---\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Asetukset kuvaajille\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 7) # Hieman isommat kuvaajat\n",
        "# Estetään turhia varoituksia\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"Osa 1: Tuonnit ja Asetukset (GRU v3) - OK\")\n",
        "print(f\"Käytettävä laite: {device}\")\n",
        "print(f\"Ladataan data: {DATA_URL}\")\n",
        "print(f\"Kohdemuuttuja: {TARGET_COLUMN}\")"
      ],
      "metadata": {
        "id": "PNDNPfUgu6xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Funktiot Datan Lataukseen ja Käsittelyyn (GRU v3)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "# Funktio datan lataamiseen URL:sta (jos paikallista ei löydy)\n",
        "def download_data(url, local_path):\n",
        "    \"\"\"Lataa tiedoston URL:sta paikalliseen polkuun.\"\"\"\n",
        "    try:\n",
        "        print(f\"Yritetään ladata dataa osoitteesta {url}...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        # Varmista että hakemisto on olemassa\n",
        "        # Tarkista onko local_path tiedosto vai hakemisto\n",
        "        target_dir = os.path.dirname(local_path)\n",
        "        if target_dir and not os.path.exists(target_dir): # Luo hakemisto vain jos se ei ole tyhjä (eli ei juurihakemisto)\n",
        "             os.makedirs(target_dir, exist_ok=True)\n",
        "             print(f\"Luotiin hakemisto: {target_dir}\")\n",
        "\n",
        "        with open(local_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Data ladattu onnistuneesti: {local_path}\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Virhe datan latauksessa: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "         print(f\"Odottamaton virhe tiedoston tallennuksessa polkuun {local_path}: {e}\")\n",
        "         traceback.print_exc()\n",
        "         return False\n",
        "\n",
        "# Funktio Parquet-datan lataamiseen\n",
        "def load_parquet_data(filepath_or_url, local_cache_path=LOCAL_DATA_PATH):\n",
        "    \"\"\"Lataa datan Parquet-tiedostosta (paikallisesti tai URL:sta, käyttää välimuistia).\"\"\"\n",
        "    filepath_to_read = None\n",
        "    # Jos annettu on URL\n",
        "    if filepath_or_url.startswith('http'):\n",
        "         # Tarkista ensin paikallinen välimuisti\n",
        "         if os.path.exists(local_cache_path):\n",
        "              print(f\"Käytetään paikallista välimuistitiedostoa: {local_cache_path}\")\n",
        "              filepath_to_read = local_cache_path\n",
        "         # Jos ei välimuistissa, yritä ladata\n",
        "         else:\n",
        "              print(f\"Paikallista tiedostoa {local_cache_path} ei löytynyt.\")\n",
        "              if download_data(filepath_or_url, local_cache_path):\n",
        "                   filepath_to_read = local_cache_path\n",
        "              else:\n",
        "                   return None # Lataus epäonnistui\n",
        "    # Jos annettu on paikallinen polku\n",
        "    else:\n",
        "         filepath_to_read = filepath_or_url\n",
        "         if not os.path.exists(filepath_to_read):\n",
        "              print(f\"VIRHE: Annettua paikallista tiedostoa {filepath_to_read} ei löytynyt.\")\n",
        "              return None\n",
        "\n",
        "    # Jos tiedostopolku on saatu (joko paikallisesti tai lataamalla)\n",
        "    if filepath_to_read:\n",
        "        try:\n",
        "            print(f\"Ladataan dataa tiedostosta: {filepath_to_read}\")\n",
        "            df = pd.read_parquet(filepath_to_read)\n",
        "            print(f\"Parquet-tiedosto ladattu, muoto: {df.shape}\")\n",
        "\n",
        "            # Varmista DatetimeIndex\n",
        "            if not isinstance(df.index, pd.DatetimeIndex):\n",
        "                print(\"Indeksi ei ole DatetimeIndex. Yritetään muuntaa...\")\n",
        "                try:\n",
        "                    df.index = pd.to_datetime(df.index)\n",
        "                    print(\"Indeksi muunnettu datetime-indeksiksi.\")\n",
        "                except Exception as e_idx:\n",
        "                    print(f\"VIRHE: Indeksin muunto datetimeksi epäonnistui: {e_idx}\")\n",
        "                    return None # Palauta None jos kriittinen virhe\n",
        "\n",
        "            # Varmista aikavyöhyke (jos puuttuu, aseta Europe/Helsinki)\n",
        "            if df.index.tz is None:\n",
        "                 print(\"Asetetaan aikavyöhykkeeksi Europe/Helsinki...\")\n",
        "                 try:\n",
        "                     df = df.tz_localize('Europe/Helsinki', ambiguous='NaT', nonexistent='NaT')\n",
        "                     rows_before_nat = len(df)\n",
        "                     # Poista rivit, jotka saivat NaT-aikaleiman DST-käsittelyssä\n",
        "                     df.dropna(axis=0, subset=[df.index.name], inplace=True)\n",
        "                     if len(df) < rows_before_nat:\n",
        "                          print(f\"Poistettu {rows_before_nat - len(df)} riviä aikavyöhykkeen asetuksen (NaT) vuoksi.\")\n",
        "                 except Exception as e_tz:\n",
        "                      print(f\"VAROITUS: Aikavyöhykkeen asetus epäonnistui: {e_tz}. Jatketaan naiivilla indeksillä.\")\n",
        "\n",
        "            df.sort_index(inplace=True)\n",
        "            print(f\"Datan perustiedot: Aikaväli=[{df.index.min()} - {df.index.max()}]\")\n",
        "            print(f\"Sarakkeet: {df.columns.tolist()}\")\n",
        "\n",
        "            # Tarkista NaN-arvot\n",
        "            if df.isnull().any().any():\n",
        "                print(\"VAROITUS: Datassa NaN-arvoja latauksen jälkeen. Yritetään täyttää ffill/bfill...\")\n",
        "                df.ffill(inplace=True)\n",
        "                df.bfill(inplace=True)\n",
        "                if df.isnull().any().any():\n",
        "                     print(\"VIRHE: Ei voitu täyttää kaikkia NaN-arvoja. Poistetaan rivit joissa NaN.\")\n",
        "                     df.dropna(inplace=True)\n",
        "                     print(f\"Datan muoto NaN-poiston jälkeen: {df.shape}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"VIRHE Parquet-tiedoston lukemisessa ('{filepath_to_read}'): {e}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "    else:\n",
        "         # Tänne ei pitäisi päätyä jos logiikka toimii, mutta varmistus\n",
        "         print(\"VIRHE: Tiedostopolkua lukemiseen ei saatu määritettyä.\")\n",
        "         return None\n",
        "\n",
        "\n",
        "# Funktio ominaisuuksien muokkaukseen (feature engineering) neuroverkolle\n",
        "def feature_engineer_gru(df):\n",
        "    \"\"\"Lisää aika- ja syklisiä ominaisuuksia neuroverkkokäyttöön.\"\"\"\n",
        "    print(\"\\nSuoritetaan ominaisuuksien muokkaus (Feature Engineering)...\")\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"VIRHE: feature_engineer_gru sai syötteenä jotain muuta kuin DataFramen.\")\n",
        "        return None\n",
        "    df_eng = df.copy()\n",
        "\n",
        "    # Varmistetaan, että indeksi on DatetimeIndex\n",
        "    if not isinstance(df_eng.index, pd.DatetimeIndex):\n",
        "        print(\"VIRHE: DataFrame-indeksi ei ole DatetimeIndex feature engineeringissä.\")\n",
        "        return df_eng # Palauta alkuperäinen, jos indeksi väärä\n",
        "\n",
        "    # 1. Kellonaika (syklinen)\n",
        "    try:\n",
        "        df_eng['hour_sin'] = np.sin(2 * np.pi * df_eng.index.hour / 24.0)\n",
        "        df_eng['hour_cos'] = np.cos(2 * np.pi * df_eng.index.hour / 24.0)\n",
        "        print(\"Lisätty syklinen kellonaika (hour_sin, hour_cos).\")\n",
        "    except Exception as e: print(f\"VIRHE kellonaikaominaisuuksien luonnissa: {e}\")\n",
        "\n",
        "    # 2. Tuulen suunta (syklinen)\n",
        "    # Käytetään Osa 1:ssä määriteltyä sarakkeen nimeä (oletetaan sen olevan ORIGINAL_FEATURE_COLUMNSissa)\n",
        "    wind_dir_col_orig = 'Tuulen suunnan keskiarvo [°]'\n",
        "    if wind_dir_col_orig in df_eng.columns:\n",
        "        try:\n",
        "            # Muunnetaan numeeriseksi ensin, jos se ei ole (esim. object-tyyppiä)\n",
        "            df_eng[wind_dir_col_orig] = pd.to_numeric(df_eng[wind_dir_col_orig], errors='coerce')\n",
        "            # Tarkista tuliko NaNneja\n",
        "            if df_eng[wind_dir_col_orig].isnull().any():\n",
        "                 print(f\"VAROITUS: NaN-arvoja sarakkeessa '{wind_dir_col_orig}' tyyppimuunnoksen jälkeen. Täytetään...\")\n",
        "                 df_eng[wind_dir_col_orig].ffill(inplace=True)\n",
        "                 df_eng[wind_dir_col_orig].bfill(inplace=True)\n",
        "                 df_eng[wind_dir_col_orig].dropna(inplace=True) # Poista jos jäi vielä\n",
        "\n",
        "            df_eng['wind_dir_rad'] = np.deg2rad(df_eng[wind_dir_col_orig])\n",
        "            df_eng['wind_dir_sin'] = np.sin(df_eng['wind_dir_rad'])\n",
        "            df_eng['wind_dir_cos'] = np.cos(df_eng['wind_dir_rad'])\n",
        "            # Poista alkuperäinen tuulensuunta asteina ja radiaaneina\n",
        "            df_eng.drop(columns=[wind_dir_col_orig, 'wind_dir_rad'], inplace=True, errors='ignore')\n",
        "            print(\"Muunnettu tuulen suunta sykliseksi (wind_dir_sin, wind_dir_cos).\")\n",
        "        except Exception as e:\n",
        "            print(f\"VIRHE tuulensuuntaominaisuuksien luonnissa: {e}\")\n",
        "            # Poista mahdolliset osittain luodut sarakkeet\n",
        "            df_eng.drop(columns=['wind_dir_rad', 'wind_dir_sin', 'wind_dir_cos'], inplace=True, errors='ignore')\n",
        "    else:\n",
        "        print(f\"Saraketta '{wind_dir_col_orig}' ei löytynyt, tuulensuunnan muokkausta ei tehty.\")\n",
        "\n",
        "    # 3. Muita mahdollisia (esim. kuukausi, viikonpäivä) - jätetään nyt pois\n",
        "    # ...\n",
        "\n",
        "    print(\"Ominaisuuksien muokkaus valmis.\")\n",
        "    # Palautetaan DataFrame, jossa on alkuperäiset + uudet sarakkeet (miinus poistetut)\n",
        "    return df_eng\n",
        "\n",
        "# Funktio sekvenssien luontiin PyTorchille\n",
        "def create_sequences_pytorch(features_scaled, targets_original, sequence_length, prediction_horizon):\n",
        "    \"\"\"\n",
        "    Luo syötesekvenssejä (X, skaalattu) ja alkuperäisiä kohde-ennusteita (y_orig).\n",
        "    Kohde (y_orig) sisältää vain TARGET_COLUMNin arvot.\n",
        "    Palauttaa X ja y_orig numpy arrayna.\n",
        "    \"\"\"\n",
        "    X, y_orig = [], []\n",
        "    print(f\"\\nLuodaan PyTorch-sekvenssejä: sequence_length={sequence_length}, prediction_horizon={prediction_horizon}\")\n",
        "    print(f\"features_scaled shape: {features_scaled.shape}, targets_original shape: {targets_original.shape}\")\n",
        "\n",
        "    required_len = sequence_length + prediction_horizon\n",
        "    if len(features_scaled) < required_len:\n",
        "        print(f\"VAROITUS: Ei tarpeeksi dataa ({len(features_scaled)}) sekvenssien luomiseen. Tarvitaan vähintään {required_len}.\")\n",
        "        return np.array(X), np.array(y_orig)\n",
        "\n",
        "    # Varmistetaan targets_original muoto (N, 1) käsittelyä varten\n",
        "    if targets_original.ndim == 1:\n",
        "        targets_original = targets_original.reshape(-1, 1)\n",
        "        print(f\"Muutettu targets_original muotoon: {targets_original.shape}\")\n",
        "\n",
        "    for i in range(len(features_scaled) - required_len + 1):\n",
        "        # X sisältää kaikki skaalatut ominaisuudet menneisyydestä\n",
        "        X.append(features_scaled[i:(i + sequence_length)])\n",
        "        # y sisältää VAIN kohdemuuttujan arvot tulevaisuudesta (alkuperäisessä skaalassa)\n",
        "        # Otetaan [:, 0], jotta saadaan 1D array (horizon,) ennen kuin lisätään listaan\n",
        "        y_orig.append(targets_original[i + sequence_length : i + sequence_length + prediction_horizon, 0])\n",
        "\n",
        "    print(f\"Luotu {len(X)} sekvenssiä.\")\n",
        "    X = np.array(X)     # Muoto: (samples, sequence_length, num_features)\n",
        "    y_orig = np.array(y_orig) # Muoto: (samples, prediction_horizon)\n",
        "\n",
        "    # Muutetaan y_orig muotoon (samples, prediction_horizon, 1) PyTorchia varten\n",
        "    if y_orig.ndim == 2:\n",
        "         y_orig = y_orig[..., np.newaxis]\n",
        "         print(f\"Muutettu y_orig muotoon: {y_orig.shape}\")\n",
        "    elif y_orig.size > 0: # Varmista ettei yritetä muokata tyhjää arrayta\n",
        "         print(f\"VAROITUS: y_orig muoto ({y_orig.shape}) ei ole odotettu (samples, horizon) tai (samples, horizon, 1).\")\n",
        "\n",
        "    return X, y_orig\n",
        "\n",
        "print(\"Osa 2: Funktiot datan käsittelyyn (GRU v3) - OK\")"
      ],
      "metadata": {
        "id": "XN8xVjtGv3M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Pääskriptin Suoritus: Datan Käsittely (GRU v3)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "# Varmistetaan, että PyTorch on tuotu\n",
        "try:\n",
        "    import torch\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "except ImportError:\n",
        "    print(\"VIRHE: PyTorch ei ole asennettu tai sitä ei voitu tuoda.\")\n",
        "    # Aseta loaderit Noneksi, jotta myöhemmät vaiheet eivät suoritu\n",
        "    train_loader = None; valid_loader = None; test_loader = None;\n",
        "# Varmistetaan Scaler\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "except ImportError:\n",
        "     print(\"VIRHE: scikit-learn ei ole asennettu tai sitä ei voitu tuoda.\")\n",
        "     feature_scaler = None; o3_scaler = None\n",
        "\n",
        "print(\"--- Aloitetaan Datan Käsittely GRU-mallia varten ---\")\n",
        "\n",
        "# Alustetaan muuttujat varmuuden vuoksi\n",
        "df_raw_full = None; df_raw = None; df_engineered = None\n",
        "train_loader = None; valid_loader = None; test_loader = None\n",
        "feature_scaler = None; o3_scaler = None\n",
        "INPUT_SIZE = None\n",
        "test_timestamps = None\n",
        "# Sekvenssimuuttujat\n",
        "X_train, y_train_original, X_valid, y_valid_original, X_test, y_test_original = [None]*6\n",
        "y_train_scaled, y_valid_scaled = None, None\n",
        "\n",
        "\n",
        "# Suoritetaan päälogiikka try-except-lohkossa\n",
        "try:\n",
        "    # 1. Lataa data käyttäen Osa 2:n funktiota\n",
        "    df_raw_full = load_parquet_data(DATA_URL) # Käyttää DATA_URL ja LOCAL_DATA_PATH Osa 1:stä\n",
        "\n",
        "    if df_raw_full is not None:\n",
        "        # 2. Tarkista ja valitse alkuperäiset sarakkeet\n",
        "        print(\"\\nTarkistetaan ja valitaan alkuperäiset sarakkeet...\")\n",
        "        missing_cols_raw = [col for col in ALL_COLUMNS_IN_PARQUET if col not in df_raw_full.columns]\n",
        "        if missing_cols_raw:\n",
        "            raise ValueError(f\"Seuraavat määritellyt sarakkeet puuttuvat Parquet-tiedostosta: {missing_cols_raw}\")\n",
        "        df_raw = df_raw_full[ALL_COLUMNS_IN_PARQUET].copy()\n",
        "        print(\"Alkuperäiset sarakkeet valittu.\")\n",
        "\n",
        "        # 3. Suorita Feature Engineering\n",
        "        df_engineered = feature_engineer_gru(df_raw)\n",
        "        if df_engineered is None: raise ValueError(\"Ominaisuuksien muokkaus (feature_engineer_gru) epäonnistui.\")\n",
        "\n",
        "        # 4. Määritä lopulliset ominaisuudet ja INPUT_SIZE\n",
        "        FINAL_FEATURE_COLUMNS = df_engineered.columns.tolist()\n",
        "        # Varmista, ettei kohdemuuttuja ole vahingossa ominaisuuksissa\n",
        "        # (Sen pitäisi olla, koska käytämme historiaa, mutta tarkistus on hyvä)\n",
        "        if TARGET_COLUMN not in FINAL_FEATURE_COLUMNS:\n",
        "             print(f\"VAROITUS: Kohdemuuttuja '{TARGET_COLUMN}' ei ole lopullisissa ominaisuuksissa!\")\n",
        "             # Tässä vaiheessa voitaisiin päättää lisätä se tai pysäyttää ajo\n",
        "        INPUT_SIZE = len(FINAL_FEATURE_COLUMNS)\n",
        "        if INPUT_SIZE == 0: raise ValueError(\"Lopullisia ominaisuuksia ei löytynyt feature engineeringin jälkeen.\")\n",
        "        print(f\"\\nLopullinen ominaisuuksien määrä (INPUT_SIZE): {INPUT_SIZE}\")\n",
        "        print(f\"Lopulliset ominaisuudet mallille: {FINAL_FEATURE_COLUMNS}\")\n",
        "\n",
        "        # 5. Jaa data harjoitus-, validointi- ja testijoukkoihin\n",
        "        n = len(df_engineered)\n",
        "        min_data_len_needed = (SEQUENCE_LENGTH + PREDICTION_HORIZON) * 3\n",
        "        if n < min_data_len_needed:\n",
        "             print(f\"VAROITUS: Datan pituus ({n}) voi olla liian lyhyt jakoon ja sekvenssien luontiin (tarvitaan > {min_data_len_needed}).\")\n",
        "\n",
        "        test_split_idx = int(n * (1 - TEST_SPLIT_RATIO))\n",
        "        valid_split_idx = int(test_split_idx * (1 - VALID_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO)))\n",
        "\n",
        "        df_train = df_engineered[:valid_split_idx]\n",
        "        df_valid = df_engineered[valid_split_idx:test_split_idx]\n",
        "        df_test = df_engineered[test_split_idx:]\n",
        "\n",
        "        min_len_for_seq = SEQUENCE_LENGTH + PREDICTION_HORIZON\n",
        "        if len(df_train) < min_len_for_seq or len(df_valid) < min_len_for_seq or len(df_test) < min_len_for_seq:\n",
        "            print(f\"Train len: {len(df_train)}, Valid len: {len(df_valid)}, Test len: {len(df_test)}, Min required: {min_len_for_seq}\")\n",
        "            raise ValueError(\"Liian vähän dataa yhdessä tai useammassa jaossa sekvenssien luontia varten.\")\n",
        "\n",
        "        print(f\"\\nDatan jako:\")\n",
        "        print(f\"Train: {df_train.shape[0]} riviä ({df_train.index.min()} - {df_train.index.max()})\")\n",
        "        print(f\"Valid: {df_valid.shape[0]} riviä ({df_valid.index.min()} - {df_valid.index.max()})\")\n",
        "        print(f\"Test:  {df_test.shape[0]} riviä ({df_test.index.min()} - {df_test.index.max()})\")\n",
        "\n",
        "        # 6. Skaalaa data\n",
        "        print(\"\\nSkaalataan dataa...\")\n",
        "        # Skaalaa kaikki lopulliset ominaisuudet\n",
        "        feature_scaler = StandardScaler()\n",
        "        # Sovita VAIN harjoitusdataan\n",
        "        scaled_train_features = feature_scaler.fit_transform(df_train)\n",
        "        # Muunna validointi- ja testidata\n",
        "        scaled_valid_features = feature_scaler.transform(df_valid)\n",
        "        scaled_test_features = feature_scaler.transform(df_test)\n",
        "        print(\"Ominaisuudet skaalattu.\")\n",
        "\n",
        "        # Luo ja sovita skaalain erikseen VAIN kohdemuuttujalle (käytä df_raw:ta!)\n",
        "        o3_scaler = StandardScaler()\n",
        "        o3_scaler.fit(df_raw.loc[df_train.index, [TARGET_COLUMN]])\n",
        "        print(\"Erillinen skaalain luotu ja sovitettu kohdemuuttujalle (O3).\")\n",
        "\n",
        "        # 7. Hae alkuperäiset O3-kohdearvot (skaalaamattomat)\n",
        "        o3_train_targets_original = df_raw.loc[df_train.index, [TARGET_COLUMN]].values\n",
        "        o3_valid_targets_original = df_raw.loc[df_valid.index, [TARGET_COLUMN]].values\n",
        "        o3_test_targets_original = df_raw.loc[df_test.index, [TARGET_COLUMN]].values\n",
        "\n",
        "        # 8. Luo sekvenssit\n",
        "        X_train, y_train_original = create_sequences_pytorch(scaled_train_features, o3_train_targets_original, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "        X_valid, y_valid_original = create_sequences_pytorch(scaled_valid_features, o3_valid_targets_original, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "        X_test, y_test_original = create_sequences_pytorch(scaled_test_features, o3_test_targets_original, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "\n",
        "        if X_train.size == 0 or X_valid.size == 0 or X_test.size == 0:\n",
        "             raise ValueError(\"Sekvenssien luonti epäonnistui (yksi tai useampi X on tyhjä).\")\n",
        "\n",
        "        # 9. Skaalaa kohdesekvenssit (y) koulutusta ja validointia varten\n",
        "        # Muoto y_..._original on nyt (samples, horizon, 1)\n",
        "        y_train_scaled = o3_scaler.transform(y_train_original.reshape(-1, 1)).reshape(y_train_original.shape)\n",
        "        y_valid_scaled = o3_scaler.transform(y_valid_original.reshape(-1, 1)).reshape(y_valid_original.shape)\n",
        "        print(\"Kohdesekvenssit (y) skaalattu koulutusta ja validointia varten.\")\n",
        "\n",
        "        # 10. Muunna PyTorch Tensoreiksi\n",
        "        print(\"\\nMuunnetaan data PyTorch-tensoreiksi...\")\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "        X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
        "        y_valid_tensor = torch.tensor(y_valid_scaled, dtype=torch.float32)\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_test_tensor_original = torch.tensor(y_test_original, dtype=torch.float32) # Alkuperäinen y testiä varten\n",
        "        print(\"Tensorit luotu.\")\n",
        "\n",
        "        # 11. Luo DataLoaderit\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) # drop_last voi auttaa jos viimeinen erä on pieni\n",
        "        valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor_original) # Testilataaja tarvitsee X:n ja alkuperäisen y:n\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "        print(\"DataLoaderit luotu.\")\n",
        "\n",
        "        # 12. Tallenna testiaikaleimat visualisointia varten\n",
        "        print(\"\\nTallennetaan testiaikaleimoja...\")\n",
        "        # Varmistetaan df_test ja X_test olemassaolo ja pituus\n",
        "        if 'df_test' in locals() and not df_test.empty and len(df_test) >= SEQUENCE_LENGTH and 'X_test' in locals() and len(X_test) > 0:\n",
        "            try:\n",
        "                test_start_index_loc = df_engineered.index.get_loc(df_test.index[0])\n",
        "                test_start_indices = test_start_index_loc + SEQUENCE_LENGTH\n",
        "                end_index = test_start_indices + len(X_test) # X_test pituus vastaa sekvenssien määrää\n",
        "                if end_index <= len(df_engineered.index):\n",
        "                    test_timestamps = df_engineered.index[test_start_indices : end_index]\n",
        "                    print(f\"Testiaikaleimat tallennettu ({len(test_timestamps)} kpl). Alkaa: {test_timestamps.min()}, Päättyy: {test_timestamps.max()}\")\n",
        "                else:\n",
        "                    print(f\"VAROITUS: Ei voitu määrittää kaikkia testiaikaleimoja, loppuindeksi ({end_index}) ylittää datan pituuden ({len(df_engineered.index)}).\")\n",
        "                    test_timestamps = None\n",
        "            except KeyError:\n",
        "                 print(\"VAROITUS: Testijoukon alkuaikaleimaa ei löytynyt alkuperäisestä indeksistä.\")\n",
        "                 test_timestamps = None\n",
        "            except Exception as e_ts:\n",
        "                 print(f\"VIRHE testiaikaleimojen tallennuksessa: {e_ts}\")\n",
        "                 test_timestamps = None\n",
        "        else:\n",
        "             print(\"VAROITUS: Testidata liian lyhyt, tyhjä tai sekvenssejä ei luotu. Ei voida tallentaa aikaleimoja.\")\n",
        "             test_timestamps = None\n",
        "\n",
        "\n",
        "        print(f\"\\nLopulliset muodot DataLoadereihin menevälle datalle:\")\n",
        "        print(f\"X_train_tensor: {X_train_tensor.shape}, y_train_tensor: {y_train_tensor.shape}\")\n",
        "        print(f\"X_valid_tensor: {X_valid_tensor.shape}, y_valid_tensor: {y_valid_tensor.shape}\")\n",
        "        print(f\"X_test_tensor:  {X_test_tensor.shape}, y_test_tensor_original: {y_test_tensor_original.shape}\")\n",
        "\n",
        "        print(\"\\nOsa 3: Datan käsittely (GRU v3) - VALMIS\")\n",
        "\n",
        "    # else: # df_raw_full is None\n",
        "    #     print(\"Datan lataus epäonnistui kokonaan. Skriptin suoritus keskeytetty aiemmin.\")\n",
        "\n",
        "# Käsittele mahdolliset virheet päälohkossa\n",
        "except ValueError as ve:\n",
        "     print(f\"\\n---> VIRHE DATAN KÄSITTELYSSÄ (ValueError): {ve} <---\")\n",
        "     traceback.print_exc()\n",
        "     # Asetetaan loaderit Noneksi, jotta myöhemmät vaiheet eivät suoritu\n",
        "     train_loader = None; valid_loader = None; test_loader = None; INPUT_SIZE=None;\n",
        "except KeyError as ke:\n",
        "     print(f\"\\n---> VIRHE DATAN KÄSITTELYSSÄ (KeyError): Saraketta {ke} ei löytynyt <---\")\n",
        "     traceback.print_exc()\n",
        "     train_loader = None; valid_loader = None; test_loader = None; INPUT_SIZE=None;\n",
        "except Exception as e:\n",
        "     print(f\"\\n---> ODOTTAMATON VIRHE DATAN KÄSITTELYSSÄ (Osa 3) <---\")\n",
        "     print(f\"Virhetyyppi: {type(e).__name__}\")\n",
        "     print(f\"Virheilmoitus: {e}\")\n",
        "     traceback.print_exc()\n",
        "     train_loader = None; valid_loader = None; test_loader = None; INPUT_SIZE=None;\n",
        "     print(\"----------------------------------------------------\")"
      ],
      "metadata": {
        "id": "CEjiLfMlwSfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. GRU-Mallin Määrittely (GRU v3)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import traceback # Virheiden jäljitykseen\n",
        "\n",
        "print(\"--- Määritellään GRU-malli ---\")\n",
        "\n",
        "# Varmistetaan tarvittavat parametrit Osa 1:stä\n",
        "required_params = ['INPUT_SIZE', 'HIDDEN_SIZE', 'NUM_LAYERS', 'OUTPUT_SIZE', 'DROPOUT_PROB']\n",
        "params_ok = True\n",
        "for p in required_params:\n",
        "    if p not in locals() or locals()[p] is None:\n",
        "         print(f\"VIRHE: Mallin hyperparametri '{p}' puuttuu tai on None. Aja Osa 1 uudelleen.\")\n",
        "         params_ok = False\n",
        "if not params_ok:\n",
        "     raise ValueError(\"Yksi tai useampi mallin hyperparametri puuttuu.\")\n",
        "\n",
        "# Itse malliluokka\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
        "        super(GRUModel, self).__init__()\n",
        "        # Varmistetaan parametrien oikeellisuus myös tässä\n",
        "        assert isinstance(input_size, int) and input_size > 0, \"input_size > 0\"\n",
        "        assert isinstance(hidden_size, int) and hidden_size > 0, \"hidden_size > 0\"\n",
        "        assert isinstance(num_layers, int) and num_layers > 0, \"num_layers > 0\"\n",
        "        assert isinstance(output_size, int) and output_size > 0, \"output_size > 0\"\n",
        "        assert isinstance(dropout_prob, float) and 0.0 <= dropout_prob < 1.0, \"dropout_prob [0, 1)\"\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # GRU-kerros\n",
        "        # batch_first=True -> input/output muoto: (batch, seq_len, features)\n",
        "        # dropout lisätään vain jos kerroksia > 1\n",
        "        gru_dropout = dropout_prob if num_layers > 1 else 0.0\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=gru_dropout)\n",
        "\n",
        "        # Lineaarinen kerros, joka muuntaa GRU:n piilotilan ennusteeksi\n",
        "        # Sisääntulo: hidden_size, Ulostulo: output_size (esim. 24 tuntia)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x:n odotettu muoto: (batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Alustetaan piilotila nollilla\n",
        "        # Muoto: (num_layers, batch_size, hidden_size)\n",
        "        # Siirretään piilotila samalle laitteelle kuin input x\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Eteenpäinsyöttö GRU-kerroksen läpi\n",
        "        # out: sisältää kaikkien aika-askelten piilotilat viimeisestä kerroksesta\n",
        "        #      muoto: (batch_size, sequence_length, hidden_size)\n",
        "        out, _ = self.gru(x, h0)\n",
        "\n",
        "        # Otetaan vain viimeisen aika-askeleen piilotila (-1) GRU:n ulostulosta\n",
        "        # Muoto: (batch_size, hidden_size)\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Syötetään viimeinen piilotila lineaarisen kerroksen läpi\n",
        "        # Muoto: (batch_size, output_size)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Testataan luokan alustus nopeasti (varmistaa että parametrit ok)\n",
        "try:\n",
        "     # Luo malli testiksi käyttäen Osa 1:n parametreja\n",
        "     print(f\"\\nTestataan mallin alustusta: INPUT={INPUT_SIZE}, HIDDEN={HIDDEN_SIZE}, LAYERS={NUM_LAYERS}, OUTPUT={OUTPUT_SIZE}, DROPOUT={DROPOUT_PROB}\")\n",
        "     temp_model_test = GRUModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE, DROPOUT_PROB)\n",
        "     print(\"GRUModel-luokka määritelty ja alustus testattu onnistuneesti.\")\n",
        "     # Tulostetaan mallin rakenne\n",
        "     print(\"\\nMallin rakenne:\")\n",
        "     print(temp_model_test)\n",
        "     # Vapauta muisti heti\n",
        "     del temp_model_test\n",
        "except Exception as e_model_def:\n",
        "     print(f\"VIRHE GRUModel-luokan alustuksessa: {e_model_def}\")\n",
        "     traceback.print_exc()\n",
        "\n",
        "\n",
        "print(\"\\nOsa 4: GRU-Mallin Määrittely (GRU v3) - OK\")"
      ],
      "metadata": {
        "id": "-tLzIhPYxfIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Koulutusfunktion Määrittely (GRU v3)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm # Edistymispalkki\n",
        "import copy # Mallin tilan kopiointiin\n",
        "import traceback\n",
        "\n",
        "print(\"--- Määritellään train_model -funktio ---\")\n",
        "\n",
        "# Varmistetaan tarvittavien luokkien olemassaolo\n",
        "if 'DataLoader' not in locals(): raise NameError(\"DataLoader ei ole määritelty (tuo torch.utils.data).\")\n",
        "if 'nn' not in locals() or 'optim' not in locals(): raise NameError(\"torch.nn tai torch.optim ei ole tuotu.\")\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, device, patience):\n",
        "    \"\"\"Kouluttaa mallin ja käyttää Early Stoppingia (sis. .squeeze(-1) korjauksen).\"\"\"\n",
        "\n",
        "    # Alkutarkistukset\n",
        "    if not isinstance(model, nn.Module): raise TypeError(\"Vaaditaan PyTorch-malli.\")\n",
        "    if not isinstance(train_loader, DataLoader): raise TypeError(\"Vaaditaan DataLoader train_loaderille.\")\n",
        "    if not isinstance(valid_loader, DataLoader): raise TypeError(\"Vaaditaan DataLoader valid_loaderille.\")\n",
        "    if not isinstance(criterion, nn.modules.loss._Loss): raise TypeError(\"Vaaditaan PyTorch häviöfunktio.\")\n",
        "    if not isinstance(optimizer, optim.Optimizer): raise TypeError(\"Vaaditaan PyTorch optimoija.\")\n",
        "    if not isinstance(epochs, int) or epochs <= 0: raise ValueError(\"Epochs oltava positiivinen kokonaisluku.\")\n",
        "    if not isinstance(patience, int) or patience <= 0: raise ValueError(\"Patience oltava positiivinen kokonaisluku.\")\n",
        "\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    best_valid_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None # Tähän tallennetaan parhaan mallin tila\n",
        "\n",
        "    print(f\"\\nAloitetaan koulutus {epochs} epochilla...\")\n",
        "    print(f\"Early stopping -kärsivällisyys: {patience} epochia.\")\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        model.train() # Aseta malli koulutustilaan\n",
        "        running_train_loss = 0.0\n",
        "        batch_count = 0\n",
        "        try: # Try-except epochin sisällä\n",
        "            for inputs, targets_scaled in train_loader:\n",
        "                batch_count += 1\n",
        "                inputs, targets_scaled = inputs.to(device), targets_scaled.to(device)\n",
        "\n",
        "                # Nollaa gradientit\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Eteenpäinsyöttö\n",
        "                outputs_scaled = model(inputs) # Ennusteet, muoto (batch, 24)\n",
        "\n",
        "                # --- Käsittele kohdemuoto (squeeze) ---\n",
        "                if targets_scaled.ndim == outputs_scaled.ndim + 1 and targets_scaled.shape[-1] == 1:\n",
        "                    targets_squeezed = targets_scaled.squeeze(-1) # Muoto (batch, 24)\n",
        "                elif targets_scaled.shape == outputs_scaled.shape:\n",
        "                    targets_squeezed = targets_scaled # Muodot täsmäävät jo\n",
        "                else:\n",
        "                     print(f\"\\nVIRHE Epoch {epoch+1}, Batch {batch_count} (Train): Muodot eivät täsmää loss-laskentaa varten!\")\n",
        "                     print(f\"Output shape: {outputs_scaled.shape}, Target shape: {targets_scaled.shape}\")\n",
        "                     raise RuntimeError(\"Muodot eivät täsmää loss-laskentaa varten\")\n",
        "\n",
        "                # Lasketaan häviö\n",
        "                loss = criterion(outputs_scaled, targets_squeezed)\n",
        "\n",
        "                # Taaksepäinvienti ja optimointi\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item() * inputs.size(0) # Kerro erän koolla painotusta varten\n",
        "\n",
        "            # Laske keskimääräinen häviö koko epochille\n",
        "            epoch_train_loss = running_train_loss / len(train_loader.dataset) if len(train_loader.dataset) > 0 else 0\n",
        "            train_losses.append(epoch_train_loss)\n",
        "\n",
        "        except Exception as e_train_epoch:\n",
        "             print(f\"\\nVIRHE koulutusloopissa (Epoch {epoch+1}, Batch {batch_count}): {e_train_epoch}\")\n",
        "             traceback.print_exc()\n",
        "             print(\"Keskeytetään koulutus.\")\n",
        "             # Palauta tähänastiset tulokset ja None mallille osoittamaan virhettä\n",
        "             model = None # Merkitään malli vialliseksi\n",
        "             return model, train_losses, valid_losses\n",
        "\n",
        "\n",
        "        # --- Validointivaihe ---\n",
        "        model.eval() # Aseta malli evaluointitilaan\n",
        "        running_valid_loss = 0.0\n",
        "        valid_batch_count = 0\n",
        "        try: # Try-except validointiin\n",
        "            with torch.no_grad(): # Ei lasketa gradientteja validoinnissa\n",
        "                for inputs, targets_scaled in valid_loader:\n",
        "                    valid_batch_count += 1\n",
        "                    inputs, targets_scaled = inputs.to(device), targets_scaled.to(device)\n",
        "                    outputs_scaled = model(inputs)\n",
        "\n",
        "                    # --- Käsittele kohdemuoto (squeeze) ---\n",
        "                    if targets_scaled.ndim == outputs_scaled.ndim + 1 and targets_scaled.shape[-1] == 1:\n",
        "                        targets_squeezed = targets_scaled.squeeze(-1)\n",
        "                    elif targets_scaled.shape == outputs_scaled.shape:\n",
        "                        targets_squeezed = targets_scaled\n",
        "                    else:\n",
        "                         print(f\"\\nVIRHE Epoch {epoch+1} (Valid), Batch {valid_batch_count}: Muodot eivät täsmää loss-laskentaa varten!\")\n",
        "                         print(f\"Output shape: {outputs_scaled.shape}, Target shape: {targets_scaled.shape}\")\n",
        "                         raise RuntimeError(\"Muodot eivät täsmää validointi loss-laskentaa varten\")\n",
        "\n",
        "                    loss = criterion(outputs_scaled, targets_squeezed)\n",
        "                    running_valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_valid_loss = running_valid_loss / len(valid_loader.dataset) if len(valid_loader.dataset) > 0 else 0\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1:02d}/{epochs} - Train Loss: {epoch_train_loss:.6f} - Valid Loss: {epoch_valid_loss:.6f}\", end=\"\")\n",
        "\n",
        "            # Early Stopping Check\n",
        "            if epoch_valid_loss < best_valid_loss:\n",
        "                best_valid_loss = epoch_valid_loss\n",
        "                epochs_no_improve = 0\n",
        "                try: # Tallenna paras malli\n",
        "                    best_model_state = copy.deepcopy(model.state_dict())\n",
        "                    print(\" (Uusi paras!)\")\n",
        "                except Exception as e_state:\n",
        "                     print(f\" (VIRHE mallin tilan tallennuksessa: {e_state})\")\n",
        "                     best_model_state = None # Älä käytä viallista tilaa\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                print(f\" (Ei parannusta {epochs_no_improve}/{patience})\")\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"\\nEarly stopping {patience} epochin jälkeen ilman parannusta.\")\n",
        "                break # Pysäytä koulutusloop\n",
        "\n",
        "        except Exception as e_valid_epoch:\n",
        "             print(f\"\\nVIRHE validointiloopissa (Epoch {epoch+1}, Batch {valid_batch_count}): {e_valid_epoch}\")\n",
        "             traceback.print_exc()\n",
        "             print(\"Keskeytetään koulutus.\")\n",
        "             model = None # Merkitään malli vialliseksi\n",
        "             return model, train_losses, valid_losses\n",
        "\n",
        "\n",
        "    # --- Koulutusloopin jälkeen ---\n",
        "    # Palauta paras löydetty mallin tila (jos löytyi ja tallennus onnistui)\n",
        "    if best_model_state:\n",
        "         print(\"\\nLadataan paras malli Early Stoppingin perusteella.\")\n",
        "         try:\n",
        "              model.load_state_dict(best_model_state)\n",
        "         except Exception as e_load:\n",
        "              print(f\"VIRHE parhaan mallin tilan latauksessa: {e_load}. Jatketaan viimeisimmällä mallilla.\")\n",
        "    elif epochs > 0 and train_losses is not None : # Jos ei early stopping, mutta ajettiin onnistuneesti\n",
        "         print(\"\\nKoulutus päättyi ilman Early Stoppingia tai paras tila viallinen. Käytetään viimeisintä mallia.\")\n",
        "    else: # Jos ei ajettu yhtään epochia tai koulutus keskeytyi heti\n",
        "         print(\"\\nKoulutusta ei ajettu loppuun / keskeytyi. Malli saattaa olla alustamaton.\")\n",
        "\n",
        "\n",
        "    return model, train_losses, valid_losses\n",
        "\n",
        "print(\"\\nOsa 5: Koulutusfunktion Määrittely (GRU v3) - OK\")"
      ],
      "metadata": {
        "id": "_I86WDqdyVUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Mallin Koulutus (Suoritus) (GRU v3)\n",
        "\n",
        "import torch.nn as nn # Varmistetaan tuonnit\n",
        "import torch.optim as optim # Varmistetaan tuonnit\n",
        "import matplotlib.pyplot as plt # Varmistetaan tuonnit\n",
        "import traceback # Varmistetaan tuonti\n",
        "\n",
        "print(\"--- Aloitetaan Mallin Koulutus (Suoritus) ---\")\n",
        "\n",
        "# Alustetaan model Noneksi siltä varalta että koulutus ei käynnisty tai epäonnistuu\n",
        "model = None\n",
        "train_losses = None\n",
        "valid_losses = None\n",
        "\n",
        "# Varmistetaan, että kaikki tarvittavat muuttujat edellisistä osista ovat olemassa\n",
        "required_vars_exist = True\n",
        "vars_to_check = ['train_loader', 'valid_loader', 'INPUT_SIZE', 'device',\n",
        "                 'HIDDEN_SIZE', 'NUM_LAYERS', 'OUTPUT_SIZE', 'DROPOUT_PROB',\n",
        "                 'LEARNING_RATE', 'EPOCHS', 'EARLY_STOPPING_PATIENCE', 'GRUModel', 'train_model']\n",
        "missing_vars = []\n",
        "for var in vars_to_check:\n",
        "    # Tarkista sekä olemassaolo että ettei arvo ole None (paitsi mahdollisesti train_losses/valid_losses)\n",
        "    if var not in locals() or (locals()[var] is None and var not in ['train_losses', 'valid_losses']):\n",
        "        missing_vars.append(var)\n",
        "        required_vars_exist = False\n",
        "\n",
        "if required_vars_exist:\n",
        "    # --- Mallin, häviöfunktion ja optimoijan alustus ---\n",
        "    if isinstance(INPUT_SIZE, int) and INPUT_SIZE > 0:\n",
        "        try:\n",
        "            model = GRUModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE, DROPOUT_PROB).to(device)\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "            print(\"\\n--- GRU-Malli (ennen koulutusta) ---\")\n",
        "            print(model)\n",
        "            print(f\"Ominaisuuksien määrä (Input size): {INPUT_SIZE}\")\n",
        "            param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            print(f\"Koulutettavien parametrien määrä: {param_count}\")\n",
        "            print(f\"Käytettävä laite: {device}\")\n",
        "            print(\"-------------------------------------\\n\")\n",
        "\n",
        "            # --- Koulutetaan malli ---\n",
        "            try:\n",
        "                model, train_losses, valid_losses = train_model(\n",
        "                    model, train_loader, valid_loader, criterion, optimizer, EPOCHS, device, EARLY_STOPPING_PATIENCE\n",
        "                )\n",
        "\n",
        "                # Tarkistetaan paluuarvot\n",
        "                if model is None or train_losses is None or valid_losses is None:\n",
        "                     print(\"\\nKoulutus keskeytyi tai epäonnistui train_model-funktiossa.\")\n",
        "                     model = None # Varmistetaan Noneksi\n",
        "                else:\n",
        "                     print(\"\\nKoulutus suoritettu.\")\n",
        "\n",
        "                     # --- Piirretään häviökäyrät (vain jos koulutus onnistui ja listat ok) ---\n",
        "                     if train_losses and valid_losses: # Varmista että listat eivät ole tyhjiä tai None\n",
        "                        plt.figure(figsize=(10, 5))\n",
        "                        plt.plot(train_losses, label='Training Loss')\n",
        "                        plt.plot(valid_losses, label='Validation Loss')\n",
        "                        plt.title('Training and Validation Loss')\n",
        "                        plt.xlabel('Epoch')\n",
        "                        plt.ylabel('Loss (MSE)')\n",
        "                        # Yritä käyttää log-skaalaa vain jos arvot sen sallivat\n",
        "                        try:\n",
        "                            min_loss_val = min(min(train_losses, default=1.0), min(valid_losses, default=1.0))\n",
        "                            if min_loss_val > 1e-9: # Tarkistus > 0\n",
        "                               plt.yscale('log')\n",
        "                               print(\"Käytetään logaritmista skaalaa häviökuvaajassa.\")\n",
        "                            else:\n",
        "                               print(\"Käytetään lineaarista skaalaa häviökuvaajassa (min loss <= 0).\")\n",
        "                        except: # Jos minimin laskenta epäonnistuu tms.\n",
        "                            print(\"Käytetään lineaarista skaalaa häviökuvaajassa.\")\n",
        "                            plt.yscale('linear')\n",
        "\n",
        "                        plt.legend()\n",
        "                        plt.grid(True, linestyle=':', alpha=0.7)\n",
        "                        plt.show()\n",
        "                     elif train_losses is not None and valid_losses is not None:\n",
        "                          print(\"Häviölistat ovat tyhjiä, ei voida piirtää kuvaajaa.\")\n",
        "                     else:\n",
        "                          print(\"Häviölistoja ei saatu koulutuksesta, ei voida piirtää kuvaajaa.\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nVIRHE train_model-kutsun aikana: {e}\")\n",
        "                traceback.print_exc()\n",
        "                model = None # Aseta malli Noneksi\n",
        "\n",
        "        except Exception as e_init:\n",
        "             print(f\"\\nVIRHE mallin, criterionin tai optimizerin alustuksessa: {e_init}\")\n",
        "             traceback.print_exc()\n",
        "             model = None # Aseta malli Noneksi\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nKoulutusta ei aloiteta, virheellinen INPUT_SIZE: {INPUT_SIZE}\")\n",
        "        model = None # Estetään jatko\n",
        "\n",
        "else:\n",
        "    print(f\"\\nKoulutusta ei voida aloittaa, koska yksi tai useampi tarvittava muuttuja puuttuu: {missing_vars}\")\n",
        "    model = None # Estetään jatko\n",
        "\n",
        "# Tulostetaan lopputilanne\n",
        "if model is not None:\n",
        "     print(\"\\nOsa 6: Koulutuksen suoritus - VALMIS\")\n",
        "else:\n",
        "     print(\"\\nOsa 6: Koulutuksen suoritus - EPÄONNISTUI / OHITETTIIN\")"
      ],
      "metadata": {
        "id": "j8uNzobg02KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Arviointifunktioiden Määrittely (GRU v3)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler # Varmistetaan tuonti\n",
        "import torch # Varmistetaan tuonti\n",
        "from torch.utils.data import DataLoader # Varmistetaan tuonti\n",
        "from tqdm.notebook import tqdm # Varmistetaan tuonti\n",
        "import traceback\n",
        "\n",
        "print(\"--- Määritellään arviointifunktiot ---\")\n",
        "\n",
        "# Funktio baseline-ennusteen laskemiseen\n",
        "# Käytetään yksinkertaista persistenssiä: ennustetaan koko 24h jaksolle\n",
        "# sama arvo kuin TODELLINEN arvo jakson ensimmäisellä tunnilla.\n",
        "def calculate_baseline_persistence_pytorch(targets_original_np, prediction_horizon):\n",
        "    \"\"\"Laskee naiivin persistenssi-baselinen PyTorch-datarakenteelle.\"\"\"\n",
        "    print(\"Lasketaan Baseline-ennuste (jakson eka arvo toistuu)...\")\n",
        "    try:\n",
        "        # targets_original_np muoto: (samples, horizon, 1)\n",
        "        if targets_original_np.ndim != 3 or targets_original_np.shape[-1] != 1:\n",
        "             raise ValueError(f\"Odotettiin 3D-muotoa (samples, horizon, 1), saatiin {targets_original_np.shape}\")\n",
        "        if targets_original_np.shape[0] == 0:\n",
        "             return np.array([]) # Palauta tyhjä, jos ei näytteitä\n",
        "\n",
        "        # Otetaan ensimmäisen tunnin arvo (indeksi 0) jokaisesta näytteestä\n",
        "        first_vals = targets_original_np[:, 0, 0] # Muoto: (samples,)\n",
        "        # Muotoillaan (samples, 1) repeatia varten\n",
        "        first_vals_for_repeat = first_vals[:, np.newaxis]\n",
        "\n",
        "        # Toistetaan ensimmäinen arvo koko ennustehorisontille\n",
        "        baseline_preds = np.repeat(first_vals_for_repeat, prediction_horizon, axis=1) # Muoto (samples, horizon)\n",
        "        return baseline_preds\n",
        "    except Exception as e_base:\n",
        "        print(f\"VIRHE baseline-laskennassa: {e_base}\")\n",
        "        traceback.print_exc()\n",
        "        # Yritetään palauttaa oikean muotoisia nollia\n",
        "        try:\n",
        "            return np.zeros((targets_original_np.shape[0], prediction_horizon))\n",
        "        except: return None\n",
        "\n",
        "\n",
        "# Funktio mallin suorituskyvyn arviointiin\n",
        "def evaluate_model_performance_pytorch(model, test_loader, device, o3_scaler, o3_threshold_8h, prediction_horizon):\n",
        "    \"\"\"Arvioi PyTorch-mallia testidatalla, laskee metriikat ja vertaa baselineen.\"\"\"\n",
        "    print(\"\\n--- evaluate_model_performance_pytorch -funktion suoritus alkaa ---\")\n",
        "\n",
        "    # Alkutarkistukset\n",
        "    if model is None: print(\"evaluate_model_performance_pytorch: Malli puuttuu.\"); return None, None\n",
        "    if test_loader is None: print(\"evaluate_model_performance_pytorch: Testilataaja puuttuu.\"); return None, None\n",
        "    if o3_scaler is None: print(\"evaluate_model_performance_pytorch: O3-skaalain puuttuu.\"); return None, None\n",
        "    if not isinstance(o3_scaler, StandardScaler): print(\"VAROITUS: o3_scaler ei ole StandardScaler.\"); # Jatketaan silti\n",
        "\n",
        "    model.eval() # Aseta malli arviointitilaan\n",
        "    all_preds_orig_list = []\n",
        "    all_targets_orig_list = []\n",
        "\n",
        "    print(\"Aloitetaan ennusteiden tekeminen testidatalla...\")\n",
        "    try: # Try-except ennustusloopin ympärille\n",
        "        with torch.no_grad():\n",
        "            # test_loader tuottaa (inputs, targets_original_batch)\n",
        "            for inputs, targets_original_batch in tqdm(test_loader, desc=\"Testaus (evaluate)\"):\n",
        "                inputs = inputs.to(device)\n",
        "                # targets_original_batch on jo oikeassa muodossa (batch, horizon, 1)\n",
        "\n",
        "                # Tee ennuste mallilla\n",
        "                outputs_scaled = model(inputs) # Malli tuottaa skaalattuja ennusteita, muoto (batch, horizon)\n",
        "\n",
        "                # Käännä ennusteiden skaalaus\n",
        "                preds_scaled_np = outputs_scaled.cpu().numpy()\n",
        "                # Skaalain odottaa 2D-muotoa (n_samples * n_features, 1)\n",
        "                preds_reshaped = preds_scaled_np.reshape(-1, 1)\n",
        "                preds_orig_np = o3_scaler.inverse_transform(preds_reshaped).reshape(preds_scaled_np.shape) # Palauta muotoon (batch, horizon)\n",
        "\n",
        "                # Kerää ennusteet ja alkuperäiset kohteet listoihin\n",
        "                all_preds_orig_list.append(preds_orig_np)\n",
        "                all_targets_orig_list.append(targets_original_batch.cpu().numpy()) # Muoto (batch, horizon, 1)\n",
        "\n",
        "        print(\"Ennusteiden tekeminen ja kerääminen valmis.\")\n",
        "\n",
        "        # Yhdistä kaikki erät yhdeksi isoksi numpy arrayksi\n",
        "        if not all_preds_orig_list or not all_targets_orig_list:\n",
        "             raise ValueError(\"Ennusteiden tai kohteiden keräys epäonnistui (listat tyhjiä).\")\n",
        "\n",
        "        all_preds_orig = np.concatenate(all_preds_orig_list, axis=0) # Muoto (total_samples, horizon)\n",
        "        all_targets_original = np.concatenate(all_targets_orig_list, axis=0) # Muoto (total_samples, horizon, 1)\n",
        "\n",
        "        print(f\"Kerätty {all_preds_orig.shape[0]} ennustetta/kohdetta.\")\n",
        "        print(f\"all_preds_orig muoto: {all_preds_orig.shape}, all_targets_original muoto: {all_targets_original.shape}\")\n",
        "\n",
        "        # Poista viimeinen dimensio kohteista metriikoita varten\n",
        "        if all_targets_original.ndim == 3 and all_targets_original.shape[-1] == 1:\n",
        "            targets_eval = all_targets_original.squeeze(-1) # Muoto (total_samples, horizon)\n",
        "            print(f\"Muutettu kohdemuoto metriikoita varten: {targets_eval.shape}\")\n",
        "        elif all_targets_original.ndim == 2:\n",
        "             targets_eval = all_targets_original # Oletetaan jo oikea muoto\n",
        "             print(f\"Kohdemuoto oli jo 2D: {targets_eval.shape}\")\n",
        "        else:\n",
        "             raise ValueError(f\"Odottamaton kohdemuoto metriikoille: {all_targets_original.shape}\")\n",
        "\n",
        "        # Varmistetaan muodot vielä kerran\n",
        "        if all_preds_orig.shape != targets_eval.shape:\n",
        "             raise ValueError(f\"Lopulliset muodot eivät täsmää: Pred={all_preds_orig.shape}, Target={targets_eval.shape}\")\n",
        "\n",
        "        # --- Laske regressiometriikat (GRU) ---\n",
        "        print(\"\\nLasketaan GRU-mallin regressiometriikat...\")\n",
        "        # Lasketaan virhe kaikista ennustepisteistä (samples * horizon)\n",
        "        rmse_gru = np.sqrt(mean_squared_error(targets_eval.ravel(), all_preds_orig.ravel()))\n",
        "        mae_gru = mean_absolute_error(targets_eval.ravel(), all_preds_orig.ravel())\n",
        "        print(f\"\\n--- GRU-Mallin Arviointi (kaikki {prediction_horizon} tuntia) ---\")\n",
        "        print(f\"RMSE: {rmse_gru:.4f} µg/m³\")\n",
        "        print(f\"MAE:  {mae_gru:.4f} µg/m³\")\n",
        "\n",
        "        # --- Laske Baseline ---\n",
        "        baseline_preds = calculate_baseline_persistence_pytorch(all_targets_original, prediction_horizon) # Käyttää 3D-muotoa sisäisesti\n",
        "        if baseline_preds is None or baseline_preds.shape != targets_eval.shape:\n",
        "             print(\"VIRHE: Baseline-laskenta epäonnistui tai muoto väärä.\")\n",
        "             rmse_baseline, mae_baseline = None, None\n",
        "        else:\n",
        "            # --- Laske regressiometriikat (Baseline) ---\n",
        "            print(\"\\nLasketaan Baseline-mallin regressiometriikat...\")\n",
        "            rmse_baseline = np.sqrt(mean_squared_error(targets_eval.ravel(), baseline_preds.ravel()))\n",
        "            mae_baseline = mean_absolute_error(targets_eval.ravel(), baseline_preds.ravel())\n",
        "            print(f\"\\n--- Baseline-Mallin Arviointi (Naiivi Persistenssi) ---\")\n",
        "            print(f\"RMSE: {rmse_baseline:.4f} µg/m³\")\n",
        "            print(f\"MAE:  {mae_baseline:.4f} µg/m³\")\n",
        "\n",
        "            # --- Vertailu ---\n",
        "            print(\"\\n--- Vertailu Baselineen ---\")\n",
        "            if rmse_gru is not None and rmse_baseline is not None:\n",
        "                improvement_rmse = rmse_baseline - rmse_gru\n",
        "                print(f\"GRU vs Baseline RMSE: {improvement_rmse:+.4f} µg/m³ ({'GRU parempi' if improvement_rmse > 0 else 'Baseline parempi tai sama'})\")\n",
        "            else: print(\"RMSE-vertailua ei voida tehdä.\")\n",
        "            if mae_gru is not None and mae_baseline is not None:\n",
        "                improvement_mae = mae_baseline - mae_gru\n",
        "                print(f\"GRU vs Baseline MAE:  {improvement_mae:+.4f} µg/m³ ({'GRU parempi' if improvement_mae > 0 else 'Baseline parempi tai sama'})\")\n",
        "            else: print(\"MAE-vertailua ei voida tehdä.\")\n",
        "\n",
        "        # --- 8h Liukuvan keskiarvon arviointi ---\n",
        "        # Lasketaan kullekin 24h jaksolle erikseen\n",
        "        warnings_actual = []\n",
        "        warnings_pred = []\n",
        "        n_samples_total = all_preds_orig.shape[0]\n",
        "        print(f\"\\nLasketaan 8h liukuvia keskiarvoja {n_samples_total} jaksolle ja verrataan kynnysarvoon ({o3_threshold_8h} µg/m³)...\")\n",
        "\n",
        "        for i in range(n_samples_total):\n",
        "            actual_24h = targets_eval[i, :] # Muoto (horizon,)\n",
        "            pred_24h = all_preds_orig[i, :]  # Muoto (horizon,)\n",
        "\n",
        "            actual_series = pd.Series(actual_24h)\n",
        "            pred_series = pd.Series(pred_24h)\n",
        "\n",
        "            actual_8h_avg = actual_series.rolling(window=8, min_periods=1).mean()\n",
        "            pred_8h_avg = pred_series.rolling(window=8, min_periods=1).mean()\n",
        "\n",
        "            actual_warning_triggered = actual_8h_avg.max() > o3_threshold_8h\n",
        "            pred_warning_triggered = pred_8h_avg.max() > o3_threshold_8h\n",
        "\n",
        "            warnings_actual.append(actual_warning_triggered)\n",
        "            warnings_pred.append(pred_warning_triggered)\n",
        "\n",
        "        warnings_actual = np.array(warnings_actual)\n",
        "        warnings_pred = np.array(warnings_pred)\n",
        "        print(\"Liukuvien keskiarvojen laskenta valmis.\")\n",
        "\n",
        "        # --- Tulosta 8h varoitusmetriikat ---\n",
        "        print(\"\\n--- 8h Liukuvan Keskiarvon Varoitustason Ylityksen Arviointi (GRU - Jaksoittain) ---\")\n",
        "        print(f\"Todellisia varoitusjaksoja testidatassa (> {o3_threshold_8h} µg/m³): {warnings_actual.sum()} / {n_samples_total}\")\n",
        "        print(f\"Ennustettuja varoitusjaksoja (GRU):                      {warnings_pred.sum()} / {n_samples_total}\")\n",
        "\n",
        "        if warnings_actual.sum() > 0 or warnings_pred.sum() > 0:\n",
        "            print(\"\\nSekaannusmatriisi (Confusion Matrix) varoituksille (GRU - Jaksoittain):\")\n",
        "            cm = confusion_matrix(warnings_actual, warnings_pred, labels=[False, True])\n",
        "            cm_df = pd.DataFrame(cm, index=['Todellinen EI Varoitusta', 'Todellinen KYLLÄ Varoitus'],\n",
        "                               columns=['Ennuste EI', 'Ennuste KYLLÄ'])\n",
        "            print(cm_df)\n",
        "            print(\"\\nLuokitteluraportti varoituksille (GRU - Jaksoittain):\")\n",
        "            report = classification_report(warnings_actual, warnings_pred, target_names=['Ei Varoitusta', 'Varoitus'], labels=[False, True], zero_division=0)\n",
        "            print(report)\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "            recall_warning = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            precision_warning = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            print(f\"\\n---> TÄRKEIMMÄT VAROITUSMETRIIKAT (Jaksoittain):\")\n",
        "            print(f\"  Recall (Herkkyys) 'Varoitus'-luokalle: {recall_warning:.4f}\")\n",
        "            print(f\"  Precision (Tarkkuus) 'Varoitus'-luokalle: {precision_warning:.4f}\")\n",
        "        else:\n",
        "            print(f\"\\nEi todellisia eikä ennustettuja varoitusjaksoja testidatassa kynnysarvolla {o3_threshold_8h} µg/m³.\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- evaluate_model_performance_pytorch -funktion suoritus päättyi onnistuneesti ---\")\n",
        "        # Palauta alkuperäiset ennusteet ja kohteet (muokattu 2D-muotoon)\n",
        "        return all_preds_orig, targets_eval # Palautetaan 2D muodot\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n-----> VIRHE evaluate_model_performance_pytorch -FUNKTIOSSA <-----\")\n",
        "        print(f\"Virhetyyppi: {type(e).__name__}\")\n",
        "        print(f\"Virheilmoitus: {e}\")\n",
        "        print(\"Traceback:\")\n",
        "        traceback.print_exc()\n",
        "        print(\"---------------------------------------------------------\")\n",
        "        print(\"Palautetaan None, None, koska arviointi epäonnistui.\")\n",
        "        return None, None # Palauta None virhetilanteessa\n",
        "\n",
        "\n",
        "print(\"\\nOsa 7: Arviointifunktioiden Määrittely (GRU v3) - OK\")"
      ],
      "metadata": {
        "id": "HZCCAe9S2RNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Mallin Arviointi (Suoritus) (GRU v3)\n",
        "\n",
        "import traceback\n",
        "import numpy as np # Varmistetaan tuonti\n",
        "import pandas as pd # Varmistetaan tuonti\n",
        "\n",
        "print(\"--- Aloitetaan Mallin Arviointi (Suoritus) ---\")\n",
        "\n",
        "# Alustetaan tulosmuuttujat Noneksi\n",
        "test_preds_orig_gru = None\n",
        "test_targets_orig_gru = None\n",
        "evaluation_gru_successful = False # Lipuke onnistumiselle\n",
        "\n",
        "# Varmistetaan, että kaikki tarvittavat muuttujat edellisistä osista ovat olemassa\n",
        "required_eval_vars = ['model', 'test_loader', 'device', 'o3_scaler',\n",
        "                      'O3_THRESHOLD_8H_AVG', 'PREDICTION_HORIZON',\n",
        "                      'evaluate_model_performance_pytorch'] # Tarkista myös funktion olemassaolo\n",
        "missing_eval_vars = []\n",
        "for var in required_eval_vars:\n",
        "     # Tarkista olemassaolo JA ettei arvo ole None (paitsi model voi olla None jos koulutus epäonnistui)\n",
        "     if var not in locals() or (locals()[var] is None and var != 'model'):\n",
        "          missing_eval_vars.append(var)\n",
        "\n",
        "# Tarkistetaan erikseen, onko malli None (koulutus epäonnistui?)\n",
        "if 'model' not in locals() or model is None:\n",
        "     missing_eval_vars.append('model (ei koulutettu/epäonnistui)')\n",
        "\n",
        "if not missing_eval_vars:\n",
        "    # --- Suoritetaan arviointi ---\n",
        "    try:\n",
        "        print(\"\\nKutsutaan evaluate_model_performance_pytorch...\")\n",
        "        # Huom: Funktio palauttaa ennusteet ja kohteet 2D-muodossa (samples, horizon)\n",
        "        test_preds_orig_gru, test_targets_orig_gru = evaluate_model_performance_pytorch(\n",
        "            model,\n",
        "            test_loader,\n",
        "            device,\n",
        "            o3_scaler,\n",
        "            O3_THRESHOLD_8H_AVG, # Käyttää Osa 1:ssä määriteltyä (tai Osa 9:ssä muokattua, jos tämä olisi ARIMA-notebook)\n",
        "            PREDICTION_HORIZON\n",
        "        )\n",
        "\n",
        "        # Tarkistetaan paluuarvot\n",
        "        if test_preds_orig_gru is not None and test_targets_orig_gru is not None:\n",
        "            print(\"\\nArviointifunktion ajo suoritettu.\")\n",
        "            # Tarkistetaan vielä tyypit ja muodot varmuuden vuoksi\n",
        "            if isinstance(test_preds_orig_gru, np.ndarray) and isinstance(test_targets_orig_gru, np.ndarray) \\\n",
        "               and test_preds_orig_gru.ndim == 2 and test_targets_orig_gru.ndim == 2 \\\n",
        "               and test_preds_orig_gru.shape == test_targets_orig_gru.shape:\n",
        "                 print(f\"Arviointi palautti validit tulokset muodoissa: Preds={test_preds_orig_gru.shape}, Targets={test_targets_orig_gru.shape}\")\n",
        "                 evaluation_gru_successful = True\n",
        "                 # Tallennetaan tulokset myöhempää käyttöä varten (ne ovat jo muuttujissa)\n",
        "            else:\n",
        "                 print(\"VIRHE: Arviointifunktion palauttamat arvot eivät ole odotettuja numpy arrayta tai muodot eivät täsmää.\")\n",
        "                 test_preds_orig_gru, test_targets_orig_gru = None, None # Nollataan virheelliset tulokset\n",
        "\n",
        "        else:\n",
        "            print(\"\\nArviointi epäonnistui (evaluate_model_performance_pytorch palautti None).\")\n",
        "            # Muuttujat ovat jo None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nVIRHE arvioinnin suorituksessa: {e}\")\n",
        "        traceback.print_exc()\n",
        "        # Varmistetaan, että muuttujat ovat None virheen sattuessa\n",
        "        test_preds_orig_gru, test_targets_orig_gru = None, None\n",
        "\n",
        "else:\n",
        "    print(f\"\\nArviointia ei voida suorittaa, koska yksi tai useampi tarvittava muuttuja puuttuu tai on None: {missing_vars}\")\n",
        "    # Varmistetaan, että muuttujat ovat None\n",
        "    test_preds_orig_gru, test_targets_orig_gru = None, None\n",
        "\n",
        "\n",
        "# Tulostetaan lopputilanne\n",
        "if evaluation_gru_successful:\n",
        "     print(\"\\nOsa 8: Arvioinnin suoritus (GRU v3) - VALMIS\")\n",
        "else:\n",
        "     print(\"\\nOsa 8: Arvioinnin suoritus (GRU v3) - EPÄONNISTUI / OHITETTIIN\")"
      ],
      "metadata": {
        "id": "vcLkjAQ72cKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}