{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPGYlBJ7Q6z91j56AFKcWYM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrwiren/ilmanlaatu-ennuste-helsinki/blob/main/GRU_2025-04-11_0001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 0. Esitiedot ja Tavoite (GRU v4 - Laaj. FE + Soluittain) (2025-04-10 19:44)\n",
        "\"\"\"\n",
        "Otsoniennuste Helsinki - GRU-malli v4 (Laajennetulla FE:llä ja Vahvistuksilla)\n",
        "\n",
        "Tavoite:\n",
        "1. Ladata data, sisältäen pilvisyyden.\n",
        "2. Suorittaa laajennettu ominaisuusmuokkaus (FE).\n",
        "3. Määritellä ja kouluttaa GRU-malli käyttäen laajennettuja ominaisuuksia.\n",
        "4. Arvioida malli ja analysoida tulokset.\n",
        "5. Visualisoida.\n",
        "6. Toimitetaan solu kerrallaan selkeyden ja virheiden minimoimiseksi.\n",
        "\"\"\"\n",
        "print(\"--- Osa 0: Esitiedot (GRU v4) - OK ---\")"
      ],
      "metadata": {
        "id": "mu5Z3SzAz3bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Tuonnit ja Asetukset (Refaktoroitu - GRU) (2025-04-10 19:59) # Päivitetty aika\n",
        "\n",
        "# Peruskirjastot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import traceback\n",
        "import warnings\n",
        "import time # <--- LISÄTTY IMPORT\n",
        "\n",
        "# Sklearn\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report\n",
        "except ImportError: raise ImportError(\"scikit-learn puuttuu.\")\n",
        "\n",
        "# PyTorch\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "except ImportError: raise ImportError(\"PyTorch puuttuu.\")\n",
        "\n",
        "# Muut\n",
        "try:\n",
        "    from tqdm.notebook import tqdm\n",
        "    import seaborn as sns\n",
        "except ImportError: tqdm = None; sns = None; print(\"Tqdm/Seaborn puuttuu.\")\n",
        "\n",
        "\n",
        "# --- Yleisasetukset ---\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 7)\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "print(f\"Käytettävä laite: {device}\")\n",
        "\n",
        "# --- Data-asetukset ---\n",
        "BASE_GITHUB_URL = 'https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/'\n",
        "PARQUET_PATH = 'data/processed/processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet'\n",
        "DATA_URL = BASE_GITHUB_URL + PARQUET_PATH\n",
        "LOCAL_DATA_PATH = 'processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet'\n",
        "TARGET_COLUMN = 'Otsoni [µg/m³]'\n",
        "BASE_COLUMNS_TO_LOAD = [\n",
        "    'Otsoni [µg/m³]', 'Lämpötilan keskiarvo [°C]', 'Keskituulen nopeus [m/s]',\n",
        "    'Ilmanpaineen keskiarvo [hPa]', 'Tuulen suunnan keskiarvo [°]', 'Pilvisyys [okta]'\n",
        "]\n",
        "\n",
        "# --- Ennustus- ja Jakoasetukset ---\n",
        "FORECAST_HORIZON = 24\n",
        "SEQUENCE_LENGTH = 72\n",
        "TEST_SPLIT_RATIO = 0.15\n",
        "VALID_SPLIT_RATIO = 0.15\n",
        "\n",
        "# --- RNN/LSTM/GRU Mallin Hyperparametrit ---\n",
        "RNN_HYPERPARAMS = {\n",
        "    'model_type': 'GRU',  # Varmistetaan GRU\n",
        "    'input_size': None,    # Lasketaan myöhemmin\n",
        "    'hidden_size': 64,\n",
        "    'num_layers': 2,\n",
        "    'output_size': FORECAST_HORIZON,\n",
        "    'dropout_prob': 0.2\n",
        "}\n",
        "\n",
        "# --- Neuroverkkojen Koulutusparametrit ---\n",
        "TRAIN_HYPERPARAMS = {\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 75,\n",
        "    'patience': 10\n",
        "}\n",
        "\n",
        "# --- Arviointiasetukset ---\n",
        "O3_THRESHOLD_8H_AVG = 120 # µg/m³\n",
        "\n",
        "# --- Ajanotto ---\n",
        "script_start_time = time.time() # <--- LISÄTTY AJANOTON ALOITUS\n",
        "\n",
        "print(\"\\nOsa 1: Tuonnit ja Asetukset (GRU v4) - SUORITETTU ONNISTUNEESTI.\") # Päivitetty vahvistusviesti\n",
        "print(f\"Kohdemuuttuja: {TARGET_COLUMN}\")\n",
        "print(f\"Käytettävä RNN-tyyppi: {RNN_HYPERPARAMS['model_type']}\")\n",
        "print(f\"8h Keskiarvon kynnysarvo: {O3_THRESHOLD_8H_AVG} µg/m³\")"
      ],
      "metadata": {
        "id": "_hgb2MkC5x9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Funktiot Datan Lataukseen, FE:hen ja Luokittelukohteen Luontiin (GRU Luokittelu v1) (2025-04-10 21:25)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "except ImportError:\n",
        "    StandardScaler = None\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "except ImportError:\n",
        "    sns = None; plt = None\n",
        "# Varmistetaan tqdm tuonti\n",
        "try:\n",
        "    from tqdm.notebook import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, **kwargs: x # Korvike, jos tqdm puuttuu\n",
        "    print(\"Tqdm puuttuu.\")\n",
        "\n",
        "\n",
        "# --- Funktiot datan lataamiseen (samantyyppiset kuin ennen) ---\n",
        "def download_data(url, local_path):\n",
        "    \"\"\"Lataa tiedoston URL:sta paikalliseen polkuun.\"\"\"\n",
        "    try:\n",
        "        print(f\"Yritetään ladata dataa: {url[:60]}...\")\n",
        "        response = requests.get(url); response.raise_for_status()\n",
        "        target_dir = os.path.dirname(local_path);\n",
        "        if target_dir and not os.path.exists(target_dir): os.makedirs(target_dir, exist_ok=True)\n",
        "        with open(local_path, 'wb') as f: f.write(response.content)\n",
        "        print(f\"Data ladattu: {local_path}\"); return True\n",
        "    except Exception as e: print(f\"Latausvirhe: {e}\"); return False\n",
        "\n",
        "def load_parquet_data(filepath_or_url, local_cache_path=\"default_cache.parquet\"):\n",
        "    \"\"\"Lataa datan Parquet-tiedostosta (paikallisesti tai URL:sta, käyttää välimuistia).\"\"\"\n",
        "    filepath_to_read = None\n",
        "    # Käytetään globaalia LOCAL_DATA_PATH, jos se on määritelty\n",
        "    cache_path_to_use = local_cache_path\n",
        "    if 'LOCAL_DATA_PATH' in globals() and LOCAL_DATA_PATH:\n",
        "         cache_path_to_use = LOCAL_DATA_PATH\n",
        "\n",
        "    if filepath_or_url.startswith('http'):\n",
        "         if os.path.exists(cache_path_to_use): filepath_to_read = cache_path_to_use; print(f\"Käytetään välimuistia: {cache_path_to_use}\")\n",
        "         else: print(f\"Paikallista tiedostoa {cache_path_to_use} ei löytynyt.\"); filepath_to_read = cache_path_to_use if download_data(filepath_or_url, cache_path_to_use) else None\n",
        "    else: filepath_to_read = filepath_or_url\n",
        "    if not filepath_to_read or not os.path.exists(filepath_to_read): print(f\"VIRHE: Tiedostoa '{filepath_to_read}' ei löytynyt.\"); return None\n",
        "    try:\n",
        "        print(f\"Ladataan dataa: {filepath_to_read}\"); df = pd.read_parquet(filepath_to_read); print(f\"Parquet ladattu ({df.shape})\")\n",
        "        if not isinstance(df.index, pd.DatetimeIndex): df.index = pd.to_datetime(df.index)\n",
        "        if df.index.tz is None: print(\"Asetetaan TZ=Europe/Helsinki...\"); df = df.tz_localize('Europe/Helsinki', ambiguous='NaT', nonexistent='NaT')\n",
        "        nat_rows = df.index.isnull();\n",
        "        if nat_rows.any(): print(f\"VAROITUS: Poistetaan {nat_rows.sum()} NaT-indeksiriviä...\"); df = df[~nat_rows]\n",
        "        df.sort_index(inplace=True); print(f\"Data aikaväliltä: [{df.index.min()} - {df.index.max()}]\")\n",
        "        if df.isnull().any().any(): print(\"VAROITUS: NaN-arvoja datassa. Täytetään...\"); df=df.ffill().bfill(); df.dropna(inplace=True)\n",
        "        print(f\"Lopullinen ladattu muoto: {df.shape}\"); return df\n",
        "    except Exception as e: print(f\"VIRHE tiedoston luvussa/käsittelyssä: {e}\"); traceback.print_exc(); return None\n",
        "\n",
        "# --- Laajennettu Feature Engineering funktio (sama kuin aiemmin) ---\n",
        "def feature_engineer_advanced_gru(df, target_column):\n",
        "    \"\"\"Lisää aika-, syklisiä, viive- ja liukuvia ominaisuuksia.\"\"\"\n",
        "    print(\"\\nSuoritetaan laajennettu FE...\"); df_eng = df.copy()\n",
        "    if not isinstance(df_eng.index, pd.DatetimeIndex): print(\"VIRHE: Indeksi ei DatetimeIndex.\"); return None\n",
        "    try: # Aika\n",
        "        df_eng['hour'] = df_eng.index.hour; df_eng['dayofweek'] = df_eng.index.dayofweek; df_eng['dayofyear'] = df_eng.index.dayofyear\n",
        "        days_in_year = df_eng.index.is_leap_year.map({True: 366.0, False: 365.0})\n",
        "        df_eng['hour_sin']=np.sin(2*np.pi*df_eng['hour']/24.0); df_eng['hour_cos']=np.cos(2*np.pi*df_eng['hour']/24.0)\n",
        "        df_eng['dayofweek_sin']=np.sin(2*np.pi*df_eng['dayofweek']/7.0); df_eng['dayofweek_cos']=np.cos(2*np.pi*df_eng['dayofweek']/7.0)\n",
        "        df_eng['dayofyear_sin']=np.sin(2*np.pi*df_eng['dayofyear']/days_in_year); df_eng['dayofyear_cos']=np.cos(2*np.pi*df_eng['dayofyear']/days_in_year)\n",
        "        df_eng.drop(columns=['hour', 'dayofweek', 'dayofyear'], inplace=True); print(\"Lisätty sykliset aikaominaisuudet.\")\n",
        "    except Exception as e: print(f\"VIRHE aika FE: {e}\")\n",
        "    wind_dir_col_orig = 'Tuulen suunnan keskiarvo [°]' # Tuuli\n",
        "    if wind_dir_col_orig in df_eng.columns:\n",
        "        try:\n",
        "            wind_dir_numeric = pd.to_numeric(df_eng[wind_dir_col_orig], errors='coerce').ffill().bfill()\n",
        "            if wind_dir_numeric.notna().all():\n",
        "                 df_eng['wind_dir_sin']=np.sin(np.deg2rad(wind_dir_numeric)); df_eng['wind_dir_cos']=np.cos(np.deg2rad(wind_dir_numeric))\n",
        "                 df_eng.drop(columns=[wind_dir_col_orig], inplace=True); print(\"Muunnettu tuulen suunta sykliseksi.\")\n",
        "            else: print(f\"VAROITUS: Kaikkia tuulensuuntia ei voitu muuntaa numeroksi.\")\n",
        "        except Exception as e: print(f\"VIRHE tuulensuunta FE: {e}\"); df_eng.drop(columns=['wind_dir_sin','wind_dir_cos'], inplace=True, errors='ignore')\n",
        "    else: print(f\"Saraketta '{wind_dir_col_orig}' ei löytynyt.\")\n",
        "    # Viiveet\n",
        "    lag_config = { target_column: [1, 2, 3, 6, 12, 24, 48, 72, 168], 'Lämpötilan keskiarvo [°C]': [1, 6, 24], 'Pilvisyys [okta]': [1, 6, 24], 'Keskituulen nopeus [m/s]': [1, 6, 24] }\n",
        "    print(\"Luodaan viiveitä...\"); original_cols = list(df_eng.columns); lag_cols_added = 0\n",
        "    for col, lags in lag_config.items():\n",
        "        if col in original_cols:\n",
        "            for lag in lags: df_eng[f'{col}_lag_{lag}h'] = df_eng[col].shift(lag); lag_cols_added+=1\n",
        "        else: print(f\"VAROITUS: Sarake '{col}' puuttuu viiveitä varten.\")\n",
        "    print(f\"Viiveitä lisätty ({lag_cols_added} kpl).\")\n",
        "    # Liukuvat tilastot\n",
        "    rolling_config = { target_column: [3, 6, 12, 24], 'Lämpötilan keskiarvo [°C]': [6, 24], 'Pilvisyys [okta]': [6, 24] }\n",
        "    print(\"Luodaan liukuvia tilastoja...\"); rolling_cols_added = 0\n",
        "    for col, windows in rolling_config.items():\n",
        "         if col in original_cols:\n",
        "            for window in windows:\n",
        "                 df_eng[f'{col}_roll_mean_{window}h'] = df_eng[col].rolling(window=window, min_periods=1).mean()\n",
        "                 df_eng[f'{col}_roll_std_{window}h'] = df_eng[col].rolling(window=window, min_periods=1).std()\n",
        "                 rolling_cols_added += 2\n",
        "         else: print(f\"VAROITUS: Sarake '{col}' puuttuu tilastoja varten.\")\n",
        "    print(f\"Tilastoja lisätty ({rolling_cols_added} kpl).\")\n",
        "    # Siivous: Poista NaN-rivit alusta (syntyivät viiveistä/liukuvista)\n",
        "    initial_rows = len(df_eng); df_eng.dropna(inplace=True); rows_removed = initial_rows - len(df_eng)\n",
        "    if rows_removed > 0: print(f\"\\nPoistettu {rows_removed} riviä alun NaN-arvojen vuoksi.\")\n",
        "    print(f\"\\nFE valmis. Lopullinen muoto: {df_eng.shape}\"); return df_eng\n",
        "\n",
        "\n",
        "# --- UUSI FUNKTIO: Binääriluokan laskenta ---\n",
        "def calculate_binary_target(ozone_series, horizon, threshold, rolling_window=8):\n",
        "    \"\"\"\n",
        "    Laskee binäärisen kohdemuuttujan (0/1). Tarkistaa jokaiselle tunnille t,\n",
        "    ylittääkö rolling_window tunnin liukuva keskiarvo kynnysarvon\n",
        "    missään vaiheessa aikavälillä [t+1, t+horizon].\n",
        "    \"\"\"\n",
        "    print(f\"\\nLasketaan binääristä kohdemuuttujaa ({rolling_window}h ka > {threshold} seuraavan {horizon}h aikana)...\")\n",
        "    if not isinstance(ozone_series, pd.Series): print(\"VIRHE: Vaaditaan Pandas Series.\"); return None\n",
        "    if len(ozone_series) < horizon + rolling_window: print(\"VIRHE: Liian vähän dataa kohdeluokan laskentaan.\"); return None\n",
        "\n",
        "    target = pd.Series(np.nan, index=ozone_series.index, dtype=float) # Alusta NaN floatina\n",
        "\n",
        "    # Laske liukuvat keskiarvot tehokkaasti kerralla\n",
        "    rolling_avg = ozone_series.rolling(window=rolling_window, min_periods=1).mean()\n",
        "\n",
        "    # Tehokkaampi tapa löytää maksimi tulevasta ikkunasta:\n",
        "    # Käytä rolling maxia käänteisessä järjestyksessä tulevaisuuteen katsoen\n",
        "    # Tai yksinkertaisempi (mutta hitaampi): iteroi ja laske max jokaiselle ikkunalle\n",
        "    print(\"Iteroidaan aikasarjaa kohdeluokkien määrittämiseksi...\")\n",
        "    iterator = range(len(ozone_series) - horizon)\n",
        "    if tqdm: iterator = tqdm(iterator, desc=\"Lasketaan kohdeluokkia\")\n",
        "\n",
        "    results = [] # Kerää tulokset listaan ensin\n",
        "    for i in iterator:\n",
        "        future_window_rolling_avg = rolling_avg.iloc[i + 1 : i + 1 + horizon]\n",
        "        max_future_avg = future_window_rolling_avg.max() # Max ka kyseisessä ikkunassa\n",
        "        results.append(1 if max_future_avg > threshold else 0)\n",
        "\n",
        "    # Aseta lasketut arvot target Seriesiin (huomioi pituusero)\n",
        "    target.iloc[0:len(results)] = results\n",
        "\n",
        "    # Poistetaan lopusta ne rivit, joille targetia ei voitu laskea\n",
        "    target.dropna(inplace=True)\n",
        "    target = target.astype(int) # Muunna kokonaisluvuiksi (0 tai 1)\n",
        "\n",
        "    print(f\"Binäärinen kohde laskettu {len(target)} pisteelle.\")\n",
        "    if len(target) > 0:\n",
        "        print(f\"Luokkajakauma (0=Ei Ylitystä, 1=Ylitys): \\n{target.value_counts(normalize=True).round(4)}\")\n",
        "        print(f\"Ylityksiä yhteensä: {target.sum()}\")\n",
        "    else:\n",
        "        print(\"Kohdesarja on tyhjä.\")\n",
        "    return target\n",
        "\n",
        "# --- Sekvenssifunktio luokittelua varten ---\n",
        "def create_sequences_classification(features_scaled_df, binary_target_series, sequence_length):\n",
        "    \"\"\"\n",
        "    Luo syötesekvenssejä (X) ja niitä vastaavat binääriset kohdeluokat (y).\n",
        "    Kohdeluokka y[i] vastaa X[i]:n viimeistä aika-askelta.\n",
        "    \"\"\"\n",
        "    X_seq, y_binary = [], []\n",
        "    print(f\"\\nLuodaan luokittelusekvenssejä: sequence_length={sequence_length}\")\n",
        "    if not isinstance(features_scaled_df, pd.DataFrame): print(\"VIRHE: features_scaled_df ei DataFrame.\"); return np.array(X_seq), np.array(y_binary)\n",
        "    if not isinstance(binary_target_series, pd.Series): print(\"VIRHE: binary_target_series ei Series.\"); return np.array(X_seq), np.array(y_binary)\n",
        "\n",
        "    # Kohdista indeksit varmuuden vuoksi\n",
        "    common_index = features_scaled_df.index.intersection(binary_target_series.index)\n",
        "    if len(common_index) < sequence_length:\n",
        "         print(f\"VAROITUS: Liian vähän dataa ({len(common_index)}) kohdistuksen jälkeen.\")\n",
        "         return np.array(X_seq), np.array(y_binary)\n",
        "\n",
        "    features_aligned = features_scaled_df.loc[common_index]\n",
        "    target_aligned = binary_target_series.loc[common_index]\n",
        "    print(f\"Kohdistuksen jälkeen dataa: {len(features_aligned)}\")\n",
        "\n",
        "    # Iteroidaan niin, että viimeinen X päättyy targetin viimeiseen arvoon\n",
        "    # Tarvitaan dataa vähintään sequence_length verran\n",
        "    if len(features_aligned) < sequence_length:\n",
        "         print(\"VIRHE: Ei tarpeeksi dataa edes yhden sekvenssin luontiin.\")\n",
        "         return np.array(X_seq), np.array(y_binary)\n",
        "\n",
        "    for i in range(len(features_aligned) - sequence_length + 1):\n",
        "        # Ota ominaisuussekvenssi (numpy arrayna)\n",
        "        X_seq.append(features_aligned.iloc[i : i + sequence_length].values)\n",
        "        # Ota vastaava binäärinen target-arvo sekvenssin LOPPUhetkeltä\n",
        "        # iloc[i + sequence_length - 1] vastaa viimeistä pistettä X_seq[i]:ssä\n",
        "        y_binary.append(target_aligned.iloc[i + sequence_length - 1])\n",
        "\n",
        "    print(f\"Luotu {len(X_seq)} luokittelusekvenssiä.\")\n",
        "    X_seq = np.array(X_seq)\n",
        "    y_binary = np.array(y_binary) # Tämä on nyt 1D array (samples,)\n",
        "    print(f\"Palautetaan X_seq muoto: {X_seq.shape}, y_binary muoto: {y_binary.shape}\")\n",
        "    return X_seq, y_binary\n",
        "\n",
        "\n",
        "print(\"\\nOsa 2: Funktiot datan käsittelyyn (GRU Luokittelu v1) - OK\")"
      ],
      "metadata": {
        "id": "ikuGewg4LHpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Pääskriptin Suoritus: Datan Käsittely (GRU Luokittelu v1) (2025-04-10 21:28)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "# Tuodaan luokkapainoja varten\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# Varmistetaan muut tuonnit\n",
        "try: import torch; from torch.utils.data import TensorDataset, DataLoader\n",
        "except ImportError: raise ImportError(\"PyTorch puuttuu.\")\n",
        "try: from sklearn.preprocessing import StandardScaler\n",
        "except ImportError: raise ImportError(\"scikit-learn puuttuu.\")\n",
        "# Varmistetaan funktioiden olemassaolo\n",
        "if 'load_parquet_data' not in locals(): raise NameError(\"load_parquet_data puuttuu.\")\n",
        "if 'feature_engineer_advanced_gru' not in locals(): raise NameError(\"feature_engineer_advanced_gru puuttuu.\")\n",
        "if 'calculate_binary_target' not in locals(): raise NameError(\"calculate_binary_target puuttuu.\")\n",
        "if 'create_sequences_classification' not in locals(): raise NameError(\"create_sequences_classification puuttuu.\")\n",
        "\n",
        "\n",
        "print(\"--- Aloitetaan Datan Käsittely (GRU Luokittelu v1) ---\")\n",
        "\n",
        "# Alustetaan muuttujat\n",
        "df_raw_full=None; df_engineered=None; y_binary_target_full=None; df_features_aligned=None; y_binary_aligned=None\n",
        "train_loader=None; valid_loader=None; test_loader=None\n",
        "feature_scaler=None; INPUT_SIZE=None; test_timestamps=None\n",
        "X_train_seq, y_train_final, X_valid_seq, y_valid_final, X_test_seq, y_test_final = [None]*6\n",
        "pos_weight_tensor = None # Luokkapainolle\n",
        "data_processing_ok = False\n",
        "\n",
        "try:\n",
        "    # Varmistetaan parametrit\n",
        "    if 'SEQUENCE_LENGTH' not in locals(): SEQUENCE_LENGTH = 72\n",
        "    if 'PREDICTION_HORIZON' not in locals(): PREDICTION_HORIZON = 24\n",
        "    if 'TARGET_COLUMN_REGRESSION' not in locals(): TARGET_COLUMN_REGRESSION = 'Otsoni [µg/m³]'\n",
        "    if 'CLASSIFICATION_THRESHOLD' not in locals(): CLASSIFICATION_THRESHOLD = 85\n",
        "    if 'TRAIN_HYPERPARAMS' not in locals() or 'batch_size' not in TRAIN_HYPERPARAMS: TRAIN_HYPERPARAMS = {'batch_size': 64}\n",
        "    print(f\"Käytetään: SeqLen={SEQUENCE_LENGTH}, PredHoriz={PREDICTION_HORIZON}, Threshold={CLASSIFICATION_THRESHOLD}, Batch={TRAIN_HYPERPARAMS['batch_size']}\")\n",
        "\n",
        "    # 1. Lataa raakadata (sisältää alkuperäisen otsonin)\n",
        "    df_raw_full = load_parquet_data(DATA_URL, LOCAL_DATA_PATH)\n",
        "    if df_raw_full is None or df_raw_full.empty: raise ValueError(\"Datan lataus epäonnistui.\")\n",
        "    if TARGET_COLUMN_REGRESSION not in df_raw_full.columns: raise ValueError(f\"Saraketta '{TARGET_COLUMN_REGRESSION}' ei löydy kohdeluokan laskentaan.\")\n",
        "\n",
        "    # 2. Laske binäärinen kohdemuuttuja (käyttäen alkuperäistä otsonia)\n",
        "    y_binary_target_full = calculate_binary_target(\n",
        "        df_raw_full[TARGET_COLUMN_REGRESSION],\n",
        "        horizon=PREDICTION_HORIZON,\n",
        "        threshold=CLASSIFICATION_THRESHOLD,\n",
        "        rolling_window=8 # Käytetään 8h keskiarvoa targetissa\n",
        "    )\n",
        "    if y_binary_target_full is None or y_binary_target_full.empty: raise ValueError(\"Binäärisen kohdemuuttujan laskenta epäonnistui.\")\n",
        "\n",
        "    # 3. Suorita Laajennettu Feature Engineering (käyttää df_raw_fullia)\n",
        "    # HUOM: FE tehdään koko datalle ennen kohdistusta targettiin, jotta vältetään data leakage viiveistä/liukuvista\n",
        "    df_engineered = feature_engineer_advanced_gru(df_raw_full.copy(), TARGET_COLUMN_REGRESSION)\n",
        "    if df_engineered is None or df_engineered.empty: raise ValueError(\"Laajennettu FE epäonnistui.\")\n",
        "\n",
        "    # 4. Kohdista ominaisuudet ja binäärinen kohde\n",
        "    print(\"\\nKohdistetaan ominaisuudet ja binäärinen kohde...\")\n",
        "    common_index = df_engineered.index.intersection(y_binary_target_full.index)\n",
        "    if len(common_index) == 0: raise ValueError(\"Ei yhteisiä indeksejä ominaisuuksien ja kohteen välillä.\")\n",
        "    df_features_aligned = df_engineered.loc[common_index]\n",
        "    y_binary_aligned = y_binary_target_full.loc[common_index]\n",
        "    print(f\"Kohdistuksen jälkeen dataa: {len(df_features_aligned)} riviä.\")\n",
        "\n",
        "    # 5. Määritä lopulliset ominaisuudet ja INPUT_SIZE\n",
        "    FINAL_FEATURE_COLUMNS = df_features_aligned.columns.tolist()\n",
        "    INPUT_SIZE = len(FINAL_FEATURE_COLUMNS)\n",
        "    if INPUT_SIZE == 0: raise ValueError(\"Ei lopullisia ominaisuuksia.\");\n",
        "    print(f\"\\nLopullinen ominaisuuksien määrä (INPUT_SIZE): {INPUT_SIZE}\")\n",
        "\n",
        "    # 6. Jaa data harjoitus-, validointi- ja testijoukkoihin (sekä X että y)\n",
        "    n_aligned = len(df_features_aligned)\n",
        "    test_split_idx_aligned = int(n_aligned * (1 - TEST_SPLIT_RATIO))\n",
        "    valid_split_idx_aligned = int(test_split_idx_aligned * (1 - VALID_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO)))\n",
        "\n",
        "    df_train = df_features_aligned[:valid_split_idx_aligned]\n",
        "    y_train_binary = y_binary_aligned[:valid_split_idx_aligned]\n",
        "\n",
        "    df_valid = df_features_aligned[valid_split_idx_aligned:test_split_idx_aligned]\n",
        "    y_valid_binary = y_binary_aligned[valid_split_idx_aligned:test_split_idx_aligned]\n",
        "\n",
        "    df_test = df_features_aligned[test_split_idx_aligned:]\n",
        "    y_test_binary = y_binary_aligned[test_split_idx_aligned:]\n",
        "\n",
        "    print(f\"\\nDatan jako (FE + Kohdistettu):\")\n",
        "    print(f\"Train: X={df_train.shape}, y={y_train_binary.shape}\")\n",
        "    print(f\"Valid: X={df_valid.shape}, y={y_valid_binary.shape}\")\n",
        "    print(f\"Test:  X={df_test.shape}, y={y_test_binary.shape}\")\n",
        "\n",
        "    # 7. Tarkista luokkaepätasapaino harjoitusdatassa ja laske pos_weight\n",
        "    print(\"\\nTarkistetaan luokkaepätasapaino harjoitusdatassa...\")\n",
        "    class_counts = y_train_binary.value_counts()\n",
        "    print(class_counts)\n",
        "    if len(class_counts) < 2:\n",
        "         print(\"VAROITUS: Harjoitusdatassa vain yhtä luokkaa! Luokkapainoa ei voi laskea.\")\n",
        "         pos_weight = 1.0 # Oletus, ei painotusta\n",
        "    else:\n",
        "        try:\n",
        "             neg_count = class_counts.get(0, 0)\n",
        "             pos_count = class_counts.get(1, 0)\n",
        "             if pos_count == 0:\n",
        "                  print(\"VAROITUS: Ei positiivisia näytteitä harjoitusdatassa!\")\n",
        "                  pos_weight = 1.0\n",
        "             else:\n",
        "                  # Laske paino positiiviselle luokalle (yleisempi tapa on neg/pos)\n",
        "                  pos_weight = neg_count / pos_count\n",
        "                  print(f\"Laskettu pos_weight: {pos_weight:.4f} (käytetään BCEWithLogitsLossissa)\")\n",
        "             # Muunnetaan tensoriksi laitteelle\n",
        "             pos_weight_tensor = torch.tensor([pos_weight], dtype=torch.float32).to(device)\n",
        "        except Exception as e_pw:\n",
        "             print(f\"VIRHE pos_weight laskennassa: {e_pw}\")\n",
        "             pos_weight_tensor = torch.tensor([1.0], dtype=torch.float32).to(device) # Oletus jos virhe\n",
        "\n",
        "    # 8. Skaalaa ominaisuudet (X)\n",
        "    print(\"\\nSkaalataan ominaisuudet...\")\n",
        "    feature_scaler = StandardScaler()\n",
        "    # Sovita VAIN df_train:iin\n",
        "    scaled_train_features_df = pd.DataFrame(feature_scaler.fit_transform(df_train), index=df_train.index, columns=df_train.columns)\n",
        "    scaled_valid_features_df = pd.DataFrame(feature_scaler.transform(df_valid), index=df_valid.index, columns=df_valid.columns)\n",
        "    scaled_test_features_df = pd.DataFrame(feature_scaler.transform(df_test), index=df_test.index, columns=df_test.columns)\n",
        "    print(\"Ominaisuudet skaalattu.\")\n",
        "\n",
        "    # 9. Luo sekvenssit luokittelua varten\n",
        "    # Käytetään skaalattuja ominaisuus-DataFrameja ja binäärisiä target-Seriesejä\n",
        "    X_train_seq, y_train_final = create_sequences_classification(scaled_train_features_df, y_train_binary, SEQUENCE_LENGTH)\n",
        "    X_valid_seq, y_valid_final = create_sequences_classification(scaled_valid_features_df, y_valid_binary, SEQUENCE_LENGTH)\n",
        "    X_test_seq, y_test_final = create_sequences_classification(scaled_test_features_df, y_test_binary, SEQUENCE_LENGTH)\n",
        "\n",
        "    if X_train_seq.size == 0 or X_valid_seq.size == 0 or X_test_seq.size == 0:\n",
        "         raise ValueError(\"Sekvenssien luonti epäonnistui luokittelua varten.\")\n",
        "\n",
        "    # 10. Muunna PyTorch Tensoreiksi\n",
        "    print(\"\\nMuunnetaan data PyTorch-tensoreiksi...\")\n",
        "    X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
        "    # Kohde (y) on nyt 1D (samples,), muutetaan floatiksi ja lisätään yksi dimensio lossia varten (samples, 1)\n",
        "    y_train_tensor = torch.tensor(y_train_final, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    X_valid_tensor = torch.tensor(X_valid_seq, dtype=torch.float32)\n",
        "    y_valid_tensor = torch.tensor(y_valid_final, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test_final, dtype=torch.float32).unsqueeze(1) # Käytetään tätä arvioinnissa\n",
        "    print(\"Tensorit luotu.\")\n",
        "\n",
        "    # 11. Luo DataLoaderit\n",
        "    batch_size = TRAIN_HYPERPARAMS.get('batch_size', 64)\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Sekoita ja pudota viimeinen treenidatalle\n",
        "\n",
        "    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor) # Testilataaja tarvitsee X:n ja binäärisen y:n\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    print(\"DataLoaderit luotu.\")\n",
        "\n",
        "    # 12. Tallenna testiaikaleimat (vastaavat sekvenssien loppupisteitä)\n",
        "    print(\"\\nTallennetaan testiaikaleimoja...\")\n",
        "    # Käytetään df_test indeksiä ja X_test_seq pituutta\n",
        "    if 'df_test' in locals() and not df_test.empty and len(df_test) >= SEQUENCE_LENGTH and \\\n",
        "       'X_test_seq' in locals() and len(X_test_seq) > 0:\n",
        "        try:\n",
        "            # Indeksi vastaa sekvenssin viimeistä pistettä\n",
        "            test_end_indices_loc = df_features_aligned.index.get_loc(df_test.index[0]) + SEQUENCE_LENGTH - 1\n",
        "            test_timestamps = df_features_aligned.index[test_end_indices_loc : test_end_indices_loc + len(X_test_seq)]\n",
        "\n",
        "            if len(test_timestamps) == len(X_test_seq):\n",
        "                print(f\"Testiaikaleimat (sekv. loppu) tallennettu ({len(test_timestamps)} kpl). Alkaa: {test_timestamps.min()}, Päättyy: {test_timestamps.max()}\")\n",
        "            else:\n",
        "                 print(\"VAROITUS: Testiaikaleimojen pituus ei täsmää sekvenssien määrään.\")\n",
        "                 test_timestamps = None\n",
        "        except Exception as e_ts: print(f\"VIRHE testiaikaleimojen tallennuksessa: {e_ts}\"); test_timestamps = None\n",
        "    else: print(\"VAROITUS: Testidata liian lyhyt/tyhjä aikaleimoille.\"); test_timestamps = None\n",
        "\n",
        "\n",
        "    print(f\"\\nLopulliset muodot DataLoadereille:\")\n",
        "    print(f\"X_train: {X_train_tensor.shape}, y_train: {y_train_tensor.shape}\")\n",
        "    print(f\"X_valid: {X_valid_tensor.shape}, y_valid: {y_valid_tensor.shape}\")\n",
        "    print(f\"X_test:  {X_test_tensor.shape}, y_test: {y_test_tensor.shape}\")\n",
        "\n",
        "    data_processing_ok = True\n",
        "\n",
        "except ValueError as ve: print(f\"\\n---> VIRHE DATAN KÄSITTELYSSÄ (ValueError): {ve} <---\"); traceback.print_exc(); train_loader=None;INPUT_SIZE=None; data_processing_ok = False\n",
        "except KeyError as ke: print(f\"\\n---> VIRHE DATAN KÄSITTELYSSÄ (KeyError): Saraketta {ke} ei löytynyt <---\"); traceback.print_exc(); train_loader=None;INPUT_SIZE=None; data_processing_ok = False\n",
        "except Exception as e: print(f\"\\n---> ODOTTAMATON VIRHE (Osa 3): {e} <---\"); traceback.print_exc(); train_loader=None;INPUT_SIZE=None; data_processing_ok = False\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if data_processing_ok:\n",
        "    print(\"\\n--- Osa 3: Valmis (Datan käsittely luokittelua varten) ---\")\n",
        "else:\n",
        "    print(\"\\n--- Osa 3: EPÄONNISTUI ---\")"
      ],
      "metadata": {
        "id": "8UtiBijELqst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. GRU-Mallin Määrittely (GRU Luokittelu v1) (2025-04-10 21:45) - Lisätty Param Varmistus\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import traceback\n",
        "import numpy as np # Varmistetaan tuonti assertia varten\n",
        "\n",
        "print(\"--- Määritellään GRU-malli (Luokitteluun) ---\")\n",
        "\n",
        "model_definition_ok = False\n",
        "model_params_ok = False\n",
        "\n",
        "# Yritetään varmistaa tarvittavat parametrit TÄSSÄ SOLUSSA\n",
        "try:\n",
        "    # Varmista INPUT_SIZE Osasta 3\n",
        "    if 'INPUT_SIZE' not in locals() or INPUT_SIZE is None:\n",
        "         # Yritetään ladata se uudelleen laskemalla sarakkeet df_engineeredistä, JOS se on olemassa\n",
        "         if 'df_engineered' in locals() and df_engineered is not None:\n",
        "              print(\"VAROITUS: INPUT_SIZE ei löytynyt. Lasketaan uudelleen df_engineered perusteella...\")\n",
        "              INPUT_SIZE = len(df_engineered.columns)\n",
        "              if INPUT_SIZE == 0: raise ValueError(\"INPUT_SIZE laskenta epäonnistui (0 saraketta).\")\n",
        "              print(f\"INPUT_SIZE asetettu: {INPUT_SIZE}\")\n",
        "         else:\n",
        "              raise NameError(\"INPUT_SIZE ei ole määritelty, eikä df_engineered löydy sen laskemiseksi (Aja Osa 3 onnistuneesti).\")\n",
        "    input_size = INPUT_SIZE # Käytä saatua arvoa\n",
        "\n",
        "    # Varmista/Määrittele RNN_HYPERPARAMS_CLF (kopio Osasta 1)\n",
        "    if 'RNN_HYPERPARAMS_CLF' not in locals() or RNN_HYPERPARAMS_CLF is None:\n",
        "        print(\"VAROITUS: RNN_HYPERPARAMS_CLF ei löytynyt. Määritellään oletusarvot...\")\n",
        "        RNN_HYPERPARAMS_CLF = {\n",
        "            'model_type': 'GRU', 'input_size': input_size, # Käytä laskettua input_sizea\n",
        "            'hidden_size': 64, 'num_layers': 2, 'output_size': 1, 'dropout_prob': 0.2\n",
        "        }\n",
        "    else:\n",
        "         # Varmista/päivitä arvot olemassaolevassa sanakirjassa\n",
        "         RNN_HYPERPARAMS_CLF['input_size'] = input_size # Varmista oikea input size\n",
        "         RNN_HYPERPARAMS_CLF['output_size'] = 1       # Varmista luokittelu output\n",
        "         RNN_HYPERPARAMS_CLF.setdefault('hidden_size', 64)\n",
        "         RNN_HYPERPARAMS_CLF.setdefault('num_layers', 2)\n",
        "         RNN_HYPERPARAMS_CLF.setdefault('dropout_prob', 0.2)\n",
        "\n",
        "\n",
        "    # Varmista/Määrittele device (kopio Osasta 1)\n",
        "    if 'device' not in locals() or device is None:\n",
        "         print(\"VAROITUS: device ei löytynyt. Määritellään uudelleen...\")\n",
        "         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "         print(f\"Laite asetettu: {device}\")\n",
        "\n",
        "    # Hae parametrit paikallisiin muuttujiin\n",
        "    hidden_size = RNN_HYPERPARAMS_CLF['hidden_size']\n",
        "    num_layers = RNN_HYPERPARAMS_CLF['num_layers']\n",
        "    output_size = RNN_HYPERPARAMS_CLF['output_size'] # = 1\n",
        "    dropout_prob = RNN_HYPERPARAMS_CLF['dropout_prob']\n",
        "\n",
        "    print(f\"Käytetään GRU-parametreja: Input={input_size}, Hidden={hidden_size}, Layers={num_layers}, Output={output_size}, Dropout={dropout_prob}\")\n",
        "    # Parametrien validointi\n",
        "    assert all(isinstance(v, int) and v > 0 for v in [input_size, hidden_size, num_layers, output_size])\n",
        "    assert isinstance(dropout_prob, float) and 0.0 <= dropout_prob < 1.0\n",
        "    model_params_ok = True\n",
        "\n",
        "except (NameError, KeyError, TypeError, AssertionError, ValueError) as param_err:\n",
        "     print(f\"VIRHE hyperparametreissa tai niiden määrittelyssä: {param_err}. Varmista, että Osat 1 ja 3 on ajettu onnistuneesti.\")\n",
        "     model_params_ok = False\n",
        "\n",
        "# Määritellään malliluokka vain jos parametrit ok\n",
        "if model_params_ok:\n",
        "    try:\n",
        "        class GRUModel(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
        "                super(GRUModel, self).__init__()\n",
        "                self.hidden_size = hidden_size; self.num_layers = num_layers\n",
        "                gru_dropout = dropout_prob if num_layers > 1 else 0.0\n",
        "                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=gru_dropout)\n",
        "                self.fc = nn.Linear(hidden_size, output_size) # output_size = 1\n",
        "            def forward(self, x):\n",
        "                # Varmista device tässäkin scopessa\n",
        "                current_device = x.device\n",
        "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(current_device)\n",
        "                out, _ = self.gru(x, h0)\n",
        "                out = out[:, -1, :]\n",
        "                out = self.fc(out)\n",
        "                return out\n",
        "\n",
        "        # Testataan alustus\n",
        "        temp_model_test = GRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
        "        print(\"\\nGRUModel (Luokittelu) -luokka määritelty ja alustus testattu.\")\n",
        "        print(\"Mallin rakenne:\")\n",
        "        print(temp_model_test)\n",
        "        del temp_model_test; model_definition_ok = True\n",
        "\n",
        "    except Exception as e_model_def:\n",
        "         print(f\"VIRHE GRUModel-luokan määrittelyssä tai alustuksessa: {e_model_def}\")\n",
        "         traceback.print_exc(); model_definition_ok = False\n",
        "else:\n",
        "     print(\"Ohitetaan mallin määrittely, koska parametreissa oli ongelma.\")\n",
        "     model_definition_ok = False # Varmistetaan Falseksi\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if model_definition_ok:\n",
        "    print(\"\\n--- Osa 4: Valmis (GRU-Luokittelumalli määritelty) ---\")\n",
        "else:\n",
        "    print(\"\\n--- Osa 4: EPÄONNISTUI / OHITETTIIN ---\")"
      ],
      "metadata": {
        "id": "vLgcZ86rMyT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Koulutusfunktion Määrittely (GRU Luokittelu v1) (2025-04-10 21:35)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm # Varmista tqdm tuonti\n",
        "import copy\n",
        "import traceback\n",
        "\n",
        "print(\"--- Määritellään train_model -funktio (Luokitteluun) ---\")\n",
        "training_function_ok = False\n",
        "try:\n",
        "    # Varmistetaan tuonnit\n",
        "    if 'DataLoader' not in globals(): raise NameError(\"DataLoader puuttuu.\")\n",
        "    if 'nn' not in globals() or 'optim' not in globals(): raise NameError(\"PyTorch moduulit puuttuvat.\")\n",
        "    if 'tqdm' not in globals() or tqdm is None: tqdm = lambda x, **kwargs: x; print(\"Tqdm puuttuu, korvataan.\")\n",
        "\n",
        "    # HUOM: Tämä funktio on nyt LUOKITTELUA varten.\n",
        "    # Se olettaa, että criterion on BCEWithLogitsLoss ja targetit ovat muotoa (batch, 1).\n",
        "    def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, device, patience):\n",
        "        \"\"\"Kouluttaa luokittelumallin ja käyttää Early Stoppingia.\"\"\"\n",
        "        train_losses = []; valid_losses = []; best_valid_loss = float('inf'); epochs_no_improve = 0; best_model_state = None\n",
        "        # Tarkistukset argumenteille\n",
        "        if not isinstance(model, nn.Module): raise TypeError(\"Vaaditaan PyTorch-malli.\")\n",
        "        if not isinstance(train_loader, DataLoader): raise TypeError(\"Vaaditaan DataLoader train_loaderille.\")\n",
        "        if not isinstance(valid_loader, DataLoader): raise TypeError(\"Vaaditaan DataLoader valid_loaderille.\")\n",
        "        # Emme voi tarkistaa criterionin tyyppiä täsmälleen (voi olla esim. painotettu versio),\n",
        "        # mutta varmistetaan, että se on kutsuttava objekti.\n",
        "        if not callable(criterion): raise TypeError(\"Criterion ei ole kutsuttava.\")\n",
        "        if not isinstance(optimizer, optim.Optimizer): raise TypeError(\"Vaaditaan PyTorch optimoija.\")\n",
        "        if not isinstance(epochs, int) or epochs <= 0: raise ValueError(\"Epochs > 0.\")\n",
        "        if not isinstance(patience, int) or patience <= 0: raise ValueError(\"Patience > 0.\")\n",
        "\n",
        "        print(f\"\\nAloitetaan luokittelumallin koulutus {epochs} epochilla (patience={patience})...\")\n",
        "        if len(train_loader.dataset) == 0 or len(valid_loader.dataset) == 0 :\n",
        "            print(\"VIRHE: Train tai Valid Dataset on tyhjä!\"); return None, None, None\n",
        "\n",
        "        for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "            model.train(); running_train_loss = 0.0; batch_count = 0\n",
        "            try: # Train loop\n",
        "                for inputs, targets in train_loader: # targets muoto (batch, 1)\n",
        "                    batch_count += 1; inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    optimizer.zero_grad(); outputs = model(inputs) # Logitit, muoto (batch, 1)\n",
        "\n",
        "                    # Varmista muodot (erityisen tärkeää BCEWithLogitsLossille)\n",
        "                    if outputs.shape != targets.shape:\n",
        "                         raise RuntimeError(f\"Muotovirhe (Train E{epoch+1} B{batch_count}): Out={outputs.shape}, Target={targets.shape}. Odotettiin samaa.\")\n",
        "\n",
        "                    loss = criterion(outputs, targets) # Lasketaan BCEWithLogitsLoss\n",
        "                    loss.backward(); optimizer.step()\n",
        "                    running_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # drop_last=True huomioitu jakajassa\n",
        "                denominator = len(train_loader.sampler) if hasattr(train_loader,'sampler') and train_loader.drop_last else len(train_loader.dataset)\n",
        "                epoch_train_loss = running_train_loss / denominator if denominator > 0 else 0\n",
        "                train_losses.append(epoch_train_loss)\n",
        "\n",
        "            except Exception as e_tr: print(f\"\\nVIRHE koulutusloopissa (E{epoch+1}): {e_tr}\"); traceback.print_exc(); return None, train_losses, valid_losses\n",
        "\n",
        "            # --- Validointivaihe ---\n",
        "            model.eval(); running_valid_loss = 0.0; valid_batch_count = 0\n",
        "            try: # Validation loop\n",
        "                with torch.no_grad():\n",
        "                    for inputs, targets in valid_loader: # targets muoto (batch, 1)\n",
        "                        valid_batch_count += 1; inputs, targets = inputs.to(device), targets.to(device)\n",
        "                        outputs = model(inputs) # Logitit (batch, 1)\n",
        "\n",
        "                        if outputs.shape != targets.shape:\n",
        "                             raise RuntimeError(f\"Muotovirhe (Valid E{epoch+1} B{valid_batch_count}): Out={outputs.shape}, Target={targets.shape}. Odotettiin samaa.\")\n",
        "\n",
        "                        loss = criterion(outputs, targets) # BCEWithLogitsLoss\n",
        "                        running_valid_loss += loss.item() * inputs.size(0) # Koko datasetti validoinnissa\n",
        "\n",
        "                epoch_valid_loss = running_valid_loss / len(valid_loader.dataset) if len(valid_loader.dataset) > 0 else 0\n",
        "                valid_losses.append(epoch_valid_loss)\n",
        "                print(f\"Epoch {epoch+1:02d}/{epochs} - Train Loss: {epoch_train_loss:.6f} - Valid Loss: {epoch_valid_loss:.6f}\", end=\"\")\n",
        "\n",
        "                # Early Stopping\n",
        "                if epoch_valid_loss < best_valid_loss:\n",
        "                    best_valid_loss = epoch_valid_loss; epochs_no_improve = 0\n",
        "                    try: best_model_state = copy.deepcopy(model.state_dict()); print(\" (Paras!)\")\n",
        "                    except Exception as e_state: print(f\" (TILA EI TALLENNETTU: {e_state})\"); best_model_state = None\n",
        "                else:\n",
        "                    epochs_no_improve += 1; print(f\" (Ei par.{epochs_no_improve}/{patience})\")\n",
        "                if epochs_no_improve >= patience: print(\"\\nEarly stopping.\"); break\n",
        "\n",
        "            except Exception as e_val: print(f\"\\nVIRHE validointiloopissa (E{epoch+1}): {e_val}\"); traceback.print_exc(); return model, train_losses, valid_losses\n",
        "\n",
        "        # Koulutusloopin jälkeen\n",
        "        if best_model_state: print(\"\\nLadataan paras malli.\"); model.load_state_dict(best_model_state)\n",
        "        elif epochs > 0 and train_losses is not None : print(\"\\nEi Early Stoppingia / paras tila viallinen. Käytetään viimeisintä.\")\n",
        "        else: print(\"\\nKoulutusta ei ajettu / keskeytyi.\")\n",
        "        return model, train_losses, valid_losses\n",
        "\n",
        "    training_function_ok = True\n",
        "except NameError as ne: print(f\"VIRHE: Tuonti puuttuu: {ne}\")\n",
        "except Exception as e_def: print(f\"VIRHE train_model määrittelyssä: {e_def}\"); traceback.print_exc()\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if training_function_ok: print(\"\\n--- Osa 5: Valmis (Luokittelun Koulutusfunktio määritelty) ---\")\n",
        "else: print(\"\\n--- Osa 5: EPÄONNISTUI ---\")"
      ],
      "metadata": {
        "id": "F30QtdVQNPIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Mallin Koulutus (Suoritus) (GRU Luokittelu v1) (2025-04-10 21:42) - Param Varmistus Lisätty\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "# Varmistetaan luokat ja funktiot varmuuden vuoksi\n",
        "# Nämä TÄYTYY olla määritelty aiemmissa soluissa (4 ja 5), tarkistetaan myöhemmin\n",
        "if 'GRUModel' not in locals(): print(\"VAROITUS: GRUModel luokkaa ei ehkä ole määritelty (Aja Osa 4)\"); GRUModel = None\n",
        "if 'train_model' not in locals(): print(\"VAROITUS: train_model funktiota ei ehkä ole määritelty (Aja Osa 5)\"); train_model = None\n",
        "\n",
        "print(\"--- Aloitetaan Mallin Koulutus (Suoritus - LUOKITTELU) ---\")\n",
        "\n",
        "# Alustetaan muuttujat\n",
        "model = None; model_gru_clf = None; train_losses = None; valid_losses = None\n",
        "training_run_ok = False\n",
        "prereqs_ok_for_training = True # Oletus, muutetaan Falseksi jos jokin puuttuu\n",
        "\n",
        "try:\n",
        "    # --- Varmista/Määrittele parametrit ja objektit TÄSSÄ SOLUSSA ---\n",
        "    print(\"Tarkistetaan/varmistetaan edellytykset...\")\n",
        "\n",
        "    # Datan käsittelystä (Osa 3)\n",
        "    vars_from_data = ['train_loader', 'valid_loader', 'INPUT_SIZE', 'pos_weight_tensor']\n",
        "    for var in vars_from_data:\n",
        "        if var not in locals() or locals()[var] is None:\n",
        "            print(f\"VIRHE: Kriittinen muuttuja Osasta 3 puuttuu: {var}. Aja Osa 3 uudelleen.\")\n",
        "            prereqs_ok_for_training = False\n",
        "            break # Lopetetaan tarkistus, jos jokin tärkeä puuttuu\n",
        "    if not prereqs_ok_for_training: raise ValueError(\"Datan käsittelyn tuloksia puuttuu.\")\n",
        "    input_size = INPUT_SIZE # Otetaan arvo talteen\n",
        "\n",
        "    # Parametrit (Osa 1)\n",
        "    if 'RNN_HYPERPARAMS_CLF' not in locals() or RNN_HYPERPARAMS_CLF is None:\n",
        "        print(\"VAROITUS: RNN_HYPERPARAMS_CLF ei löytynyt. Määritellään oletusarvot...\")\n",
        "        RNN_HYPERPARAMS_CLF = {'model_type':'GRU', 'input_size':input_size, 'hidden_size':64, 'num_layers':2, 'output_size':1, 'dropout_prob':0.2}\n",
        "    else: # Varmistetaan tärkeät avaimet ja arvot\n",
        "        RNN_HYPERPARAMS_CLF.setdefault('hidden_size', 64); RNN_HYPERPARAMS_CLF.setdefault('num_layers', 2);\n",
        "        RNN_HYPERPARAMS_CLF['output_size'] = 1; RNN_HYPERPARAMS_CLF.setdefault('dropout_prob', 0.2);\n",
        "\n",
        "    if 'TRAIN_HYPERPARAMS_CLF' not in locals() or TRAIN_HYPERPARAMS_CLF is None:\n",
        "        print(\"VAROITUS: TRAIN_HYPERPARAMS_CLF ei löytynyt. Määritellään oletusarvot...\")\n",
        "        TRAIN_HYPERPARAMS_CLF = {'batch_size': 64, 'learning_rate': 0.001, 'epochs': 75, 'patience': 15}\n",
        "    else: # Varmistetaan avaimet\n",
        "        TRAIN_HYPERPARAMS_CLF.setdefault('learning_rate', 0.001); TRAIN_HYPERPARAMS_CLF.setdefault('epochs', 75);\n",
        "        TRAIN_HYPERPARAMS_CLF.setdefault('patience', 15);\n",
        "\n",
        "    # Laite (Osa 1)\n",
        "    if 'device' not in locals() or device is None:\n",
        "         print(\"VAROITUS: device ei löytynyt. Määritellään uudelleen...\"); device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(f\"Laite: {device}\")\n",
        "\n",
        "    # Funktio/Luokkamäärittelyt (Osat 4 & 5)\n",
        "    if 'GRUModel' not in locals() or GRUModel is None: print(\"VIRHE: GRUModel-luokkaa ei löytynyt (Aja Osa 4).\"); prereqs_ok_for_training = False\n",
        "    if 'train_model' not in locals() or train_model is None: print(\"VIRHE: train_model-funktiota ei löytynyt (Aja Osa 5).\"); prereqs_ok_for_training = False\n",
        "\n",
        "    # Haetaan parametrit paikallisiin muuttujiin (jos edellytykset ok)\n",
        "    if prereqs_ok_for_training:\n",
        "        hidden_size=RNN_HYPERPARAMS_CLF['hidden_size']; num_layers=RNN_HYPERPARAMS_CLF['num_layers']\n",
        "        output_size=RNN_HYPERPARAMS_CLF['output_size']; dropout_prob=RNN_HYPERPARAMS_CLF['dropout_prob']\n",
        "        learning_rate=TRAIN_HYPERPARAMS_CLF['learning_rate']; epochs=TRAIN_HYPERPARAMS_CLF['epochs']; patience=TRAIN_HYPERPARAMS_CLF['patience']\n",
        "        print(\"Parametrit ja objektit tarkistettu/varmistettu Osa 6:lle.\")\n",
        "    else:\n",
        "         raise ValueError(\"Kaikkia koulutuksen edellytyksiä ei löytynyt.\")\n",
        "\n",
        "\n",
        "    # --- Mallin, häviöfunktion ja optimoijan alustus ---\n",
        "    model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout_prob).to(device)\n",
        "    print(f\"Käytetään pos_weightia BCEWithLogitsLossissa: {pos_weight_tensor.item():.4f}\")\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"\\n--- GRU-Luokittelumalli (ennen koulutusta) ---\")\n",
        "    print(model)\n",
        "    print(f\"Input size: {input_size}, Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "    print(\"-------------------------------------------\\n\")\n",
        "\n",
        "    # --- Koulutetaan malli ---\n",
        "    # Kutsu train_model funktiota (jonka pitäisi olla nyt muistissa)\n",
        "    model, train_losses, valid_losses = train_model(\n",
        "        model, train_loader, valid_loader, criterion, optimizer, epochs, device, patience\n",
        "    )\n",
        "\n",
        "    if model is not None and train_losses is not None and valid_losses is not None:\n",
        "        print(\"\\nKoulutus suoritettu.\")\n",
        "        model_gru_clf = model # Tallenna onnistunut malli\n",
        "        training_run_ok = True\n",
        "        # Piirrä häviökäyrät\n",
        "        if train_losses and valid_losses:\n",
        "            plt.figure(figsize=(10, 5)); plt.plot(train_losses, label='Training'); plt.plot(valid_losses, label='Validation')\n",
        "            plt.title('Loss (GRU Classification)'); plt.xlabel('Epoch'); plt.ylabel('Loss (BCE)')\n",
        "            try: min_loss=min(min(train_losses,default=1),min(valid_losses,default=1)); plt.yscale('log' if min_loss>1e-7 else 'linear')\n",
        "            except: plt.yscale('linear')\n",
        "            plt.legend(); plt.grid(True,alpha=0.6); plt.show()\n",
        "        else: print(\"Häviölistoja ei saatu.\")\n",
        "    else:\n",
        "        print(\"\\nKoulutus epäonnistui train_model-funktion sisällä.\")\n",
        "        model_gru_clf = None\n",
        "\n",
        "\n",
        "except (NameError, KeyError, AssertionError, ValueError, RuntimeError, TypeError) as e_prereq:\n",
        "     print(f\"\\nVIRHE edellytyksissä tai alustuksessa: {e_prereq}\")\n",
        "     traceback.print_exc()\n",
        "     model_gru_clf = None # Varmista Noneksi\n",
        "     training_run_ok = False # Merkitse epäonnistuneeksi\n",
        "except Exception as e_train_run:\n",
        "     print(f\"\\nODOTTAMATON VIRHE koulutuksen ajossa: {e_train_run}\")\n",
        "     traceback.print_exc()\n",
        "     model_gru_clf = None\n",
        "     training_run_ok = False\n",
        "\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if training_run_ok and model_gru_clf is not None:\n",
        "     print(\"\\n--- Osa 6: Valmis (Luokittelumallin koulutus) ---\")\n",
        "else:\n",
        "     print(\"\\n--- Osa 6: EPÄONNISTUI / OHITETTIIN ---\")"
      ],
      "metadata": {
        "id": "64HiGkyUPD1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Luokittelun Arviointifunktion Määrittely (GRU Luokittelu v1) (2025-04-10 21:50)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import traceback\n",
        "# Varmistetaan tuonnit\n",
        "try:\n",
        "    from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                                 roc_auc_score, precision_recall_curve, auc,\n",
        "                                 accuracy_score) # Lisätään accuracy\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "    from tqdm.notebook import tqdm\n",
        "    import matplotlib.pyplot as plt # Kuvaajia varten\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Tarvittava kirjasto puuttuu: {e}\")\n",
        "\n",
        "print(\"--- Määritellään evaluate_classification_model -funktio ---\")\n",
        "evaluation_functions_ok = False\n",
        "try:\n",
        "    def evaluate_classification_model(model, model_name, test_loader, device, decision_threshold=0.5):\n",
        "        \"\"\"\n",
        "        Arvioi binääristä luokittelumallia testidatalla.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): Koulutettu PyTorch-malli.\n",
        "            model_name (str): Mallin nimi tulosteisiin.\n",
        "            test_loader (DataLoader): Testidatan lataaja (tuottaa X, y_true).\n",
        "            device (torch.device): Laite (cpu tai cuda).\n",
        "            decision_threshold (float): Kynnysarvo todennäköisyyksille luokittelua varten.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Sisältää (all_targets_np, all_predictions_np, all_probabilities_np)\n",
        "                   tai (None, None, None) jos virhe.\n",
        "                   Muodot: (num_samples,)\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- evaluate_classification_model ({model_name}) alkaa ---\")\n",
        "        if model is None: print(\"Malli puuttuu.\"); return None, None, None\n",
        "        if test_loader is None: print(\"Testilataaja puuttuu.\"); return None, None, None\n",
        "        if not (0 < decision_threshold < 1): print(\"VAROITUS: decision_threshold ei ole välillä (0, 1).\"); decision_threshold = 0.5\n",
        "\n",
        "        model.eval() # Aseta malli arviointitilaan\n",
        "        all_outputs_list = [] # Kerätään mallin logitit\n",
        "        all_targets_list = [] # Kerätään todelliset luokat\n",
        "\n",
        "        print(\"Ennustetaan todennäköisyyksiä testidatalla...\")\n",
        "        try:\n",
        "            if len(test_loader.dataset) == 0: print(\"Testidata on tyhjä.\"); return None, None, None\n",
        "            with torch.no_grad():\n",
        "                # test_loader tuottaa (X_test_tensor, y_test_tensor), missä y on (batch, 1)\n",
        "                for inputs, targets in tqdm(test_loader, desc=f\"Testaus ({model_name})\"):\n",
        "                    inputs = inputs.to(device)\n",
        "                    # targets ei tarvitse siirtää laitteelle, jos sitä käytetään vain lopussa\n",
        "                    outputs = model(inputs) # Malli tuottaa logitit, muoto (batch, 1)\n",
        "                    all_outputs_list.append(outputs.cpu().numpy())\n",
        "                    all_targets_list.append(targets.cpu().numpy()) # Säilytetään muoto (batch, 1)\n",
        "\n",
        "            print(\"Ennusteet kerätty.\")\n",
        "            # Yhdistä kaikki erät\n",
        "            all_outputs_np = np.concatenate(all_outputs_list, axis=0) # Muoto (total_samples, 1)\n",
        "            all_targets_np = np.concatenate(all_targets_list, axis=0) # Muoto (total_samples, 1)\n",
        "            print(f\"Kerätty {all_outputs_np.shape[0]} ennustetta/kohdetta.\")\n",
        "\n",
        "            # --- Muunna logitit todennäköisyyksiksi ja ennusteiksi ---\n",
        "            # Käytä sigmoidia logitteihin -> todennäköisyydet\n",
        "            # Siirretään takaisin tensoreiksi sigmoidia varten (GPU-tuki)\n",
        "            with torch.no_grad():\n",
        "                 all_probabilities_tensor = torch.sigmoid(torch.tensor(all_outputs_np).to(device))\n",
        "                 all_probabilities_np = all_probabilities_tensor.cpu().numpy() # Muoto (total_samples, 1)\n",
        "\n",
        "            # Tee binääriset ennusteet kynnysarvon perusteella\n",
        "            all_predictions_np = (all_probabilities_np >= decision_threshold).astype(int) # Muoto (total_samples, 1)\n",
        "\n",
        "            # --- Metriikoiden Laskenta ---\n",
        "            print(\"\\nLasketaan luokittelumetriikat...\")\n",
        "            # Poistetaan viimeinen dimensio metriikoita varten (sklearn odottaa 1D-taulukoita)\n",
        "            targets_flat = all_targets_np.ravel()\n",
        "            predictions_flat = all_predictions_np.ravel()\n",
        "            probabilities_flat = all_probabilities_np.ravel() # Todennäköisyydet AUC:ia varten\n",
        "\n",
        "            print(f\"\\n--- {model_name} Luokittelun Arviointi (Kynnys={decision_threshold:.2f}) ---\")\n",
        "\n",
        "            # Sekaannusmatriisi\n",
        "            print(\"\\nSekaannusmatriisi:\")\n",
        "            cm = confusion_matrix(targets_flat, predictions_flat, labels=[0, 1]) # Määritä luokkien järjestys\n",
        "            print(pd.DataFrame(cm, index=['Todellinen 0 (EI)', 'Todellinen 1 (KYLLÄ)'],\n",
        "                               columns=['Ennuste 0', 'Ennuste 1']))\n",
        "\n",
        "            # Luokitteluraportti\n",
        "            print(\"\\nLuokitteluraportti:\")\n",
        "            report = classification_report(targets_flat, predictions_flat, target_names=['Luokka 0 (EI)', 'Luokka 1 (KYLLÄ)'], labels=[0, 1], zero_division=0)\n",
        "            print(report)\n",
        "\n",
        "            # AUC ROC\n",
        "            try:\n",
        "                auc_roc = roc_auc_score(targets_flat, probabilities_flat) # Käytä todennäköisyyksiä\n",
        "                print(f\"\\nAUC ROC Score: {auc_roc:.4f}\")\n",
        "            except ValueError as e_auc:\n",
        "                 # Tämä voi tapahtua, jos testidatassa on vain yhtä luokkaa\n",
        "                 print(f\"VAROITUS: AUC ROC -pisteytystä ei voitu laskea: {e_auc}\")\n",
        "                 auc_roc = None\n",
        "\n",
        "            # Tulostetaan tärkeimmät metriikat uudelleen selkeyden vuoksi\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "            recall_class1 = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            precision_class1 = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            accuracy = accuracy_score(targets_flat, predictions_flat)\n",
        "            print(\"\\n--- Yhteenveto ---\")\n",
        "            print(f\"  Accuracy:      {accuracy:.4f}\")\n",
        "            print(f\"  Recall (Luokka 1 - KYLLÄ): {recall_class1:.4f}  <-- TÄRKEIN?\")\n",
        "            print(f\"  Precision (Luokka 1 - KYLLÄ): {precision_class1:.4f}\")\n",
        "\n",
        "\n",
        "            print(f\"\\n--- evaluate_classification_model ({model_name}) päättyi ---\")\n",
        "            # Palautetaan litistetyt taulukot (1D)\n",
        "            return targets_flat, predictions_flat, probabilities_flat\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n-----> VIRHE evaluate_classification_model ({model_name}) <-----\")\n",
        "            traceback.print_exc(); return None, None, None\n",
        "\n",
        "    evaluation_functions_ok = True\n",
        "except NameError as ne: print(f\"VIRHE: Tuonti puuttuu: {ne}\")\n",
        "except Exception as e_def: print(f\"VIRHE arviointifunktioiden määrittelyssä: {e_def}\"); traceback.print_exc()\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if evaluation_functions_ok:\n",
        "    print(\"\\n--- Osa 7: Valmis (Luokittelun Arviointifunktiot määritelty) ---\")\n",
        "else:\n",
        "    print(\"\\n--- Osa 7: EPÄONNISTUI ---\")"
      ],
      "metadata": {
        "id": "qOfy4bA0PsBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Mallin Arviointi (Suoritus) (GRU Luokittelu v1) (2025-04-10 21:50)\n",
        "\n",
        "import traceback\n",
        "import numpy as np # Varmistetaan tuonti\n",
        "import pandas as pd # Varmistetaan tuonti\n",
        "\n",
        "print(\"--- Aloitetaan Mallin Arviointi (Suoritus - LUOKITTELU) ---\")\n",
        "\n",
        "# Alustetaan tulosmuuttujat\n",
        "true_labels_gru = None\n",
        "predictions_gru = None\n",
        "probabilities_gru = None\n",
        "evaluation_gru_clf_successful = False # Lipuke tälle ajolle\n",
        "\n",
        "# Tarkistetaan edellytykset\n",
        "required_eval_vars = ['model_gru_clf', 'test_loader', 'device',\n",
        "                      'O3_THRESHOLD_8H_AVG', # Vaikka ei suoraan käytetä, hyvä olla kontekstina\n",
        "                      'evaluate_classification_model'] # Tarkista funktion olemassaolo\n",
        "missing_eval_vars = []\n",
        "for var in required_eval_vars:\n",
        "     if var not in locals() or locals()[var] is None:\n",
        "          missing_eval_vars.append(var)\n",
        "\n",
        "if not missing_eval_vars:\n",
        "    # --- Suoritetaan arviointi ---\n",
        "    try:\n",
        "        print(\"\\nKutsutaan evaluate_classification_model...\")\n",
        "        # Aseta päätöksentekokynnys (yleensä 0.5, voit kokeilla säätää tätä myöhemmin)\n",
        "        DECISION_THRESHOLD = 0.95\n",
        "\n",
        "        # Käytetään Osassa 6 tallennettua model_gru_clf -mallia\n",
        "        true_labels_gru, predictions_gru, probabilities_gru = evaluate_classification_model(\n",
        "            model=model_gru_clf,\n",
        "            model_name=\"GRU (Laaj. FE - Luokittelu)\",\n",
        "            test_loader=test_loader,\n",
        "            device=device,\n",
        "            decision_threshold=DECISION_THRESHOLD\n",
        "        )\n",
        "\n",
        "        # Tarkistetaan paluuarvot\n",
        "        if true_labels_gru is not None and predictions_gru is not None and probabilities_gru is not None:\n",
        "            print(\"\\nLuokittelun arviointifunktion ajo suoritettu.\")\n",
        "            # Tarkistetaan tyypit ja pituudet\n",
        "            if isinstance(true_labels_gru, np.ndarray) and isinstance(predictions_gru, np.ndarray) and isinstance(probabilities_gru, np.ndarray) \\\n",
        "               and len(true_labels_gru) == len(predictions_gru) == len(probabilities_gru) and len(true_labels_gru) > 0:\n",
        "                 print(f\"Arviointi palautti validit tulokset ({len(true_labels_gru)} kpl).\")\n",
        "                 evaluation_gru_clf_successful = True\n",
        "            else:\n",
        "                 print(\"VIRHE: Arviointifunktion palautukset eivät ole odotettuja numpy arrayta tai pituudet eivät täsmää.\")\n",
        "                 true_labels_gru, predictions_gru, probabilities_gru = None, None, None # Nollataan\n",
        "        else:\n",
        "            print(\"\\nLuokittelun arviointi epäonnistui (funktio palautti None).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nVIRHE luokittelun arvioinnin suorituksessa: {e}\")\n",
        "        traceback.print_exc()\n",
        "        true_labels_gru, predictions_gru, probabilities_gru = None, None, None\n",
        "\n",
        "else:\n",
        "    print(f\"\\nArviointia ei voida suorittaa, koska yksi tai useampi tarvittava edellytys puuttuu: {missing_eval_vars}\")\n",
        "    true_labels_gru, predictions_gru, probabilities_gru = None, None, None\n",
        "\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if evaluation_gru_clf_successful:\n",
        "     print(\"\\n--- Osa 8: Valmis (Luokittelumallin arviointi) ---\")\n",
        "else:\n",
        "     print(\"\\n--- Osa 8: EPÄONNISTUI / OHITETTIIN ---\")"
      ],
      "metadata": {
        "id": "n3DVBKOlQAji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tarkista 120 µg/m³ ylitysten määrä datassa\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Varmistetaan tqdm tuonti\n",
        "try: from tqdm.notebook import tqdm\n",
        "except ImportError: tqdm = lambda x, **kwargs: x\n",
        "\n",
        "print(f\"--- Tarkistetaan 8h keskiarvon > 120 µg/m³ ylitysten määrä ---\")\n",
        "print(f\"(Käytetään kynnysarvoa: 120 µg/m³)\")\n",
        "\n",
        "# Varmistetaan tarvittavat muuttujat ja funktio edellisistä soluista\n",
        "if 'df_raw_full' in locals() and df_raw_full is not None and \\\n",
        "   'TARGET_COLUMN_REGRESSION' in locals() and TARGET_COLUMN_REGRESSION in df_raw_full.columns and \\\n",
        "   'PREDICTION_HORIZON' in locals() and PREDICTION_HORIZON is not None and \\\n",
        "   'calculate_binary_target' in locals() and callable(calculate_binary_target):\n",
        "\n",
        "    try:\n",
        "        threshold_to_check = 120\n",
        "        print(f\"\\nKäytetään dataa aikaväliltä: {df_raw_full.index.min()} - {df_raw_full.index.max()}\")\n",
        "        print(f\"Lasketaan kohdeluokka kynnyksellä {threshold_to_check} µg/m³...\")\n",
        "\n",
        "        # Kutsutaan funktiota koko datalle (ennen FE:n aiheuttamaa datan lyhenemistä)\n",
        "        # Funktio itse tulostaa jakauman ja summan\n",
        "        binary_target_120 = calculate_binary_target(\n",
        "            ozone_series=df_raw_full[TARGET_COLUMN_REGRESSION],\n",
        "            horizon=PREDICTION_HORIZON,\n",
        "            threshold=threshold_to_check, # Käytetään 120 rajaa\n",
        "            rolling_window=8\n",
        "        )\n",
        "\n",
        "        if binary_target_120 is None:\n",
        "             print(\"Kohdeluokan laskenta epäonnistui.\")\n",
        "        # Funktio tulosti jo tarvittavat tiedot, mutta lisätään selkeä yhteenveto\n",
        "        elif len(binary_target_120) > 0:\n",
        "            total_calculated = len(binary_target_120)\n",
        "            total_exceedances = binary_target_120.sum()\n",
        "            percentage_exceedances = binary_target_120.mean() * 100\n",
        "            print(\"\\n---> Yhteenveto (koko data, jolle kohde laskettiin):\")\n",
        "            print(f\"   Kohdeluokka laskettu {total_calculated} tunnille.\")\n",
        "            print(f\"   Kertaa, jolloin max 8h ka > {threshold_to_check} µg/m³ seuraavan 24h aikana: {total_exceedances} ({percentage_exceedances:.2f}%)\")\n",
        "        else:\n",
        "            print(\"Kohdesarja jäi tyhjäksi laskennan jälkeen.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Virhe tarkistuksen aikana: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    # Tulostetaan selkeämmin, mikä puuttuu\n",
        "    missing_check = []\n",
        "    if 'df_raw_full' not in locals() or df_raw_full is None: missing_check.append('df_raw_full')\n",
        "    if 'TARGET_COLUMN_REGRESSION' not in locals(): missing_check.append('TARGET_COLUMN_REGRESSION')\n",
        "    if 'PREDICTION_HORIZON' not in locals() or PREDICTION_HORIZON is None: missing_check.append('PREDICTION_HORIZON')\n",
        "    if 'calculate_binary_target' not in locals() or not callable(calculate_binary_target): missing_check.append('calculate_binary_target function')\n",
        "    print(f\"Tarvittavia muuttujia/funktioita puuttuu: {missing_check}\")\n",
        "    print(\"Aja solut 1, 2 ja 3 uudelleen.\")"
      ],
      "metadata": {
        "id": "8LSsnQONRGfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aA8KORDWML8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Viimeisimmän Ennustejakson Huippuarvo (GRU Luokittelu v1) (2025-04-10 21:55)\n",
        "\n",
        "print(\"\\n--- Osa 9: Viimeisimmän Ennustejakson Huippuarvo ---\")\n",
        "print(\"Tämä osio ei ole relevantti luokittelumallille, joka ennustaa Kyllä/Ei.\")\n",
        "print(\"Siirrytään visualisointiin (Osa 10).\")\n",
        "print(\"\\n--- Osa 9: Ohitettu ---\")"
      ],
      "metadata": {
        "id": "THVpCghVP-zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. Visualisointi (Luokittelutulokset) (GRU v4 - Laaj. FE) (2025-04-10 22:16) # Päivitetty aika\n",
        "\n",
        "import matplotlib.pyplot as plt # Varmista tuonti\n",
        "import numpy as np # Varmista tuonti\n",
        "import pandas as pd # Varmista tuonti\n",
        "import traceback\n",
        "import seaborn as sns # Tuodaan heatmapia varten\n",
        "# Varmistetaan sklearn-tuonnit\n",
        "try:\n",
        "    from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc\n",
        "except ImportError:\n",
        "    print(\"VIRHE: Sklearn-metriikoiden tuonti epäonnistui!\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n--- Osa 10: Visualisointi (Luokittelutulokset) ---\")\n",
        "visualization_possible = False\n",
        "\n",
        "# Tarkistetaan edellytykset (Osa 8:n onnistuminen ja tulosmuuttujat)\n",
        "# Käytetään _gru päätteisiä muuttujia\n",
        "if 'evaluation_gru_clf_successful' in locals() and evaluation_gru_clf_successful and \\\n",
        "   'true_labels_gru' in locals() and isinstance(true_labels_gru, np.ndarray) and \\\n",
        "   'predictions_gru' in locals() and isinstance(predictions_gru, np.ndarray) and \\\n",
        "   'probabilities_gru' in locals() and isinstance(probabilities_gru, np.ndarray):\n",
        "\n",
        "    print(\"\\nArviointi onnistui, piirretään luokittelukuvaajat...\")\n",
        "    # Varmistetaan, ettei data ole tyhjää\n",
        "    if len(true_labels_gru) > 0 and len(true_labels_gru) == len(predictions_gru) == len(probabilities_gru):\n",
        "        visualization_possible = True\n",
        "        try: # --- Kuvaaja 1: Sekaannusmatriisi Heatmap ---\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            # Lasketaan CM uudelleen tässä, jotta solu on itsenäisempi\n",
        "            cm = confusion_matrix(true_labels_gru, predictions_gru, labels=[0, 1])\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=['Ennuste 0 (EI)', 'Ennuste 1 (KYLLÄ)'],\n",
        "                        yticklabels=['Todellinen 0 (EI)', 'Todellinen 1 (KYLLÄ)'])\n",
        "            plt.ylabel('Todellinen luokka')\n",
        "            plt.xlabel('Ennustettu luokka')\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "            plt.title(f'Sekaannusmatriisi (Kynnys=0.5)\\nTN={TN}, FP={FP}, FN={FN}, TP={TP}')\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e_cm:\n",
        "            print(f\"VIRHE Sekaannusmatriisin piirrossa: {e_cm}\"); traceback.print_exc(); visualization_possible = False # Merkitse epäonnistuneeksi\n",
        "\n",
        "        try: # --- Kuvaaja 2: Precision-Recall -käyrä ---\n",
        "            # Käytetään todennäköisyyksiä käyrän piirtoon\n",
        "            precision_curve, recall_curve, thresholds_pr = precision_recall_curve(true_labels_gru, probabilities_gru)\n",
        "            pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            # Poistetaan viimeinen piste käyristä (vastaa kynnystä > 1)\n",
        "            plt.plot(recall_curve[:-1], precision_curve[:-1], marker='.', label=f'GRU (PR AUC = {pr_auc:.3f})')\n",
        "\n",
        "            # Etsitään piste, joka on lähimpänä kynnystä 0.5\n",
        "            # Huom: thresholds_pr ei sisällä arvoa 1.0, joka vastaa viimeistä pistettä\n",
        "            close_threshold_idx = np.argmin(np.abs(thresholds_pr - 0.5))\n",
        "\n",
        "            # Piirretään piste kynnystä 0.5 vastaavalle kohdalle käyrällä\n",
        "            plt.plot(recall_curve[close_threshold_idx], precision_curve[close_threshold_idx], 'o', markersize=8,\n",
        "                     label=f'Kynnys ~0.5 (P={precision_curve[close_threshold_idx]:.2f}, R={recall_curve[close_threshold_idx]:.2f})', color=\"red\")\n",
        "\n",
        "            plt.xlabel('Recall (Herkkyys)')\n",
        "            plt.ylabel('Precision (Tarkkuus)')\n",
        "            plt.title('Precision-Recall -käyrä')\n",
        "            plt.legend()\n",
        "            plt.grid(True, linestyle=':')\n",
        "            plt.xlim([-0.05, 1.05]); plt.ylim([-0.05, 1.05]) # Akselien rajat\n",
        "            plt.show()\n",
        "            print(\"PR-käyrä näyttää tarkkuuden ja herkkyyden välisen kompromissin eri päätöskynnyksillä.\")\n",
        "            print(\"Parempi malli on lähempänä oikeaa yläkulmaa (Precision=1, Recall=1).\")\n",
        "            print(\"Punainen piste näyttää nykyisen 0.5 kynnyksen sijainnin.\")\n",
        "            print(\"Harkitse, olisiko jokin muu piste käyrällä (eri kynnysarvo) parempi kompromissi.\")\n",
        "\n",
        "        except Exception as e_prc:\n",
        "            print(f\"VIRHE Precision-Recall -käyrän piirrossa: {e_prc}\"); traceback.print_exc(); visualization_possible = False\n",
        "\n",
        "    else:\n",
        "        print(\"Visualisointia ei voida suorittaa, koska arvioinnin tulosdata on tyhjää tai pituudet eivät täsmää.\")\n",
        "else:\n",
        "     print(\"\\nVisualisointia ei suoriteta, koska arviointi epäonnistui / tarvittavia muuttujia puuttuu.\")\n",
        "\n",
        "# --- VAHVISTUSTULOSTE ---\n",
        "if visualization_possible:\n",
        "    print(\"\\n--- Osa 10: Valmis ---\")\n",
        "else:\n",
        "    print(\"\\n--- Osa 10: EPÄONNISTUI / OHITETTIIN ---\")\n",
        "\n",
        "# --- LOPPUTULOSTE ---\n",
        "print(f\"\\n--- Koko skriptin ajo päättyi ---\")\n",
        "script_end_time = time.time()\n",
        "if 'script_start_time' in locals() and script_start_time is not None:\n",
        "     try: print(f\"Kokonaisajoaika: {script_end_time - script_start_time:.2f} sekuntia.\")\n",
        "     except: print(\"Ajoaikaa ei voitu laskea.\")\n",
        "else: print(\"Kokonaisajoaikaa ei voitu laskea.\")"
      ],
      "metadata": {
        "id": "NWRXlOgpWUfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}