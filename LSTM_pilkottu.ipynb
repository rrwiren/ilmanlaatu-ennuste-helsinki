{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/yoapWU6M1dbUFdxDMVFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrwiren/ilmanlaatu-ennuste-helsinki/blob/main/LSTM_pilkottu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0X204NoKu6p"
      },
      "outputs": [],
      "source": [
        "# @title 0. Esitiedot ja Tavoite (LSTM v1 - Uusi Data)\n",
        "\"\"\"\n",
        "Otsoniennuste Helsinki - LSTM-malli v1 (Uudella Datalla)\n",
        "\n",
        "Tavoite:\n",
        "1. Ladata uusi esikäsitelty data Parquet-tiedostosta (sis. pilvisyys).\n",
        "2. Esikäsitellä data neuroverkkomallille sopivaksi (kuten aiemmassa GRU v3:ssa).\n",
        "3. Määritellä ja kouluttaa LSTM (Long Short-Term Memory) -malli PyTorchilla.\n",
        "4. Tehdä ennusteita testijaksolle.\n",
        "5. Arvioida mallin suorituskykyä (RMSE, MAE, baseline-vertailu, 8h raja)\n",
        "   ja verrata sitä aiempiin GRU-, ARIMA- ja SARIMAX-tuloksiin.\n",
        "6. Visualisoida ennusteita.\n",
        "\n",
        "Käyttää refaktoroitua asetustiedostoa (Osa 1).\n",
        "\"\"\"\n",
        "print(\"Osa 0: Esitiedot (LSTM v1) - OK\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Tuonnit ja Asetukset (Korjattu & Selkeytetty RNN/LSTM varten)\n",
        "\n",
        "# Peruskirjastot\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import traceback\n",
        "import warnings\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report\n",
        "\n",
        "# Statsmodels (Vain ADF-testi ja kuvaajat tarvitaan nyt)\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "# SARIMAXia ei tuoda tässä skriptissä\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Muut\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Yleisasetukset ---\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 7)\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "print(f\"Käytettävä laite: {device}\")\n",
        "\n",
        "# --- Data-asetukset ---\n",
        "BASE_GITHUB_URL = 'https://raw.githubusercontent.com/rrwiren/ilmanlaatu-ennuste-helsinki/main/'\n",
        "PARQUET_PATH = 'data/processed/processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet'\n",
        "DATA_URL = BASE_GITHUB_URL + PARQUET_PATH\n",
        "LOCAL_DATA_PATH = 'processed_Helsinki_O3_Weather_Cloudiness_2024_2025_v3.parquet'\n",
        "TARGET_COLUMN = 'Otsoni [µg/m³]'\n",
        "ALL_COLUMNS_IN_PARQUET = [\n",
        "    'Otsoni [µg/m³]',\n",
        "    'Lämpötilan keskiarvo [°C]',\n",
        "    'Keskituulen nopeus [m/s]',\n",
        "    'Ilmanpaineen keskiarvo [hPa]',\n",
        "    'Tuulen suunnan keskiarvo [°]',\n",
        "    'Pilvisyys [okta]'\n",
        "]\n",
        "# EXOG_COLUMNS_BASE ei tarvita suoraan RNN-malleille tällä tavalla,\n",
        "# koska kaikki ominaisuudet menevät INPUT_SIZEen.\n",
        "\n",
        "# --- Ennustus- ja Jakoasetukset ---\n",
        "FORECAST_HORIZON = 24 # Tuntia\n",
        "TEST_SPLIT_RATIO = 0.15\n",
        "VALID_SPLIT_RATIO = 0.15\n",
        "\n",
        "# --- RNN/LSTM/GRU Mallin Hyperparametrit ---\n",
        "RNN_HYPERPARAMS = {\n",
        "    'model_type': 'LSTM',  # Vaihda 'GRU' tarvittaessa\n",
        "    'input_size': None,    # Lasketaan Osa 3:ssa datan käsittelyn jälkeen\n",
        "    'hidden_size': 64,\n",
        "    'num_layers': 2,\n",
        "    'output_size': FORECAST_HORIZON, # = 24\n",
        "    'dropout_prob': 0.2\n",
        "}\n",
        "\n",
        "# --- Neuroverkkojen Koulutusparametrit ---\n",
        "TRAIN_HYPERPARAMS = {\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 75,\n",
        "    'patience': 10 # Early stopping\n",
        "}\n",
        "\n",
        "# --- Arviointiasetukset ---\n",
        "O3_THRESHOLD_8H_AVG = 85 # µg/m³ (Käytetään testattua alempaa rajaa)\n",
        "\n",
        "print(\"\\nOsa 1: Tuonnit ja Asetukset (Korjattu RNN/LSTM) - OK\")\n",
        "print(f\"Kohdemuuttuja: {TARGET_COLUMN}\")\n",
        "print(f\"Parquet-polku: {PARQUET_PATH}\")\n",
        "print(f\"Käytettävä RNN-tyyppi: {RNN_HYPERPARAMS['model_type']}\")\n",
        "print(f\"8h Keskiarvon kynnysarvo: {O3_THRESHOLD_8H_AVG} µg/m³\")"
      ],
      "metadata": {
        "id": "kbt0VMTfOt8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Funktiot Datan Lataukseen ja Käsittelyyn (GRU v3 / LSTM v1) - KORJATTU NaT KÄSITTELY\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "# Varmistetaan StandardScaler tuonti\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "except ImportError:\n",
        "     print(\"VIRHE: scikit-learn ei ole asennettu tai sitä ei voitu tuoda.\")\n",
        "     StandardScaler = None\n",
        "\n",
        "\n",
        "# Funktio datan lataamiseen URL:sta (jos paikallista ei löydy)\n",
        "def download_data(url, local_path):\n",
        "    \"\"\"Lataa tiedoston URL:sta paikalliseen polkuun.\"\"\"\n",
        "    try:\n",
        "        print(f\"Yritetään ladata dataa osoitteesta {url}...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        target_dir = os.path.dirname(local_path)\n",
        "        if target_dir and not os.path.exists(target_dir):\n",
        "             os.makedirs(target_dir, exist_ok=True)\n",
        "             print(f\"Luotiin hakemisto: {target_dir}\")\n",
        "        with open(local_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Data ladattu onnistuneesti: {local_path}\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Virhe datan latauksessa: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "         print(f\"Odottamaton virhe tiedoston tallennuksessa polkuun {local_path}: {e}\")\n",
        "         traceback.print_exc()\n",
        "         return False\n",
        "\n",
        "# Funktio Parquet-datan lataamiseen\n",
        "def load_parquet_data(filepath_or_url, local_cache_path=LOCAL_DATA_PATH):\n",
        "    \"\"\"Lataa datan Parquet-tiedostosta (paikallisesti tai URL:sta, käyttää välimuistia). KORJATTU NaT-käsittely.\"\"\"\n",
        "    filepath_to_read = None\n",
        "    if filepath_or_url.startswith('http'):\n",
        "         if os.path.exists(local_cache_path):\n",
        "              print(f\"Käytetään paikallista välimuistitiedostoa: {local_cache_path}\")\n",
        "              filepath_to_read = local_cache_path\n",
        "         else:\n",
        "              print(f\"Paikallista tiedostoa {local_cache_path} ei löytynyt.\")\n",
        "              if download_data(filepath_or_url, local_cache_path):\n",
        "                   filepath_to_read = local_cache_path\n",
        "              else: return None\n",
        "    else:\n",
        "         filepath_to_read = filepath_or_url\n",
        "         if not os.path.exists(filepath_to_read):\n",
        "              print(f\"VIRHE: Annettua paikallista tiedostoa {filepath_to_read} ei löytynyt.\")\n",
        "              return None\n",
        "\n",
        "    if filepath_to_read:\n",
        "        try:\n",
        "            print(f\"Ladataan dataa tiedostosta: {filepath_to_read}\")\n",
        "            df = pd.read_parquet(filepath_to_read)\n",
        "            print(f\"Parquet-tiedosto ladattu, muoto: {df.shape}\")\n",
        "\n",
        "            # --- Aikaindeksin varmistus ja käsittely ---\n",
        "            if 'Aikaleima' not in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
        "                 print(\"Indeksi ei ole DatetimeIndex, eikä 'Aikaleima'-sarake löydy. Yritetään muuntaa indeksi...\")\n",
        "                 try: df.index = pd.to_datetime(df.index); print(\"Indeksi muunnettu datetime-indeksiksi.\")\n",
        "                 except Exception as e_idx: print(f\"VIRHE: Indeksin muunto datetimeksi epäonnistui: {e_idx}\"); return None\n",
        "            elif 'Aikaleima' in df.columns:\n",
        "                 print(\"Käytetään 'Aikaleima'-saraketta indeksinä...\"); df['Aikaleima'] = pd.to_datetime(df['Aikaleima']); df.set_index('Aikaleima', inplace=True); print(\"'Aikaleima' asetettu indeksiksi.\")\n",
        "\n",
        "            if not isinstance(df.index, pd.DatetimeIndex): print(\"VIRHE: Indeksi ei ole DatetimeIndex käsittelyn jälkeen.\"); return None\n",
        "\n",
        "            # Varmista aikavyöhyke\n",
        "            if df.index.tz is None:\n",
        "                 print(\"Asetetaan aikavyöhykkeeksi Europe/Helsinki...\")\n",
        "                 try:\n",
        "                     df = df.tz_localize('Europe/Helsinki', ambiguous='NaT', nonexistent='NaT')\n",
        "                     print(\"Aikavyöhyke asetettu (ambiguous/nonexistent='NaT').\")\n",
        "                 except Exception as e_tz: print(f\"VAROITUS: Aikavyöhykkeen asetus epäonnistui: {e_tz}. Jatketaan naiivilla indeksillä.\")\n",
        "            else: print(f\"Aikavyöhyke on jo asetettu: {df.index.tz}\")\n",
        "\n",
        "            # *** KORJATTU NaT-käsittely ***\n",
        "            nat_index_rows = df.index.isnull()\n",
        "            if nat_index_rows.any():\n",
        "                rows_before_nat = len(df)\n",
        "                print(f\"VAROITUS: Löydettiin {nat_index_rows.sum()} virheellistä aikaleimaa (NaT) indeksistä. Poistetaan rivit...\")\n",
        "                df = df[~nat_index_rows] # Valitse rivit, joiden indeksi EI ole NaT\n",
        "                print(f\"Poistettu {rows_before_nat - len(df)} NaT-indeksiriviä.\")\n",
        "\n",
        "            df.sort_index(inplace=True)\n",
        "            print(f\"Datan perustiedot: Aikaväli=[{df.index.min()} - {df.index.max()}]\")\n",
        "            print(f\"Sarakkeet: {df.columns.tolist()}\")\n",
        "\n",
        "            # Tarkista NaN-arvot sisällössä\n",
        "            if df.isnull().any().any():\n",
        "                print(\"VAROITUS: Datassa NaN-arvoja latauksen/aikakäsittelyn jälkeen. Yritetään täyttää ffill/bfill...\")\n",
        "                df.ffill(inplace=True); df.bfill(inplace=True)\n",
        "                if df.isnull().any().any(): print(\"VIRHE: Ei voitu täyttää kaikkia NaN-arvoja soluista.\"); df.dropna(inplace=True)\n",
        "                print(f\"Datan muoto NaN-käsittelyn jälkeen: {df.shape}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"VIRHE Parquet-tiedoston lukemisessa tai aikakäsittelyssä ('{filepath_to_read}'): {e}\")\n",
        "            traceback.print_exc(); return None\n",
        "    else: print(\"VIRHE: Tiedostopolkua lukemiseen ei saatu määritettyä.\"); return None\n",
        "\n",
        "\n",
        "# Funktio ominaisuuksien muokkaukseen (feature engineering) neuroverkolle\n",
        "def feature_engineer_gru(df):\n",
        "    \"\"\"Lisää aika- ja syklisiä ominaisuuksia neuroverkkokäyttöön.\"\"\"\n",
        "    print(\"\\nSuoritetaan ominaisuuksien muokkaus (Feature Engineering)...\")\n",
        "    if not isinstance(df, pd.DataFrame): print(\"VIRHE: feature_engineer_gru syöte ei DataFrame.\"); return None\n",
        "    df_eng = df.copy()\n",
        "    if not isinstance(df_eng.index, pd.DatetimeIndex): print(\"VIRHE: Indeksi ei ole DatetimeIndex FE:ssä.\"); return df_eng\n",
        "\n",
        "    try: # Kellonaika\n",
        "        df_eng['hour_sin'] = np.sin(2 * np.pi * df_eng.index.hour / 24.0)\n",
        "        df_eng['hour_cos'] = np.cos(2 * np.pi * df_eng.index.hour / 24.0)\n",
        "        print(\"Lisätty syklinen kellonaika (hour_sin, hour_cos).\")\n",
        "    except Exception as e: print(f\"VIRHE kellonaikaominaisuuksien luonnissa: {e}\")\n",
        "\n",
        "    wind_dir_col_orig = 'Tuulen suunnan keskiarvo [°]'\n",
        "    if wind_dir_col_orig in df_eng.columns: # Tuulen suunta\n",
        "        try:\n",
        "            df_eng[wind_dir_col_orig] = pd.to_numeric(df_eng[wind_dir_col_orig], errors='coerce')\n",
        "            if df_eng[wind_dir_col_orig].isnull().any(): print(f\"VAROITUS: NaN '{wind_dir_col_orig}':ssa. Täytetään.\"); df_eng[wind_dir_col_orig]=df_eng[wind_dir_col_orig].ffill().bfill()\n",
        "            df_eng.dropna(subset=[wind_dir_col_orig], inplace=True)\n",
        "            if not df_eng.empty: # Varmista ettei kaikki mennyt NaNnien poistossa\n",
        "                 df_eng['wind_dir_rad'] = np.deg2rad(df_eng[wind_dir_col_orig])\n",
        "                 df_eng['wind_dir_sin'] = np.sin(df_eng['wind_dir_rad'])\n",
        "                 df_eng['wind_dir_cos'] = np.cos(df_eng['wind_dir_rad'])\n",
        "                 df_eng.drop(columns=[wind_dir_col_orig, 'wind_dir_rad'], inplace=True, errors='ignore')\n",
        "                 print(\"Muunnettu tuulen suunta sykliseksi (wind_dir_sin, wind_dir_cos).\")\n",
        "            else: print(f\"Data tyhjeni NaN-poiston jälkeen sarakkeelle '{wind_dir_col_orig}'.\")\n",
        "        except Exception as e: print(f\"VIRHE tuulensuuntaominaisuuksien luonnissa: {e}\"); df_eng.drop(columns=['wind_dir_rad','wind_dir_sin','wind_dir_cos'],inplace=True,errors='ignore')\n",
        "    else: print(f\"Saraketta '{wind_dir_col_orig}' ei löytynyt.\")\n",
        "\n",
        "    print(\"Ominaisuuksien muokkaus valmis.\")\n",
        "    return df_eng\n",
        "\n",
        "\n",
        "# Funktio sekvenssien luontiin PyTorchille\n",
        "def create_sequences_pytorch(features_scaled, targets_original, sequence_length, prediction_horizon):\n",
        "    \"\"\"Luo syötesekvenssejä (X, skaalattu) ja alkuperäisiä kohde-ennusteita (y_orig).\"\"\"\n",
        "    X, y_orig = [], []\n",
        "    print(f\"\\nLuodaan PyTorch-sekvenssejä: sequence_length={sequence_length}, prediction_horizon={prediction_horizon}\")\n",
        "    print(f\"features_scaled shape: {features_scaled.shape}, targets_original shape: {targets_original.shape}\")\n",
        "    required_len = sequence_length + prediction_horizon\n",
        "    if len(features_scaled) < required_len: print(f\"VAROITUS: Ei tarpeeksi dataa ({len(features_scaled)}). Tarvitaan {required_len}.\"); return np.array(X), np.array(y_orig)\n",
        "    if targets_original.ndim == 1: targets_original = targets_original.reshape(-1, 1); print(f\"Muutettu targets_original muotoon: {targets_original.shape}\")\n",
        "    for i in range(len(features_scaled) - required_len + 1):\n",
        "        X.append(features_scaled[i:(i + sequence_length)])\n",
        "        y_orig.append(targets_original[i + sequence_length : i + sequence_length + prediction_horizon, 0])\n",
        "    print(f\"Luotu {len(X)} sekvenssiä.\")\n",
        "    X = np.array(X); y_orig = np.array(y_orig)\n",
        "    if y_orig.ndim == 2: y_orig = y_orig[..., np.newaxis]; print(f\"Muutettu y_orig muotoon: {y_orig.shape}\")\n",
        "    elif y_orig.size > 0: print(f\"VAROITUS: y_orig muoto ({y_orig.shape}) ei ole odotettu (samples, horizon, 1).\")\n",
        "    return X, y_orig\n",
        "\n",
        "print(\"\\nOsa 2: Funktiot datan käsittelyyn (GRU v3 / LSTM v1) - KORJATTU - OK\")"
      ],
      "metadata": {
        "id": "pcPltimHQjP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Pääskriptin Suoritus: Datan Käsittely (GRU v3 / LSTM v1) - LISÄTTY PARAMETRIVARMISTUS\n",
        "\n",
        "# Tässä solussa kutsutaan Osassa 2 määriteltyjä funktioita\n",
        "# ja valmistellaan data DataLoadereihin asti.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "# Varmistetaan tuonnit uudelleen solun ajon varalta\n",
        "try:\n",
        "    import torch\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "except ImportError: raise ImportError(\"PyTorch ei ladattu.\")\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "except ImportError: raise ImportError(\"scikit-learn ei ladattu.\")\n",
        "\n",
        "\n",
        "print(\"--- Aloitetaan Datan Käsittely Neuroverkkoa varten ---\")\n",
        "\n",
        "# Alustetaan muuttujat\n",
        "df_raw_full = None; df_raw = None; df_engineered = None\n",
        "train_loader = None; valid_loader = None; test_loader = None\n",
        "feature_scaler = None; o3_scaler = None\n",
        "INPUT_SIZE = None\n",
        "test_timestamps = None\n",
        "X_train, y_train_original, X_valid, y_valid_original, X_test, y_test_original = [None]*6\n",
        "y_train_scaled, y_valid_scaled = None, None\n",
        "\n",
        "\n",
        "try:\n",
        "    # *** LISÄTTY VARMISTUS PARAMETREILLE TÄSSÄ SOLUSSA ***\n",
        "    # Varmistetaan, että nämä ovat määriteltyjä ennen käyttöä\n",
        "    if 'SEQUENCE_LENGTH' not in locals() or SEQUENCE_LENGTH is None:\n",
        "        print(\"VAROITUS: SEQUENCE_LENGTH ei löytynyt/ollut määritelty. Asetetaan oletusarvo 72.\")\n",
        "        SEQUENCE_LENGTH = 72\n",
        "    if 'PREDICTION_HORIZON' not in locals() or PREDICTION_HORIZON is None:\n",
        "        print(\"VAROITUS: PREDICTION_HORIZON ei löytynyt/ollut määritelty. Asetetaan oletusarvo 24.\")\n",
        "        PREDICTION_HORIZON = 24\n",
        "    if 'TARGET_COLUMN' not in locals() or TARGET_COLUMN is None:\n",
        "         # Haetaan Osasta 1, jos mahdollista, muuten asetetaan oletus\n",
        "         try: TARGET_COLUMN = TARGET_COLUMN # Yritä käyttää globaalia\n",
        "         except NameError: TARGET_COLUMN = 'Otsoni [µg/m³]'; print(\"Asetettiin TARGET_COLUMN oletusarvoon.\")\n",
        "    if 'ALL_COLUMNS_IN_PARQUET' not in locals() or ALL_COLUMNS_IN_PARQUET is None:\n",
        "         # Asetetaan oletus Osan 1 perusteella\n",
        "         ALL_COLUMNS_IN_PARQUET = ['Otsoni [µg/m³]', 'Lämpötilan keskiarvo [°C]', 'Keskituulen nopeus [m/s]', 'Ilmanpaineen keskiarvo [hPa]', 'Tuulen suunnan keskiarvo [°]', 'Pilvisyys [okta]']\n",
        "         print(\"Asetettiin ALL_COLUMNS_IN_PARQUET oletusarvoon.\")\n",
        "    if 'TRAIN_HYPERPARAMS' not in locals() or TRAIN_HYPERPARAMS is None:\n",
        "         TRAIN_HYPERPARAMS = {'batch_size': 64} # Tarvitaan vain batch_size DataLoadereihin tässä solussa\n",
        "         print(\"Asetettiin TRAIN_HYPERPARAMS['batch_size'] oletusarvoon.\")\n",
        "    elif 'batch_size' not in TRAIN_HYPERPARAMS:\n",
        "          TRAIN_HYPERPARAMS['batch_size'] = 64\n",
        "          print(\"Asetettiin TRAIN_HYPERPARAMS['batch_size'] oletusarvoon.\")\n",
        "\n",
        "\n",
        "    print(f\"\\nKäytetään Osassa 3: SEQUENCE_LENGTH={SEQUENCE_LENGTH}, PREDICTION_HORIZON={PREDICTION_HORIZON}, BATCH_SIZE={TRAIN_HYPERPARAMS['batch_size']}\")\n",
        "    # ******************************************************\n",
        "\n",
        "    # 1. Lataa data käyttäen Osa 2:n funktiota\n",
        "    df_raw_full = load_parquet_data(DATA_URL) # Käyttää DATA_URL ja LOCAL_DATA_PATH Osa 1:stä\n",
        "    if df_raw_full is None: raise ValueError(\"Datan lataus epäonnistui (load_parquet_data palautti None).\")\n",
        "    if df_raw_full.empty: raise ValueError(\"Ladattu DataFrame on tyhjä.\")\n",
        "\n",
        "    # 2. Tarkista ja valitse alkuperäiset sarakkeet\n",
        "    print(\"\\nTarkistetaan ja valitaan alkuperäiset sarakkeet...\")\n",
        "    missing_cols_raw = [col for col in ALL_COLUMNS_IN_PARQUET if col not in df_raw_full.columns]\n",
        "    if missing_cols_raw:\n",
        "        raise ValueError(f\"Seuraavat määritellyt sarakkeet puuttuvat Parquet-tiedostosta: {missing_cols_raw}\")\n",
        "    df_raw = df_raw_full[ALL_COLUMNS_IN_PARQUET].copy()\n",
        "    print(\"Alkuperäiset sarakkeet valittu.\")\n",
        "\n",
        "    # 3. Suorita Feature Engineering\n",
        "    df_engineered = feature_engineer_gru(df_raw)\n",
        "    if df_engineered is None: raise ValueError(\"Ominaisuuksien muokkaus (feature_engineer_gru) epäonnistui.\")\n",
        "    if df_engineered.empty: raise ValueError(\"DataFrame tyhjä feature engineeringin jälkeen.\")\n",
        "\n",
        "    # 4. Määritä lopulliset ominaisuudet ja INPUT_SIZE\n",
        "    FINAL_FEATURE_COLUMNS = df_engineered.columns.tolist()\n",
        "    if TARGET_COLUMN not in FINAL_FEATURE_COLUMNS: print(f\"VAROITUS: Kohde '{TARGET_COLUMN}' ei lopullisissa ominaisuuksissa!\")\n",
        "    INPUT_SIZE = len(FINAL_FEATURE_COLUMNS)\n",
        "    if INPUT_SIZE == 0: raise ValueError(\"Lopullisia ominaisuuksia ei löytynyt feature engineeringin jälkeen.\")\n",
        "    print(f\"\\nLopullinen ominaisuuksien määrä (INPUT_SIZE): {INPUT_SIZE}\")\n",
        "    print(f\"Lopulliset ominaisuudet mallille: {FINAL_FEATURE_COLUMNS}\")\n",
        "\n",
        "    # 5. Jaa data harjoitus-, validointi- ja testijoukkoihin\n",
        "    n = len(df_engineered)\n",
        "    # Käytetään nyt varmistettuja arvoja\n",
        "    min_data_len_needed = (SEQUENCE_LENGTH + PREDICTION_HORIZON) * 3\n",
        "    if n < min_data_len_needed: print(f\"VAROITUS: Datan pituus ({n}) voi olla liian lyhyt jakoon ja sekvenssien luontiin (tarvitaan > {min_data_len_needed}).\")\n",
        "    test_split_idx = int(n * (1 - TEST_SPLIT_RATIO))\n",
        "    valid_split_idx = int(test_split_idx * (1 - VALID_SPLIT_RATIO / (1 - TEST_SPLIT_RATIO)))\n",
        "    df_train = df_engineered[:valid_split_idx]\n",
        "    df_valid = df_engineered[valid_split_idx:test_split_idx]\n",
        "    df_test = df_engineered[test_split_idx:]\n",
        "    min_len_for_seq = SEQUENCE_LENGTH + PREDICTION_HORIZON\n",
        "    if len(df_train) < min_len_for_seq or len(df_valid) < min_len_for_seq or len(df_test) < min_len_for_seq:\n",
        "        print(f\"Train={len(df_train)}, Valid={len(df_valid)}, Test={len(df_test)}, Min={min_len_for_seq}\")\n",
        "        raise ValueError(\"Liian vähän dataa yhdessä tai useammassa jaossa sekvenssien luontia varten.\")\n",
        "    print(f\"\\nDatan jako: Train={len(df_train)}, Valid={len(df_valid)}, Test={len(df_test)}\")\n",
        "\n",
        "    # 6. Skaalaa data\n",
        "    print(\"\\nSkaalataan dataa...\")\n",
        "    feature_scaler = StandardScaler()\n",
        "    scaled_train_features = feature_scaler.fit_transform(df_train)\n",
        "    scaled_valid_features = feature_scaler.transform(df_valid)\n",
        "    scaled_test_features = feature_scaler.transform(df_test)\n",
        "    print(\"Ominaisuudet skaalattu.\")\n",
        "    o3_scaler = StandardScaler()\n",
        "    o3_scaler.fit(df_raw.loc[df_train.index, [TARGET_COLUMN]])\n",
        "    print(\"Kohdemuuttuja (O3) skaalain sovitettu.\")\n",
        "\n",
        "    # 7. Hae alkuperäiset O3-kohdearvot\n",
        "    o3_train_targets_original = df_raw.loc[df_train.index, [TARGET_COLUMN]].values\n",
        "    o3_valid_targets_original = df_raw.loc[df_valid.index, [TARGET_COLUMN]].values\n",
        "    o3_test_targets_original = df_raw.loc[df_test.index, [TARGET_COLUMN]].values\n",
        "\n",
        "    # 8. Luo sekvenssit\n",
        "    X_train, y_train_original = create_sequences_pytorch(scaled_train_features, o3_train_targets_original, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "    X_valid, y_valid_original = create_sequences_pytorch(scaled_valid_features, o3_valid_targets_original, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "    X_test, y_test_original = create_sequences_pytorch(scaled_test_features, o3_test_targets_original, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "    if X_train.size == 0 or X_valid.size == 0 or X_test.size == 0: raise ValueError(\"Sekvenssien luonti epäonnistui (yksi tai useampi X on tyhjä).\")\n",
        "\n",
        "    # 9. Skaalaa kohdesekvenssit\n",
        "    y_train_scaled = o3_scaler.transform(y_train_original.reshape(-1, 1)).reshape(y_train_original.shape)\n",
        "    y_valid_scaled = o3_scaler.transform(y_valid_original.reshape(-1, 1)).reshape(y_valid_original.shape)\n",
        "    print(\"Kohdesekvenssit skaalattu.\")\n",
        "\n",
        "    # 10. Muunna Tensoreiksi\n",
        "    print(\"\\nMuunnetaan data PyTorch-tensoreiksi...\")\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32); y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32); y_valid_tensor = torch.tensor(y_valid_scaled, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32); y_test_tensor_original = torch.tensor(y_test_original, dtype=torch.float32)\n",
        "    print(\"Tensorit luotu.\")\n",
        "\n",
        "    # 11. Luo DataLoaderit\n",
        "    # drop_last=True harjoitusdatalle voi auttaa jos viimeinen erä pieni\n",
        "    batch_size = TRAIN_HYPERPARAMS.get('batch_size', 64) # Hae batch_size asetuksista\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor); train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor); valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor_original); test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    print(\"DataLoaderit luotu.\")\n",
        "\n",
        "    # 12. Tallenna testiaikaleimat\n",
        "    print(\"\\nTallennetaan testiaikaleimoja...\")\n",
        "    if 'df_test' in locals() and not df_test.empty and len(df_test) >= SEQUENCE_LENGTH and 'X_test' in locals() and len(X_test) > 0:\n",
        "        try:\n",
        "            test_start_index_loc = df_engineered.index.get_loc(df_test.index[0])\n",
        "            test_start_indices = test_start_index_loc + SEQUENCE_LENGTH\n",
        "            end_index = test_start_indices + len(X_test)\n",
        "            if end_index <= len(df_engineered.index):\n",
        "                test_timestamps = df_engineered.index[test_start_indices : end_index]\n",
        "                print(f\"Testiaikaleimat tallennettu ({len(test_timestamps)} kpl).\")\n",
        "            else: print(\"VAROITUS: Ei voitu määrittää kaikkia testiaikaleimoja.\"); test_timestamps = None\n",
        "        except Exception as e_ts: print(f\"VIRHE testiaikaleimojen tallennuksessa: {e_ts}\"); test_timestamps = None\n",
        "    else: print(\"VAROITUS: Testidata liian lyhyt/tyhjä aikaleimoille.\"); test_timestamps = None\n",
        "\n",
        "    print(f\"\\nLopulliset muodot DataLoadereihin menevälle datalle:\")\n",
        "    print(f\"X_train_tensor: {X_train_tensor.shape}, y_train_tensor: {y_train_tensor.shape}\")\n",
        "    print(f\"X_valid_tensor: {X_valid_tensor.shape}, y_valid_tensor: {y_valid_tensor.shape}\")\n",
        "    print(f\"X_test_tensor:  {X_test_tensor.shape}, y_test_tensor_original (tensor): {y_test_tensor_original.shape}\")\n",
        "\n",
        "    print(\"\\nOsa 3: Datan käsittely - VALMIS\")\n",
        "\n",
        "# Käsittele mahdolliset virheet päälohkossa\n",
        "except ValueError as ve:\n",
        "     print(f\"\\n---> VIRHE DATAN KÄSITTELYSSÄ (ValueError): {ve} <---\")\n",
        "     traceback.print_exc()\n",
        "     train_loader, valid_loader, test_loader, INPUT_SIZE = [None]*4 # Nollaa kriittiset muuttujat\n",
        "except KeyError as ke:\n",
        "     print(f\"\\n---> VIRHE DATAN KÄSITTELYSSÄ (KeyError): Saraketta {ke} ei löytynyt <---\")\n",
        "     traceback.print_exc()\n",
        "     train_loader = None; valid_loader = None; test_loader = None; INPUT_SIZE=None;\n",
        "except Exception as e:\n",
        "     print(f\"\\n---> ODOTTAMATON VIRHE DATAN KÄSITTELYSSÄ (Osa 3) <---\")\n",
        "     traceback.print_exc()\n",
        "     train_loader = None; valid_loader = None; test_loader = None; INPUT_SIZE=None;"
      ],
      "metadata": {
        "id": "ga4UcdBSaCda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. LSTM-Mallin Määrittely (LSTM v1)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import traceback # Virheiden jäljitykseen\n",
        "\n",
        "print(\"--- Määritellään LSTM-malli ---\")\n",
        "\n",
        "# Varmistetaan tarvittavat parametrit globaalista scopesta (asetettu Osassa 1 & 3)\n",
        "model_params_ok = True\n",
        "# Käytetään try-exceptia, koska muuttujat voivat puuttua, jos soluja ajetaan epäjärjestyksessä\n",
        "try:\n",
        "    # Haetaan arvot aiemmin määritellyistä muuttujista/sanakirjoista\n",
        "    input_size = INPUT_SIZE\n",
        "    hidden_size = RNN_HYPERPARAMS['hidden_size']\n",
        "    num_layers = RNN_HYPERPARAMS['num_layers']\n",
        "    output_size = RNN_HYPERPARAMS['output_size']\n",
        "    dropout_prob = RNN_HYPERPARAMS['dropout_prob']\n",
        "    # Tulostetaan käytettävät arvot\n",
        "    print(f\"Käytetään LSTM-parametreja: Input={input_size}, Hidden={hidden_size}, Layers={num_layers}, Output={output_size}, Dropout={dropout_prob}\")\n",
        "    # Tarkistetaan arvojen järkevyys\n",
        "    assert all(isinstance(v, int) and v > 0 for v in [input_size, hidden_size, num_layers, output_size]), \"Kokoarvojen oltava positiivisia kokonaislukuja.\"\n",
        "    assert isinstance(dropout_prob, float) and 0.0 <= dropout_prob < 1.0, \"Dropout oltava [0, 1).\"\n",
        "\n",
        "except (NameError, KeyError, TypeError, AssertionError) as param_err:\n",
        "     print(f\"VIRHE hyperparametreissa: {param_err}. Varmista, että Osat 1 ja 3 on ajettu onnistuneesti.\")\n",
        "     model_params_ok = False # Estetään luokan määrittely\n",
        "\n",
        "\n",
        "# Määritellään malliluokka vain jos parametrit ok\n",
        "if model_params_ok:\n",
        "    class LSTMModel(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
        "            super(LSTMModel, self).__init__()\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_layers = num_layers\n",
        "            lstm_dropout = dropout_prob if num_layers > 1 else 0.0\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                                batch_first=True, dropout=lstm_dropout)\n",
        "            self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x: (batch, seq_len, input_size)\n",
        "            # Alustetaan h0 ja c0 oikealle laitteelle\n",
        "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            # LSTM ulostulo\n",
        "            out, _ = self.lstm(x, (h0, c0))\n",
        "            # Otetaan viimeinen aika-askel\n",
        "            out = out[:, -1, :] # (batch, hidden)\n",
        "            # Lineaarinen kerros\n",
        "            out = self.fc(out) # (batch, output_size)\n",
        "            return out\n",
        "\n",
        "    # Testataan luokan alustus nopeasti\n",
        "    try:\n",
        "         temp_model_test = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
        "         print(\"\\nLSTMModel-luokka määritelty ja alustus testattu onnistuneesti.\")\n",
        "         print(\"Mallin rakenne:\")\n",
        "         print(temp_model_test)\n",
        "         del temp_model_test\n",
        "    except Exception as e_model_def:\n",
        "         print(f\"VIRHE LSTMModel-luokan alustuksessa: {e_model_def}\")\n",
        "         traceback.print_exc()\n",
        "         raise RuntimeError(\"Mallin määrittely epäonnistui.\")\n",
        "\n",
        "    print(\"\\nOsa 4: LSTM-Mallin Määrittely (LSTM v1) - OK\")\n",
        "\n",
        "else:\n",
        "     print(\"\\nOsa 4: LSTM-Mallin Määrittely - EPÄONNISTUI puuttuvien/virheellisten parametrien vuoksi.\")"
      ],
      "metadata": {
        "id": "2eQ6NZeJaUa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Koulutusfunktion Määrittely (LSTM v1)\n",
        "\n",
        "# Tämä funktio on sama kuin aiemmassa GRU-skriptissä\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm # Edistymispalkki\n",
        "import copy # Mallin tilan kopiointiin\n",
        "import traceback\n",
        "\n",
        "print(\"--- Määritellään train_model -funktio ---\")\n",
        "\n",
        "# Varmistetaan tarvittavien luokkien olemassaolo\n",
        "if 'DataLoader' not in locals(): raise NameError(\"DataLoader ei ole määritelty.\")\n",
        "if 'nn' not in locals() or 'optim' not in locals(): raise NameError(\"torch.nn tai torch.optim ei ole tuotu.\")\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, device, patience):\n",
        "    \"\"\"Kouluttaa mallin ja käyttää Early Stoppingia (sis. .squeeze(-1) korjauksen).\"\"\"\n",
        "\n",
        "    # Alkutarkistukset\n",
        "    if not isinstance(model, nn.Module): raise TypeError(\"Vaaditaan PyTorch-malli.\")\n",
        "    if not isinstance(train_loader, DataLoader): raise TypeError(\"Vaaditaan DataLoader train_loaderille.\")\n",
        "    if not isinstance(valid_loader, DataLoader): raise TypeError(\"Vaaditaan DataLoader valid_loaderille.\")\n",
        "    if not isinstance(criterion, nn.modules.loss._Loss): raise TypeError(\"Vaaditaan PyTorch häviöfunktio.\")\n",
        "    if not isinstance(optimizer, optim.Optimizer): raise TypeError(\"Vaaditaan PyTorch optimoija.\")\n",
        "    if not isinstance(epochs, int) or epochs <= 0: raise ValueError(\"Epochs oltava positiivinen kokonaisluku.\")\n",
        "    if not isinstance(patience, int) or patience <= 0: raise ValueError(\"Patience oltava positiivinen kokonaisluku.\")\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    best_valid_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    print(f\"\\nAloitetaan koulutus {epochs} epochilla...\")\n",
        "    print(f\"Early stopping -kärsivällisyys: {patience} epochia.\")\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        model.train() # Koulutustila\n",
        "        running_train_loss = 0.0\n",
        "        batch_count = 0\n",
        "        try: # Try-except epochin sisällä\n",
        "            for inputs, targets_scaled in train_loader:\n",
        "                batch_count += 1\n",
        "                inputs, targets_scaled = inputs.to(device), targets_scaled.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs_scaled = model(inputs) # Ennusteet, muoto (batch, 24)\n",
        "\n",
        "                # --- Käsittele kohdemuoto (squeeze) ---\n",
        "                if targets_scaled.ndim == outputs_scaled.ndim + 1 and targets_scaled.shape[-1] == 1:\n",
        "                    targets_squeezed = targets_scaled.squeeze(-1) # Muoto (batch, 24)\n",
        "                elif targets_scaled.shape == outputs_scaled.shape:\n",
        "                    targets_squeezed = targets_scaled # Muodot täsmäävät jo\n",
        "                else:\n",
        "                     print(f\"\\nVIRHE Epoch {epoch+1}, Batch {batch_count} (Train): Muodot eivät täsmää loss-laskentaa varten!\")\n",
        "                     print(f\"Output shape: {outputs_scaled.shape}, Target shape: {targets_scaled.shape}\")\n",
        "                     raise RuntimeError(\"Muodot eivät täsmää loss-laskentaa varten\")\n",
        "\n",
        "                loss = criterion(outputs_scaled, targets_squeezed)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_train_loss = running_train_loss / len(train_loader.dataset) if len(train_loader.dataset) > 0 else 0\n",
        "            train_losses.append(epoch_train_loss)\n",
        "\n",
        "        except Exception as e_train_epoch:\n",
        "             print(f\"\\nVIRHE koulutusloopissa (Epoch {epoch+1}, Batch {batch_count}): {e_train_epoch}\")\n",
        "             traceback.print_exc()\n",
        "             print(\"Keskeytetään koulutus.\")\n",
        "             model = None; return model, train_losses, valid_losses\n",
        "\n",
        "        # --- Validointivaihe ---\n",
        "        model.eval()\n",
        "        running_valid_loss = 0.0\n",
        "        valid_batch_count = 0\n",
        "        try: # Try-except validointiin\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets_scaled in valid_loader:\n",
        "                    valid_batch_count += 1\n",
        "                    inputs, targets_scaled = inputs.to(device), targets_scaled.to(device)\n",
        "                    outputs_scaled = model(inputs)\n",
        "\n",
        "                    # --- Käsittele kohdemuoto (squeeze) ---\n",
        "                    if targets_scaled.ndim == outputs_scaled.ndim + 1 and targets_scaled.shape[-1] == 1:\n",
        "                        targets_squeezed = targets_scaled.squeeze(-1)\n",
        "                    elif targets_scaled.shape == outputs_scaled.shape:\n",
        "                        targets_squeezed = targets_scaled\n",
        "                    else:\n",
        "                         print(f\"\\nVIRHE Epoch {epoch+1} (Valid), Batch {valid_batch_count}: Muodot eivät täsmää loss-laskentaa varten!\")\n",
        "                         print(f\"Output shape: {outputs_scaled.shape}, Target shape: {targets_scaled.shape}\")\n",
        "                         raise RuntimeError(\"Muodot eivät täsmää validointi loss-laskentaa varten\")\n",
        "\n",
        "                    loss = criterion(outputs_scaled, targets_squeezed)\n",
        "                    running_valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_valid_loss = running_valid_loss / len(valid_loader.dataset) if len(valid_loader.dataset) > 0 else 0\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1:02d}/{epochs} - Train Loss: {epoch_train_loss:.6f} - Valid Loss: {epoch_valid_loss:.6f}\", end=\"\")\n",
        "\n",
        "            # Early Stopping Check\n",
        "            if epoch_valid_loss < best_valid_loss:\n",
        "                best_valid_loss = epoch_valid_loss\n",
        "                epochs_no_improve = 0\n",
        "                try: best_model_state = copy.deepcopy(model.state_dict()); print(\" (Uusi paras!)\")\n",
        "                except Exception as e_state: print(f\" (VIRHE tilan tallennuksessa: {e_state})\"); best_model_state = None\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                print(f\" (Ei parannusta {epochs_no_improve}/{patience})\")\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"\\nEarly stopping {patience} epochin jälkeen ilman parannusta.\")\n",
        "                break\n",
        "\n",
        "        except Exception as e_valid_epoch:\n",
        "             print(f\"\\nVIRHE validointiloopissa (Epoch {epoch+1}, Batch {valid_batch_count}): {e_valid_epoch}\")\n",
        "             traceback.print_exc()\n",
        "             print(\"Keskeytetään koulutus.\")\n",
        "             model = None; return model, train_losses, valid_losses\n",
        "\n",
        "    # --- Koulutusloopin jälkeen ---\n",
        "    if best_model_state:\n",
        "         print(\"\\nLadataan paras malli Early Stoppingin perusteella.\")\n",
        "         try: model.load_state_dict(best_model_state)\n",
        "         except Exception as e_load: print(f\"VIRHE parhaan mallin latauksessa: {e_load}. Jatketaan viimeisimmällä.\")\n",
        "    elif epochs > 0 and train_losses is not None : print(\"\\nKoulutus päättyi ilman Early Stoppingia. Käytetään viimeisintä mallia.\")\n",
        "    else: print(\"\\nKoulutusta ei ajettu / keskeytyi.\")\n",
        "\n",
        "    return model, train_losses, valid_losses\n",
        "\n",
        "print(\"\\nOsa 5: Koulutusfunktion Määrittely (LSTM v1) - OK\")"
      ],
      "metadata": {
        "id": "t6JPT5Gnams1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Mallin Koulutus (Suoritus) (LSTM v1)\n",
        "\n",
        "# Varmistetaan tuonnit solun itsenäistä ajoa varten\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "# Varmistetaan myös malliluokka ja koulutusfunktio\n",
        "if 'LSTMModel' not in locals(): print(\"VAROITUS: LSTMModel-luokkaa ei määritelty (Aja Osa 4)\"); LSTMModel = None\n",
        "if 'train_model' not in locals(): print(\"VAROITUS: train_model-funktiota ei määritelty (Aja Osa 5)\"); train_model = None\n",
        "\n",
        "print(\"--- Aloitetaan Mallin Koulutus (Suoritus) ---\")\n",
        "\n",
        "# Alustetaan model Noneksi siltä varalta että koulutus ei käynnisty tai epäonnistuu\n",
        "model = None # Käytetään geneeristä nimeä koulutuksen ajaksi\n",
        "model_lstm = None # Tähän tallennetaan onnistuneesti koulutettu malli\n",
        "train_losses = None\n",
        "valid_losses = None\n",
        "\n",
        "# Varmistetaan, että kaikki tarvittavat muuttujat edellisistä osista ovat olemassa\n",
        "required_vars_exist = True\n",
        "vars_to_check = ['train_loader', 'valid_loader', 'INPUT_SIZE', 'device',\n",
        "                 'RNN_HYPERPARAMS', 'TRAIN_HYPERPARAMS',\n",
        "                 'LSTMModel', 'train_model']\n",
        "missing_vars = []\n",
        "for var in vars_to_check:\n",
        "    if var not in locals() or (locals()[var] is None and var not in ['train_losses', 'valid_losses', 'model', 'model_lstm']): # Sallitaan näiden olla None aluksi\n",
        "        missing_vars.append(var)\n",
        "        required_vars_exist = False\n",
        "\n",
        "if required_vars_exist:\n",
        "    # --- Mallin, häviöfunktion ja optimoijan alustus ---\n",
        "    # Haetaan parametrit sanakirjoista\n",
        "    try:\n",
        "        model_type = RNN_HYPERPARAMS.get('model_type', 'LSTM') # Varmistetaan että LSTM ajetaan\n",
        "        input_size = INPUT_SIZE\n",
        "        hidden_size = RNN_HYPERPARAMS['hidden_size']\n",
        "        num_layers = RNN_HYPERPARAMS['num_layers']\n",
        "        output_size = RNN_HYPERPARAMS['output_size']\n",
        "        dropout_prob = RNN_HYPERPARAMS['dropout_prob']\n",
        "\n",
        "        batch_size = TRAIN_HYPERPARAMS['batch_size']\n",
        "        learning_rate = TRAIN_HYPERPARAMS['learning_rate']\n",
        "        epochs = TRAIN_HYPERPARAMS['epochs']\n",
        "        patience = TRAIN_HYPERPARAMS['patience']\n",
        "\n",
        "        print(f\"Varmistetaan ajettava mallityyppi: {model_type}\")\n",
        "        if model_type != 'LSTM':\n",
        "             print(f\"VAROITUS: RNN_HYPERPARAMS['model_type'] ei ole 'LSTM', mutta jatketaan LSTM:llä tässä solussa.\")\n",
        "             # Voitaisiin myös nostaa virhe tai vaihtaa mallia dynaamisesti\n",
        "\n",
        "        # Alustetaan LSTMModel (varmista että luokka on määritelty)\n",
        "        if LSTMModel is not None:\n",
        "            model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout_prob).to(device)\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            print(\"\\n--- LSTM-Malli (ennen koulutusta) ---\")\n",
        "            print(model)\n",
        "            print(f\"Ominaisuuksien määrä (Input size): {input_size}\")\n",
        "            param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            print(f\"Koulutettavien parametrien määrä: {param_count}\")\n",
        "            print(f\"Käytettävä laite: {device}\")\n",
        "            print(\"-------------------------------------\\n\")\n",
        "\n",
        "            # --- Koulutetaan malli ---\n",
        "            # Varmista että train_model on määritelty\n",
        "            if train_model is not None:\n",
        "                try:\n",
        "                    # Kutsutaan train_model -funktiota\n",
        "                    model, train_losses, valid_losses = train_model(\n",
        "                        model, train_loader, valid_loader, criterion, optimizer, epochs, device, patience\n",
        "                    )\n",
        "\n",
        "                    if model is None or train_losses is None or valid_losses is None:\n",
        "                         print(\"\\nKoulutus keskeytyi tai epäonnistui train_model-funktiossa.\")\n",
        "                         model_lstm = None\n",
        "                    else:\n",
        "                         print(\"\\nKoulutus suoritettu.\")\n",
        "                         model_lstm = model # Tallenna onnistunut malli omaan muuttujaan\n",
        "\n",
        "                         # --- Piirretään häviökäyrät ---\n",
        "                         if train_losses and valid_losses:\n",
        "                            plt.figure(figsize=(10, 5))\n",
        "                            plt.plot(train_losses, label='Training Loss')\n",
        "                            plt.plot(valid_losses, label='Validation Loss')\n",
        "                            plt.title('Training and Validation Loss (LSTM)')\n",
        "                            plt.xlabel('Epoch')\n",
        "                            plt.ylabel('Loss (MSE)')\n",
        "                            try:\n",
        "                                min_loss_val = min(min(train_losses, default=1.0), min(valid_losses, default=1.0))\n",
        "                                if min_loss_val > 1e-9: plt.yscale('log'); print(\"Käytetään logaritmista skaalaa.\")\n",
        "                                else: print(\"Käytetään lineaarista skaalaa (min loss <= 0).\")\n",
        "                            except: print(\"Käytetään lineaarista skaalaa.\"); plt.yscale('linear')\n",
        "                            plt.legend(); plt.grid(True, linestyle=':', alpha=0.7); plt.show()\n",
        "                         elif train_losses is not None and valid_losses is not None: print(\"Häviölistat tyhjiä.\")\n",
        "                         else: print(\"Häviölistoja ei saatu koulutuksesta.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nVIRHE train_model-kutsun aikana: {e}\")\n",
        "                    traceback.print_exc()\n",
        "                    model_lstm = None\n",
        "            else:\n",
        "                print(\"VIRHE: train_model funktiota ei ole määritelty (Aja Osa 5).\")\n",
        "                model_lstm = None\n",
        "\n",
        "        else:\n",
        "             print(\"VIRHE: LSTMModel luokkaa ei ole määritelty (Aja Osa 4).\")\n",
        "             model_lstm = None\n",
        "\n",
        "\n",
        "    except (KeyError, AssertionError, ValueError) as e_init:\n",
        "         print(f\"\\nVIRHE mallin/parametrien alustuksessa: {e_init}\")\n",
        "         traceback.print_exc()\n",
        "         model_lstm = None\n",
        "\n",
        "else:\n",
        "    print(f\"\\nKoulutusta ei voida aloittaa, koska yksi tai useampi tarvittava muuttuja/funktio puuttuu: {missing_vars}\")\n",
        "    model_lstm = None\n",
        "\n",
        "\n",
        "# Tulostetaan lopputilanne\n",
        "if model_lstm is not None:\n",
        "     print(\"\\nOsa 6: Koulutuksen suoritus (LSTM v1) - VALMIS\")\n",
        "else:\n",
        "     print(\"\\nOsa 6: Koulutuksen suoritus (LSTM v1) - EPÄONNISTUI / OHITETTIIN\")"
      ],
      "metadata": {
        "id": "-QR2g85Pbf7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Arviointifunktioiden Määrittely (LSTM v1)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Varmistetaan tuonnit\n",
        "try:\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, classification_report\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "    from tqdm.notebook import tqdm\n",
        "    import traceback\n",
        "except ImportError as e:\n",
        "    print(f\"VIRHE: Tarvittavaa kirjastoa ei löydy: {e}\")\n",
        "    # Estä funktion määrittely, jos tuonnit epäonnistuvat\n",
        "    raise ImportError(\"Tarvittavia kirjastoja puuttuu arviointia varten.\")\n",
        "\n",
        "\n",
        "print(\"--- Määritellään arviointifunktiot ---\")\n",
        "\n",
        "# Funktio baseline-ennusteen laskemiseen\n",
        "def calculate_baseline_persistence_pytorch(targets_original_np, prediction_horizon):\n",
        "    \"\"\"Laskee naiivin persistenssi-baselinen PyTorch-datarakenteelle.\"\"\"\n",
        "    print(\"Lasketaan Baseline-ennuste (jakson eka arvo toistuu)...\")\n",
        "    try:\n",
        "        if not isinstance(targets_original_np, np.ndarray): raise TypeError(\"targets_original_np pitää olla numpy array\")\n",
        "        if targets_original_np.ndim != 3 or targets_original_np.shape[-1] != 1:\n",
        "             # Yritä korjata jos muoto (samples, horizon)\n",
        "             if targets_original_np.ndim == 2 and targets_original_np.shape[1] == prediction_horizon:\n",
        "                  print(f\"Muutetaan baseline-syötteen muoto {targets_original_np.shape} -> 3D\")\n",
        "                  targets_original_np = targets_original_np[..., np.newaxis]\n",
        "             else:\n",
        "                  raise ValueError(f\"Odotettiin 3D-muotoa (samples, horizon, 1), saatiin {targets_original_np.shape}\")\n",
        "\n",
        "        if targets_original_np.shape[0] == 0: return np.zeros((0, prediction_horizon))\n",
        "        first_vals = targets_original_np[:, 0, 0]; first_vals_for_repeat = first_vals[:, np.newaxis]\n",
        "        baseline_preds = np.repeat(first_vals_for_repeat, prediction_horizon, axis=1)\n",
        "        return baseline_preds\n",
        "    except Exception as e_base:\n",
        "        print(f\"VIRHE baseline-laskennassa: {e_base}\"); traceback.print_exc()\n",
        "        try: return np.zeros((targets_original_np.shape[0], prediction_horizon))\n",
        "        except: return None\n",
        "\n",
        "# Funktio mallin suorituskyvyn arviointiin\n",
        "def evaluate_model_performance_pytorch(model, test_loader, device, o3_scaler, o3_threshold_8h, prediction_horizon):\n",
        "    \"\"\"Arvioi PyTorch-mallia testidatalla, laskee metriikat ja vertaa baselineen.\"\"\"\n",
        "    print(\"\\n--- evaluate_model_performance_pytorch -funktion suoritus alkaa ---\")\n",
        "    if model is None: print(\"Malli puuttuu.\"); return None, None\n",
        "    if test_loader is None: print(\"Testilataaja puuttuu.\"); return None, None\n",
        "    if o3_scaler is None: print(\"O3-skaalain puuttuu.\"); return None, None\n",
        "\n",
        "    model.eval(); all_preds_orig_list = []; all_targets_orig_list = []\n",
        "    print(\"Aloitetaan ennusteiden tekeminen testidatalla...\")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets_original_batch in tqdm(test_loader, desc=\"Testaus (evaluate)\"):\n",
        "                inputs = inputs.to(device)\n",
        "                outputs_scaled = model(inputs)\n",
        "                preds_scaled_np = outputs_scaled.cpu().numpy()\n",
        "                preds_reshaped = preds_scaled_np.reshape(-1, 1)\n",
        "                preds_orig_np = o3_scaler.inverse_transform(preds_reshaped).reshape(preds_scaled_np.shape)\n",
        "                all_preds_orig_list.append(preds_orig_np)\n",
        "                all_targets_orig_list.append(targets_original_batch.cpu().numpy())\n",
        "        print(\"Ennusteiden tekeminen ja kerääminen valmis.\")\n",
        "        if not all_preds_orig_list or not all_targets_orig_list: raise ValueError(\"Ennusteiden/kohteiden keräys epäonnistui.\")\n",
        "        all_preds_orig = np.concatenate(all_preds_orig_list, axis=0)\n",
        "        all_targets_original = np.concatenate(all_targets_orig_list, axis=0) # Muoto (samples, horizon, 1)\n",
        "        print(f\"Kerätty {all_preds_orig.shape[0]} ennustetta/kohdetta.\")\n",
        "\n",
        "        # Poista viimeinen dimensio kohteista\n",
        "        if all_targets_original.ndim == 3 and all_targets_original.shape[-1] == 1:\n",
        "            targets_eval = all_targets_original.squeeze(-1)\n",
        "        elif all_targets_original.ndim == 2: targets_eval = all_targets_original\n",
        "        else: raise ValueError(f\"Odottamaton kohdemuoto metriikoille: {all_targets_original.shape}\")\n",
        "        print(f\"Muutettu kohdemuoto metriikoita varten: {targets_eval.shape}\")\n",
        "        if all_preds_orig.shape != targets_eval.shape: raise ValueError(f\"Lopulliset muodot eivät täsmää: Pred={all_preds_orig.shape}, Target={targets_eval.shape}\")\n",
        "\n",
        "        # --- Laske regressiometriikat (Nykyinen malli) ---\n",
        "        print(\"\\nLasketaan nykyisen mallin regressiometriikat...\")\n",
        "        # Käytä .ravel() laskeaksesi virheen kaikista pisteistä\n",
        "        rmse_model = np.sqrt(mean_squared_error(targets_eval.ravel(), all_preds_orig.ravel()))\n",
        "        mae_model = mean_absolute_error(targets_eval.ravel(), all_preds_orig.ravel())\n",
        "        print(f\"\\n--- Nykyisen Mallin ({type(model).__name__}) Arviointi ---\")\n",
        "        print(f\"RMSE: {rmse_model:.4f} µg/m³\")\n",
        "        print(f\"MAE:  {mae_model:.4f} µg/m³\")\n",
        "\n",
        "        # --- Laske Baseline ---\n",
        "        baseline_preds = calculate_baseline_persistence_pytorch(all_targets_original, prediction_horizon) # Käyttää 3D-muotoa\n",
        "        rmse_baseline, mae_baseline = None, None # Alustus\n",
        "        if baseline_preds is not None and baseline_preds.shape == targets_eval.shape:\n",
        "            if np.isnan(baseline_preds).any(): print(\"VAROITUS: Baseline sisältää NaN. Täytetään.\"); baseline_preds = pd.DataFrame(baseline_preds).ffill().bfill().values\n",
        "            if not np.isnan(baseline_preds).any():\n",
        "                rmse_baseline = np.sqrt(mean_squared_error(targets_eval.ravel(), baseline_preds.ravel()))\n",
        "                mae_baseline = mean_absolute_error(targets_eval.ravel(), baseline_preds.ravel())\n",
        "                print(f\"\\n--- Baseline-Mallin Arviointi ---\")\n",
        "                print(f\"RMSE: {rmse_baseline:.4f} µg/m³\")\n",
        "                print(f\"MAE:  {mae_baseline:.4f} µg/m³\")\n",
        "            else: print(\"Baseline-metriikoita ei laskettu (NaN).\")\n",
        "        else: print(\"Baseline-ennustetta ei laskettu tai muoto väärä.\")\n",
        "\n",
        "        # --- Vertailu ---\n",
        "        print(\"\\n--- Vertailu Baselineen ---\")\n",
        "        if rmse_model is not None and rmse_baseline is not None: print(f\"Malli vs Baseline RMSE: {rmse_baseline - rmse_model:+.4f} µg/m³ ({'Malli parempi' if (rmse_baseline - rmse_model) > 0 else 'Baseline parempi/sama'})\")\n",
        "        else: print(\"RMSE-vertailua ei voida tehdä.\")\n",
        "        if mae_model is not None and mae_baseline is not None: print(f\"Malli vs Baseline MAE:  {mae_baseline - mae_model:+.4f} µg/m³ ({'Malli parempi' if (mae_baseline - mae_model) > 0 else 'Baseline parempi/sama'})\")\n",
        "        else: print(\"MAE-vertailua ei voida tehdä.\")\n",
        "\n",
        "        # --- 8h Liukuvan keskiarvon arviointi ---\n",
        "        warnings_actual = []; warnings_pred = []\n",
        "        n_samples_total = all_preds_orig.shape[0]\n",
        "        print(f\"\\nLasketaan 8h liukuvia keskiarvoja {n_samples_total} jaksolle (kynnys={o3_threshold_8h} µg/m³)...\")\n",
        "        for i in range(n_samples_total):\n",
        "            actual_series = pd.Series(targets_eval[i, :]); pred_series = pd.Series(all_preds_orig[i, :])\n",
        "            actual_8h_avg = actual_series.rolling(window=8, min_periods=1).mean()\n",
        "            pred_8h_avg = pred_series.rolling(window=8, min_periods=1).mean()\n",
        "            warnings_actual.append(actual_8h_avg.max() > o3_threshold_8h)\n",
        "            warnings_pred.append(pred_8h_avg.max() > o3_threshold_8h)\n",
        "        warnings_actual = np.array(warnings_actual); warnings_pred = np.array(warnings_pred)\n",
        "        print(\"Liukuvien keskiarvojen laskenta valmis.\")\n",
        "\n",
        "        print(f\"\\n--- 8h Liukuvan Keskiarvon Varoitustason Ylityksen Arviointi ({type(model).__name__} - Jaksoittain) ---\")\n",
        "        print(f\"Todellisia varoitusjaksoja (> {o3_threshold_8h}): {warnings_actual.sum()} / {n_samples_total}\")\n",
        "        print(f\"Ennustettuja varoitusjaksoja ({type(model).__name__}): {warnings_pred.sum()} / {n_samples_total}\")\n",
        "        if warnings_actual.sum() > 0 or warnings_pred.sum() > 0:\n",
        "            print(\"\\nSekaannusmatriisi:\"); cm = confusion_matrix(warnings_actual, warnings_pred, labels=[False, True])\n",
        "            print(pd.DataFrame(cm, index=['Todellinen EI', 'Todellinen KYLLÄ'], columns=['Ennuste EI', 'Ennuste KYLLÄ']))\n",
        "            print(\"\\nLuokitteluraportti:\"); report = classification_report(warnings_actual, warnings_pred, target_names=['Ei Varoitusta', 'Varoitus'], labels=[False, True], zero_division=0); print(report)\n",
        "            TN, FP, FN, TP = cm.ravel(); recall = TP/(TP+FN) if (TP+FN)>0 else 0; precision = TP/(TP+FP) if (TP+FP)>0 else 0\n",
        "            print(f\"---> Recall: {recall:.4f}, Precision: {precision:.4f}\")\n",
        "        else: print(f\"Ei todellisia eikä ennustettuja varoitusjaksoja kynnysarvolla {o3_threshold_8h}.\")\n",
        "\n",
        "        print(\"\\n--- evaluate_model_performance_pytorch -funktion suoritus päättyi onnistuneesti ---\")\n",
        "        return all_preds_orig, targets_eval # Palautetaan 2D muodot\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n-----> VIRHE evaluate_model_performance_pytorch -FUNKTIOSSA <-----\")\n",
        "        traceback.print_exc(); return None, None\n",
        "\n",
        "print(\"\\nOsa 7: Arviointifunktioiden Määrittely (LSTM v1) - OK\")"
      ],
      "metadata": {
        "id": "vtDzMKPUcHGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Mallin Arviointi (Suoritus) (LSTM v1)\n",
        "\n",
        "import traceback\n",
        "import numpy as np # Varmistetaan tuonti\n",
        "import pandas as pd # Varmistetaan tuonti\n",
        "\n",
        "print(\"--- Aloitetaan Mallin Arviointi (Suoritus) ---\")\n",
        "\n",
        "# Alustetaan tulosmuuttujat\n",
        "test_preds_lstm = None\n",
        "test_targets_lstm = None\n",
        "evaluation_lstm_successful = False # Lipuke onnistumiselle\n",
        "\n",
        "# Varmistetaan, että tarvittavat muuttujat edellisistä osista ovat olemassa\n",
        "# Käytetään nyt model_lstm muuttujaa, johon tallennettiin koulutettu malli\n",
        "required_eval_vars = ['model_lstm', 'test_loader', 'device', 'o3_scaler',\n",
        "                      'O3_THRESHOLD_8H_AVG', 'PREDICTION_HORIZON',\n",
        "                      'evaluate_model_performance_pytorch']\n",
        "missing_eval_vars = []\n",
        "for var in required_eval_vars:\n",
        "     if var not in locals() or locals()[var] is None:\n",
        "          missing_eval_vars.append(var)\n",
        "\n",
        "if not missing_eval_vars:\n",
        "    # --- Suoritetaan arviointi ---\n",
        "    try:\n",
        "        print(f\"\\nKutsutaan evaluate_model_performance_pytorch LSTM-mallille (kynnys={O3_THRESHOLD_8H_AVG})...\")\n",
        "        # Annetaan koulutettu LSTM-malli funktiolle\n",
        "        # Palauttaa 2D-muotoiset ennusteet ja kohteet\n",
        "        test_preds_lstm, test_targets_lstm = evaluate_model_performance_pytorch(\n",
        "            model_lstm, # Käytä koulutettua LSTM-mallia\n",
        "            test_loader,\n",
        "            device,\n",
        "            o3_scaler,\n",
        "            O3_THRESHOLD_8H_AVG,\n",
        "            PREDICTION_HORIZON\n",
        "        )\n",
        "\n",
        "        # Tarkistetaan paluuarvot\n",
        "        if test_preds_lstm is not None and test_targets_lstm is not None:\n",
        "            print(\"\\nArviointifunktion ajo suoritettu.\")\n",
        "            if isinstance(test_preds_lstm, np.ndarray) and isinstance(test_targets_lstm, np.ndarray) \\\n",
        "               and test_preds_lstm.ndim == 2 and test_targets_lstm.ndim == 2 \\\n",
        "               and test_preds_lstm.shape == test_targets_lstm.shape:\n",
        "                 print(f\"Arviointi palautti validit tulokset muodoissa: Preds={test_preds_lstm.shape}, Targets={test_targets_lstm.shape}\")\n",
        "                 evaluation_lstm_successful = True\n",
        "            else:\n",
        "                 print(\"VIRHE: Arviointifunktion palauttamat arvot eivät ole odotettuja numpy arrayta tai muodot eivät täsmää.\")\n",
        "                 test_preds_lstm, test_targets_lstm = None, None\n",
        "\n",
        "        else:\n",
        "            print(\"\\nArviointi epäonnistui (funktio palautti None).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nVIRHE arvioinnin suorituksessa: {e}\")\n",
        "        traceback.print_exc()\n",
        "        test_preds_lstm, test_targets_lstm = None, None\n",
        "\n",
        "else:\n",
        "    print(f\"\\nArviointia ei voida suorittaa, koska yksi tai useampi tarvittava muuttuja puuttuu: {missing_eval_vars}\")\n",
        "    test_preds_lstm, test_targets_lstm = None, None\n",
        "\n",
        "\n",
        "# Tulostetaan lopputilanne\n",
        "if evaluation_lstm_successful:\n",
        "     print(\"\\nOsa 8: Arvioinnin suoritus (LSTM v1) - VALMIS\")\n",
        "else:\n",
        "     print(\"\\nOsa 8: Arvioinnin suoritus (LSTM v1) - EPÄONNISTUI / OHITETTIIN\")"
      ],
      "metadata": {
        "id": "xlKh1ZahcbMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}